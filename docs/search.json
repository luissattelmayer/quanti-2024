[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced RStudio Labsessions",
    "section": "",
    "text": "Course Overview\nThis repository contains all the course material for the RStudio Labsessions for the Spring semester 2024 at the School of Research at SciencesPo Paris. The class follows Brenda van Coppenolle’s and Jan Rovny’s lecture on Quantitative Methods II. Furthermore, the RStudio part of the course is a direct continuation of Malo Jan’s RStudio introduction course. If you feel the need to go back to some basics of general R use, data management or visualization, feel free to check out his course’s website. Rest assured, however, that 1) we will recap plenty of things, 2) make slow but steady progress, 3) and come back to the essentials of data wrangling again during the semester while building statistical models.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Structure",
    "text": "Course Structure\nIn total we will see each other 6 times. The lessons will be structured in such a way that I will first present something to you and explain my script. Ideally, you will then start coding in groups of 2 and work on exercises related to the topic. You can find more information about the exercises in the subsection “course validation”. I will of course be there to help you. The rest you solve at home and send me your final script. At the beginning of each next meeting we will go through the solutions together. Also, I upload my own script before each session, so you can use it as a template when solving the tasks and also later, when the course is over, as a template for further coding (if you like of course…).\n\n\n\n\n\n\n\n\nSession\nDescription\nDates\n\n\n\n\nSession 1\nRStudio Recap & OLS\n01/02 & 08/02\n\n\nSession 2\nLogistic Regressions\n15/02 & 29/02\n\n\nSession 3\nMultinomial Regression\n07/03 & 14/03\n\n\nSession 4\nCausal Inference I\n21/03 & 28/03\n\n\nSession 5\nCausal Inference II\n04/04 & 11/04\n\n\nSession 6\nTime Series\n18/04 & 25/04",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-validation",
    "href": "index.html#course-validation",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Validation",
    "text": "Course Validation\nIn the two weeks between each lecture, you will be given exercises to upload to the designated link for each session. The document where you write the solutions must be written in Markdown format.\nI will grade your solutions to my exercises on a 0 to 5 scale. I would like to see that you have done something and hopefully finished the exercise. If you are unable to finish the exercise, it is no problem and I do understand that not everybody feels as comfortable with R as some other people might do. Handing something in is key to getting points! This class can be finished by everyone and I do not want you to worry about your grade too much. But I would like that you all at least try to solve the exercises! Work in groups of two and try to hand in something after each session. The precise deadline will be communicated in class, the course’s GitHub page and on the Moodle page.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#optional-course-parts",
    "href": "index.html#optional-course-parts",
    "title": "Advanced RStudio Labsessions",
    "section": "Optional Course Parts",
    "text": "Optional Course Parts\nWhen I taught the course last year, some students approached me and asked for several levels of difficulty. I will try to implement this in the homework and in class. I have also decided to add an optional part to each session. In the optional parts, I will introduce new packages, advanced methods, and I will also upload a few scripts in the appendix on things like text-as-data, webscraping or similar, if I have the time. Also – again, if the times allows it – I will go through these optional parts in class. But please be assured that if you decide not to follow the optional parts, that is okay. But if you do, I can promise you will make better and faster progress. Lastly, if you are interested in certain things, want to learn about specific methods or how to implement things or workflows in RStudio, please do not hesitate to contact me and I will see if I can squeeze it in somewhere.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Advanced RStudio Labsessions",
    "section": "Requirements",
    "text": "Requirements\nYou must have downloaded R and RStudio by the beginning of the course (you need to install both!) before our sessions. Please let me know if you encounter any problems during the installation. Here is a quick guide on how to do that: https://rstudio-education.github.io/hopr/starting.html\nR and RStudio are both free and open source. You need both of them installed in order to operate with the R coding language.\nFor R, go on the CRAN website and download the file for your respective operating system: https://cran.r-project.org/ For RStudio, you need to do the same thing by clicking on this link: https://posit.co/products/open-source/rstudio/ RStudio has received a new name recently (“posit”) but you will still find all the necessary steps behind this link under the name of RStudio.\nOtherwise, there are few prerequisites except that you must bring your computer to the sessions with the required programs installed. I will provide you with datasets in each case and I will explain everything else in the course.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#help-and-office-hours",
    "href": "index.html#help-and-office-hours",
    "title": "Advanced RStudio Labsessions",
    "section": "Help and Office Hours",
    "text": "Help and Office Hours\nThere are unfortunately no regular office hours. But please do not hesitate to reach out, if you have any concerns, questions or feedback for me! My inbox is always open. I tend to reply quickly but in the case that I have not replied in under 48h, simply send the email again. I will not be offended!\nLearning how to code and working with RStudio can be a struggle and a tough task. I have started out once like you and I will try to keep that in mind. Feel free to always ask questions in class or if you see me on campus. The most important thing, however, is that you try!",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Advanced RStudio Labsessions",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis is only a quick section to give credit, where credit is due! Malo Jan is one of my daily inspirations for anything that has to do with research and RStudio. I have taught this class already last year and had most of my scripts written in PDFs but Rohan Alexander’s book Telling Stories with Data served as a new inspiration to write this course in its Quarto book format. Also shout outs to Felix Lennert and some of his ideas for the homework.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "1.1 Introduction\nThis is a short recap of things you have seen last year and will need this year as well. It will refresh your understanding of the linear regression method called ordinary least squares (OLS). This script is supposed to serve as a cheat sheet for you to which you can always come back to.\nThese are the main points of today’s session and script:\nThese are the packages we will be using in this session:\nneeds(\n  tidyverse,\n  rio, \n  stargazer,\n  broom,\n)",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#introduction",
    "href": "session1/session1.html#introduction",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "A refresher of Ordinary Least Squares (OLS)\nWhat is Base R and what are packages?\nA recap of basic coding in R\nBuilding & Interpreting a simple linear model\nVisualizing Residuals\nThe broom package (OPTIONAL!)",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#ordinary-least-squares-ols",
    "href": "session1/session1.html#ordinary-least-squares-ols",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.2 Ordinary Least Squares (OLS)",
    "text": "1.2 Ordinary Least Squares (OLS)\nOLS regressions are the powerhouse of statistics. The world must have been a dark place without them. They are the most basic form of linear regression and are used to predict the value of a dependent variable (DV) based on the value of independent variables (IVs). It is important to note that the relationship between the DV and the IVs is assumed to be linear.\nAs a quick reminder, this is the formula for a basic linear model: \\(\\widehat{Y} = \\widehat{\\alpha} + \\widehat{\\beta} X\\).\nOLS is a certain kind of method of linear model in which we choose the line which has the least prediction errors. This means that it is the best way to fit a line through all the residuals with the least errors. It minimizes the sum of the squared prediction errors \\(\\text{SSE} = \\sum_{i=1}^{n} \\widehat{\\epsilon}_i^2\\)\nFive main assumptions have to be met to allow us to construct an OLS model:\n\nLinearity: Linear relationship between IVs and DVs\nNo endogeneity between \\(y\\) and \\(x\\)\n\nErrors are normally distributed\nHomoscedasticity (variance of errors is constant)\nNo multicolinearity (no linear relationship between the independent variables)\n\nFor this example, I will be working with some test scores of a midterm and a final exam which I once had to work through. We are trying to see if there is a relationship between the score in the midterm and the grade of the final exam. Theoretically speaking, we would expect most of the students who did well on the first exam to also get a decent grade on the second exam. If our model indicates a statistical significance between the independent and the dependent variable and a positive coefficient of the former on the latter, this theoretical idea then holds true.",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#coding-recap",
    "href": "session1/session1.html#coding-recap",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.3 Coding Recap",
    "text": "1.3 Coding Recap\nBefore we start, let’s refresh our coding basics again. RStudio works with packages and libraries. There is something called Base R, which is the basic infrastructure that R always comes with when you install it. The R coding language has a vibrant community of contributors who have written their own packages and libraries which you can install and use. As Malo, I am of the tidyverse school and mostly code with this package or in its style when I am wrangling with data, changing its format or values and so on. Here and there, I will, however, try to provide you with code that uses Base R or other packages. In coding, there are many ways to achieve the same goal – and I will probably be repeating this throughout the semester – and we always strive for the fastest or most automated way. I will not force the tidyverse environment on you but I do think that it is one of the most elegant and fastest way of doing statistics in R. It is sort of my RStudio dialect but you can obviously stick to yours if you have found it. Also, as long as you find a way that works for you, that is fine with me!\nTo load the packages, we are going to need:\n\nlibrary(tidyverse)\n\nNext we will import the dataset of grades.\n\ndata &lt;- read_csv(\"course_grades.csv\")\n\nRows: 200 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): midterm|final_exam|final_grade|var1|var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe path which I specify in the read_csv file is short as this quarto document has the same working directory to which the data set is also saved. If you, for example, have your dataset on your computer’s desktop, you can access it via some code like this one:\n\ndata &lt;- read_csv(\"~/Desktop/course_grades.csv\")\n\nOr if it is within a folder on your desktop:\n\ndata &lt;- read_csv(\"~/Desktop/folder/course_grades.csv\")\n\n\n\n\n\n\n\nI will be only working within .Rproj files and so should you. 1 This is the only way to ensure that your working directory is always the same and that you do not have to change the path to your data set every time you open a new RStudio session. Further, this is the only way to make sure that other collaborators can easily open your project and work with it as well. Simply zip the file folder in which you have your code and\n\n\n\nYou can also import a dataset directly from the internet. Several ways are possible that all lead to the same end result:\n\ndataset_from_internet_1 &lt;- read_csv(\"https://www.chesdata.eu/s/1999-2019_CHES_dataset_meansv3.csv\")\n  \n# this method uses the rio package\nlibrary(rio)\ndataset_from_internet_2 &lt;- import(\"https://jan-rovny.squarespace.com/s/ESS_FR.dta\")\n\nLet’s take a first look at the data which we just imported:\n\n# tidyverse\nglimpse(data)\n\nRows: 200\nColumns: 1\n$ `midterm|final_exam|final_grade|var1|var2` &lt;chr&gt; \"17.4990613754243|15.641013…\n\n# Base R\nstr(data)\n\nspc_tbl_ [200 × 1] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ midterm|final_exam|final_grade|var1|var2: chr [1:200] \"17.4990613754243|15.64101334897|17.63|NA|NA\" \"17.7446326301825|18.7744366510731|14.14|NA|NA\" \"13.9316618079058|14.9978584022336|18.2|NA|NA\" \"10.7068243984724|11.9479428399047|19.85|NA|NA\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `midterm|final_exam|final_grade|var1|var2` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nSomething does not look right, this happens quite frequently when saving a csv file. It stands for comma separated value. R is having trouble reading this file since I have saved all grades with commas instead of points. Thus, we need to use the read_delim() function. Sometimes the read_csv2() function also does the trick. You’d be surprised by how often you encounter this problem. This is simply to raise your awareness to it!\nThe read_delim() function is the overall function of the readr package to read any sort of data file, whereas read_csv() and read_csv2() are specific functions to read csv files. The read_delim() function has a delim argument which you can use to specify the delimiter of your data file. For the sake of the example, I had purposefully saved the csv file using the | delimiter.\n\ndata &lt;- read_delim(\"course_grades.csv\", delim = \"|\")\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"|\"\ndbl (3): midterm, final_exam, final_grade\nlgl (2): var1, var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data)\n\nRows: 200\nColumns: 5\n$ midterm     &lt;dbl&gt; 17.499061, 17.744633, 13.931662, 10.706824, 17.118799, 17.…\n$ final_exam  &lt;dbl&gt; 15.641013, 18.774437, 14.997858, 11.947943, 15.694728, 17.…\n$ final_grade &lt;dbl&gt; 17.63, 14.14, 18.20, 19.85, 14.67, 20.26, 16.90, 13.40, 12…\n$ var1        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ var2        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nThis time, it has been properly imported. But by looking closer at it, we can see that there are two columns in the data frame that are empty and do not even have a name. We need to get rid of these first. Here are several ways of doing this. In coding, many ways lead to the same goal. In R, some come with a specific package, some use Base R. It is up to you to develop your way of doing things.\n\n# This is how you could do it in Base R\ndata &lt;- data[, -c(4, 5)]\n\n# Using the select() function of the dplyr package you can drop the fourth\n# and fifth columns by their position using the - operator and the -c() to\n# remove multiple columns\ndata &lt;- data  |&gt;  select(-c(4, 5))\n\n# I have stored the mutated data set in the old object; \n# you can also just transform the object itself...\ndata |&gt; select(-c(4, 5))\n\n# ... or create a new one\ndata_2 &lt;- data |&gt; select(-c(4, 5))\n\nNow that we have set up our data frame, we can build our OLS model. For that, we can simply use the lm() function that comes with Base R, it is built into R so to speak. In this function, we specify the data and then construct the model by using the tilde (~) between the dependent variable and the independent variable(s). Store your model in an object which can later be subject to further treatment and analysis.\n\nmodel &lt;- lm(final_exam ~ midterm, data)\nsummary(model)\n\n\nCall:\nlm(formula = final_exam ~ midterm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6092 -0.8411 -0.0585  0.8712  3.3086 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.62482    0.73212   6.317 1.72e-09 ***\nmidterm      0.69027    0.04819  14.325  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.34 on 198 degrees of freedom\nMultiple R-squared:  0.5089,    Adjusted R-squared:  0.5064 \nF-statistic: 205.2 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nSince the summary() function only shows us something in our console and the output is not very pretty, I encourage you to use the broom package for a nicer regression table.\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.62     0.732       6.32 1.72e- 9\n2 midterm        0.690    0.0482     14.3  2.10e-32\n\n\nYou can also use the stargazer package in order to export your tables to text or LaTeX format which you can then copy to your documents.\n\nlibrary(stargazer)\nstargazer(model, type = \"text\", out = \"latex\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                            final_exam         \n-----------------------------------------------\nmidterm                      0.690***          \n                              (0.048)          \n                                               \nConstant                     4.625***          \n                              (0.732)          \n                                               \n-----------------------------------------------\nObservations                    200            \nR2                             0.509           \nAdjusted R2                    0.506           \nResidual Std. Error      1.340 (df = 198)      \nF Statistic          205.196*** (df = 1; 198)  \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#interpretation-of-ols-results",
    "href": "session1/session1.html#interpretation-of-ols-results",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.4 Interpretation of OLS Results",
    "text": "1.4 Interpretation of OLS Results\nHow do we interpret this?\n\n\nR2: Imagine you’re trying to draw a line that best fits a bunch of dots (data points) on a graph. The R-squared value is a way to measure how well that line fits the dots. It’s a number between 0 and 1, where 0 means the line doesn’t fit the dots at all and 1 means the line fits the dots perfectly. R-squared tells us how much of the variation in the dependent variable is explained by the variation in the predictor variables.\n\nAdjusted R2: Adjusted R-squared is the same thing as R-squared, but it adjusts for how many predictor variables you have. It’s like a better indicator of how well the line fits the dots compared to how many dots you’re trying to fit the line to. It always adjusts the R-squared value to be a bit lower so you always want your adjusted R-squared value to be as high as possible.\n\nResidual Std. Error: The residual standard error is a way to measure the average distance between the line you’ve drawn (your model’s predictions) and the actual data points. It’s like measuring how far off the line is from the actual dots on the graph. Another way to think about this is like a test where you want to get as many answers correct as possible and if you are off by a lot in your answers, the residual standard error would be high, but if you are only off by a little, the residual standard error would be low. So in summary, lower residual standard error is better, as it means that the model is making predictions that are closer to the true values in the data.\n\nF Statistics: The F-statistic is like a test score that tells you how well your model is doing compared to a really simple model. It’s a way to check if the model you’ve built is any better than just guessing. A large F-statistic means that your model is doing much better than just guessing.",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#visualizing-the-regression-line",
    "href": "session1/session1.html#visualizing-the-regression-line",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.5 Visualizing the Regression Line",
    "text": "1.5 Visualizing the Regression Line\nJust for fun and to refresh you ggplot knowledge, let’s visualize the regression line. Here, we specify\n\nggplot(data, aes(x = midterm, y = final_exam)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n    labs(\n    title = \"Relationship between Midterm and Final Exam Scores\",\n    x = \"Midterm Scores\",\n    y = \"Final Exam Scores\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nRemember how I told you above that OLS was about finding the smallest amount of squared errors! This is what we can visualize here. The red lines are the distance from the residuals to the fitted line. The OLS line is the line that minimizes the sum of the squared errors!\n\nggplot(data, aes(x = midterm, y = final_exam)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # highlight the OLS logic graphically\n  geom_segment(\n    aes(\n      x = midterm,\n      y = final_exam,\n      xend = midterm,\n      yend = predict(model),\n      color = \"red\",\n      alpha = 0.5,\n    )) +\n  labs(title = \"Relationship between Midterm and Final Exam Scores\",\n       x = \"Midterm Scores\",\n       y = \"Final Exam Scores\") +\n  guides(color = FALSE, alpha = FALSE) +\n  theme_minimal()\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#the-broom-package-optional",
    "href": "session1/session1.html#the-broom-package-optional",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.6 The broom package (OPTIONAL!)",
    "text": "1.6 The broom package (OPTIONAL!)\nThe broom package in R is designed to bridge the gap between R’s statistical output and tidy data. 2 It takes the output of various R statistical functions and turns them into tidy data frames. This is particularly useful because many of R’s modeling functions return outputs that are not immediately suitable for further data analysis or visualization within the tidyverse framework.\n\n\n\n\n\n1.6.1 Nesting with nest()\n\nNesting in the context of the broom package usually refers to the idea of creating a list-column in a data frame (or even better a tibble) where each element of this list-column is itself a data frame (again, even better a tibble) or a model object. In a complex analysis, you might fit separate models to different subsets of data. Nesting allows you to store each of these models (or their broom-tidied summaries that we will see in the next three sub-sections) within a single, larger data frame (for the third time, the best is to do this with a tibble) for easier manipulation and analysis. For questions on what tibbles are see the below section on tibbles.\nLet’s go back to the midterm data which I have used before and randomly assign students into two classes (Class A or Class B) pretending that two different classes of students had taken the exams. I will then later on nest the data by class so that you can understand the logic of nesting.\n\ndata &lt;- data |&gt; \n  mutate(class = sample(c(\"A\", \"B\"), size = nrow(data), replace = TRUE))\n\nNow I will group my observations by class using group_by() and then nest them within these groups. The output will only contain two rows, class A and class B, and each row will contain a tibble with the observations of the respective class.\n\ndata |&gt; \n  group_by(class) |&gt;\n  nest()\n\n# A tibble: 2 × 2\n# Groups:   class [2]\n  class data              \n  &lt;chr&gt; &lt;list&gt;            \n1 A     &lt;tibble [91 × 3]&gt; \n2 B     &lt;tibble [109 × 3]&gt;\n\n\nThat is a very simple application of a nesting process. You can group_by() and nest() by many different variables. You might want to nest your observations per year or within countries. You can also nest by multiple variables at the same time. We will see this idea again in the next session when we will talk about the purrr package and how to automatically run regressions for several countries at the same time\n\n1.6.2 Model estimates with tidy()\n\nThe tidy() function takes statistical output ofa model and turns it into a tidy tibble. This means each row is an observation (e.g., a coefficient in a regression output) and each column is a variable (e.g., estimate, standard error, statistic). For instance, after fitting a linear model, you can use tidy() to create a data frame where each row represents a coefficient, with columns for estimates, standard errors, t-values, and p-values.\n\n1.6.3 Key metrics with glance()\n\nglance() provides a one-row summary of a model’s information. It captures key metrics that describe the overall quality or performance of a model, like \\(R^2\\), AIC, BIC in the context of linear models. This is useful for getting a quick overview of a model’s performance metrics.\n\n1.6.4 Residuals with augment()\n\nThe augment() function adds information about individual observations to the original data, such as fitted values or residuals in a regression model. You want to do this when you are evaluating a model fit at the observation level, checking for outliers, or understanding the influence of individual data points. We will talk more about model diagnostics in the next session!\nLet’s take a look at how this would work out in practice. For simplicity sake, we will set the nest() logic aside for a second and only look at the tidy(), glance(), and augment() functions.\nHere I only build the same model that we have already seen above:\n\nmodel &lt;- lm(final_exam ~ midterm, data = data)\n\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.62     0.732       6.32 1.72e- 9\n2 midterm        0.690    0.0482     14.3  2.10e-32\n\n\nNow let’s glance the hell out of the model:\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.509         0.506  1.34      205. 2.10e-32     1  -341.  689.  699.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nDon’t worry about what these things might mean for now, AIC and BIC will for example come up again next session.\n\n1.6.5 What is a tibble?",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#footnotes",
    "href": "session1/session1.html#footnotes",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "Malo’s explanation and way of introducing you to RStudio projects can be found here.↩︎\nAs a quick reminder these three principles are guiding when we speak of tidy data: 1) Every column is a variable, 2) Every row is an observation, 3) Every cell is a single value.↩︎",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session2/session2.html",
    "href": "session2/session2.html",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "2.1 Introduction\nYou have seen the logic of Logistic Regressions with Professor Rovny in the lecture. In this lab session, we will understand how to apply this logic to R and how to build a model, interpret and visualize its results and how to run some diagnostics on your models. If the time allows it, I will also show you how automatize the construction of your model and run several logistic regressions for many countries at once.\nThese are the main points of today’s session and script:",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#introduction",
    "href": "session2/session2.html#introduction",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "Getting used to the European Social Survey\nCleaning data: dropping rows, columns, creating and mutating variables\nBuilding a generalized linear model (glm()); special focus on logit/probit\nExtracting and interpreting the coefficients\nVisualization of results\nIntroduction to the purrr package and automation in RStudio (OPTIONAL!)",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#data-management-data-cleaning",
    "href": "session2/session2.html#data-management-data-cleaning",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.2 Data Management & Data Cleaning",
    "text": "2.2 Data Management & Data Cleaning\nAs I have mentioned last session, I will try to gradually increase the data cleaning part. It is integral to R and operationalizing our quantitative questions in models. A properly cleaned data set is worth a lot. This time we will work on how to drop values of variables (and thus rows of our dataset) which we are either not interested in or, most importantly, because they skew our estimations.\n\n# these are the packages, I will need for this session \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#importing-the-data",
    "href": "session2/session2.html#importing-the-data",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.3 Importing the data",
    "text": "2.3 Importing the data\nWe have seen how to import a dataset. Create an .Rproj of your choice and create a folder in which the dataset of this lecture resides. You can download this dataset from our Moodle page. I have pre-cleaned it a bit. If you were to download this wave of the European Social Survey from the Internet, it would be a much bigger data set. I encourage you to do this and try to figure out ways to manipulate your data but for now, we’ll stick to the slightly cleaner version.\n\n# importing the data; if you are unfamiliar with this operator |&gt; , ask me or\n# go to my document \"Recap of RStudio\" which you can find on Moodle\ness &lt;- read_csv(\"ESS_10_fr.csv\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs you can see from the dataset’s name, we are going to work with the European Social Survey. It is the biggest, most comprehensive and perhaps also most important survey on social and political life in the European Union. It comes in waves of two years and all the European states which want to pay for it produce their own data. In fact, the French surveys (of which we are going to use the most recent, 10th wave) are produced at SciencesPo, at the Centre de Données Socio-Politiques (CDSP)!\nThe ESS is extremely versatile if you need a broad and comprehensive data set for both national politics in Europe or to compare European countries. Learning how to use it, how to manage and clean the ESS waves will give you all the instruments to work with almost any data set that is “out there”. Also, some of you might want to use the ESS waves for your theses or research papers. There is a lot that can be done with it, not only cross-sectionally but also over time. So give it a try :)\nEnough advertisement for the ESS, let’s get back to wrangling with our data! As always, the first step is to inspect (“glimpse”) at our data and the data frame’s structure. We do this to see if obvious issues arise at a first glance.\n\nglimpse(ess)\n\nRows: 33,351\nColumns: 25\n$ name     &lt;chr&gt; \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS1…\n$ essround &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ edition  &lt;dbl&gt; 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2…\n$ proddate &lt;chr&gt; \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.1…\n$ idno     &lt;dbl&gt; 10002, 10006, 10009, 10024, 10027, 10048, 10053, 10055, 10059…\n$ cntry    &lt;chr&gt; \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"…\n$ dweight  &lt;dbl&gt; 1.9393836, 1.6515952, 0.3150246, 0.6730366, 0.3949991, 0.8889…\n$ pspwght  &lt;dbl&gt; 1.2907065, 1.4308782, 0.1131722, 1.4363747, 0.5848892, 0.6274…\n$ pweight  &lt;dbl&gt; 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177…\n$ anweight &lt;dbl&gt; 0.28100810, 0.31152576, 0.02463945, 0.31272244, 0.12734002, 0…\n$ prob     &lt;dbl&gt; 0.0003137546, 0.0003684259, 0.0019315645, 0.0009040971, 0.001…\n$ stratum  &lt;dbl&gt; 185, 186, 175, 148, 138, 182, 157, 168, 156, 135, 162, 168, 1…\n$ psu      &lt;dbl&gt; 2429, 2387, 2256, 2105, 2065, 2377, 2169, 2219, 2155, 2053, 2…\n$ polintr  &lt;dbl&gt; 4, 1, 3, 4, 1, 1, 3, 3, 3, 3, 1, 4, 2, 2, 3, 3, 2, 2, 4, 2, 3…\n$ trstplt  &lt;dbl&gt; 3, 6, 3, 0, 0, 0, 5, 1, 2, 0, 5, 4, 7, 5, 2, 2, 2, 2, 0, 3, 0…\n$ trstprt  &lt;dbl&gt; 3, 7, 2, 0, 0, 0, 3, 1, 2, 0, 7, 4, 2, 6, 2, 1, 3, 1, 0, 3, 3…\n$ vote     &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2…\n$ prtvtefr &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ clsprty  &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2…\n$ gndr     &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1…\n$ yrbrn    &lt;dbl&gt; 1945, 1978, 1971, 1970, 1951, 1990, 1981, 1973, 1950, 1950, 1…\n$ eduyrs   &lt;dbl&gt; 12, 16, 16, 11, 17, 12, 12, 12, 11, 3, 12, 12, 15, 15, 19, 11…\n$ emplrel  &lt;dbl&gt; 1, 3, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1…\n$ uemp12m  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n$ uemp5yr  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n\n\nAs we can see, there are many many variables (25 columns) with many many observations (33351). Some are quite straight-forward and the name is clear (“essround”, “age”) and some much less. Sometimes we can guess the meaning of a variable’s name. But most of the time – either because guessing is too annoying or because the abbreviation is not making any sense – we need to turn to the documentation of the data set. You can find the documentation of this specific version of the data set in an html-file on Moodle (session 2).\nEvery (good and serious) data set has some sort of documentation somewhere. If not, it is not a good data set and I am even tempted to say that we should be careful in using it! The documentation for data sets is called a code book. Code books are sometimes well crafted documents and sometimes just terrible to read. In this class, you will be exposed to both kinds of code books in order to familiarize you with both.\nIn fact, this dataframe still contains many variables which we either won’t need later on or that are simply without any information. Let’s get rid of these first. This is a step which you can also do later on but I believe that it is smart to this right at the beginning in order to have a neat and tidy data set from the very beginning.\nYou can select variables (select()) right at the beginning when importing the csv file.\n\ness &lt;- read_csv(\"ESS_10_fr.csv\")  |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  )\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHowever, I am realizing that when looking at the number of rows that my file is a bit too large for only one wave and only one country. By inspecting the ess$cntry variable, I can see that I made a mistake while downloading the dataset because it contains all countries of wave 10 instead of just one. We can fix this really easily when importing the dataset:\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  ) |&gt;\n  filter(cntry != \"FR\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis only leaves us with the values for France!\n\n2.3.0.1 Cleaning our DV\nAt this point, you should all check out the codebook of this data set and take a look at what the values mean. If we take the variable of ess$vote for example, we can see that there are many numeric values of which we can make hardly any sense (without guessing and we don’t do this over here) of what they might stand for.\n\nsummary(ess$vote) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.415   2.000   9.000 \n\n# remember that you can summary() both dataframes and individual variables\n\nOr in a table containing the amount of times that a value was given:\n\ntable(ess$vote)\n\n\n    1     2     3     7     8     9 \n22504  6542  1939   179   165    45 \n\n\nHere we can see that the variable vote contains the numeric values of 1 to 3 and then 7, 8, and 9. If we take a look at the code book, we can see what they stand for:\n\n1 = yes (meaning that the respondent voted)\n2 = no (meaning that the respondent did not vote)\n3 = not eligible to vote\n7 = Refusal\n8 = Don’t know\n9 = No answer\n\nThe meaning behind the values of 7, 8, and 9 are quite common and you will find them in almost all data sets which were made out of surveys. Respondents might not want to give an answer, the answer was not able to be read, or they indicated that they did not remember.\nSpoiler alert: We will work on voter turnout this session: Therefore, we will try to see what makes people vote and what decreases the likelihood that they vote at election day. The question of our dependent variable will thus be: Has the respondent voted or not?. Mathematically, this question cannot be answered with a linear regression which uses the OLS method as the dependent variable is binary meaning that 0 = has not voted/1 = has voted. There is only two possible outcomes and the variable is not continuous (one of the assumptions of an OLS).\nBut if we were to use the variable on voting turnout as it is right now, we would neither have a binary variable nor have a reliable variable as it contains values in which we are both not interested in and that will skew our estimations strongly. In fact, we cannot do a logistic regression (logit) on variables other than binary.\nThus, we first need to transform our dependent variable. We need to get rid of unwanted values and transform the 1s and 2s in 0s and 1s.\nFirst we will get rid of the unwanted values in which we are not interested.\n\n# dplyr\ness &lt;- ess |&gt;\n  filter(!vote %in% c(3, 7, 8))\n\nHere are two other ways to do it:\n\n# this one would be in base R\ness[!vote %in% c(3, 7, 8, 9)]\n\n# using the subset() function, this returns a logical vector which elements of\n# vote are not in the set of values 3, 7, 8, or 9\ness &lt;- subset(ess, vote %in% c(3,7,8,9) == F) \n\n# Alternatively you can use the %in% function with ! operator as well like this:\ness &lt;- subset(ess, !vote %in% c(3,7,8,9))\n\nQuick check to see if we got rid of all the values:\n\ntable(ess$vote)\n\n\n    1     2     9 \n22504  6542    45 \n\n\nPerfect, now we just need to transform the ones and twos into zeros and ones. This is both out of convention and also to torture you with some more data management. Since we are interested in people who do not vote, we will code those people as 0 and those who did vote as 1.\n\ness &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0))\n\nThe mutate() function is not perfectly intuitive at first sight. Here, I use the ifelse() function within mutate() to check if vote is equal to 1, if it is then it will remain 1 and if not it will be replaced by 0.\nIn Base R, you could do it like this but I believe that the mutate() function is probably the most elegant way of doing things… It’s up to you though:\n\n# only use the ifelse() function\ness$vote &lt;- ifelse(ess$vote == 1, 1, 0)\n\n# This will leave the value of 1 at 1 and change 2 to 0 for the column vote.\ness$vote[ess$vote == 1] &lt;- 1\ness$vote[ess$vote == 2] &lt;- 0\n\nWe are this close to having finished our data management part and to being able to finally work on our model. But we still have many many variables which are as “untidy” as our initial dependent variable was. Lastly, and I promise that this is the last data wrangling part for today and that we will get to our model in a moment, we need to check in the code book if specific values that we cannot simply replace over the whole data frame are still void of interest.\nOur independent variables of interest:\n\npolitical interest (polintr): c(7:9) needs to be dropped\ntrust in politicians (trstplt): no recoding necessary (unwanted values already NAs)\ntrust in political parties (trstprt): already done\nfeeling close to a party (clsprty): transform 1/2 into 0/1, drop c(7:8)\n\ngender (gndr): transform into 0/1 and drop the no answers\nyear born (yrbrn): already done\nyears of full-time education completed (eduyrs): already done\n\nWe will do every single step at once now using the pipes %&gt;% (tidyverse) or |&gt; (Base R) to have a tidy and elegant way of doing everything at once.\nBelow you can see that I first filter() out the unwanted values of our dependent variable. Then open up a muatate() in which I manipulate the independent variables which we are going to need for our model. Note how you can separate the different manipulations with a comma within the same function. This renders a very clean and easy to read code. There are several different ways you could do this. The recode(), if_else() and case_when() functions are all very useful for this kind of data wrangling. For clarity, I will use case_when() here. There is a newer version of this function called case_match() but it is only a couple of months old and for now the documentation of case_when() will probably be more helpful.\ncase_when() evaluates a series of conditions and applies the corresponding transformation to each element of a vector or column in a dataset. It’s like a more flexible and vectorized version of the if…else statement, allowing for multiple if…else conditions to be checked in a single function call. This is how it reads:\n\ncase_when(\n  condition1 ~ value_if_true1,\n  condition2 ~ value_if_true2,\n  TRUE ~ default_value\n)\n\nWith case_when(), you list your rules one by one. For each rule, you start with a condition (like “score is 80 or above”), followed by a ~, and then what you want to happen if that condition is true (like “they get an ‘A’”). After listing all your specific rules, you end with a special catch-all rule: TRUE ~ x. This is like saying, “For anyone else, or any situation I didn’t mention, do this.” This TRUE ~ x part is crucial. It ensures that if none of the specific conditions match (maybe because of some unexpected situation), you have a default action ready. It’s your “in case of anything else” rule.\nNow, let’s talk about NA_integer_. In R, NA means “not available” or missing information. But R is very particular about the kind of data it deals with. If your variable is numeric, R wants to know that even your missing pieces should be numbers. Using NA_integer_is your way of telling R, “This is a missing piece of information, but if it were here, it would be a number.” It helps keep everything organized and avoids confusion when R is looking through your data. There are also other instances (when recoding character strings for example), where you would use NA_character_ instead. Pay attention that if you have numeric values with decimal points, you will have to use NA_real_ instead of NA_integer_.\n\ness_final &lt;- ess |&gt; \n  # Filtering the dependent variable to get rid of any unnecessary rows\n  filter(!vote %in% c(3, 7, 8, 9)) |&gt; \nmutate(\n    # Recode 'polintr' to drop values 7 to 9\n    polintr = case_when(\n      polintr %in% 7:9 ~ NA_integer_,\n      TRUE ~ polintr\n    ),\n    # Recode 'clsprty' 1/2 into 0/1 and drop 7, 8\n    clsprty = case_when(\n      clsprty == 1 ~ 0,\n      clsprty == 2 ~ 1,\n      clsprty %in% c(7, 8) ~ NA_integer_,\n      TRUE ~ clsprty\n    ),\n    # Recode 'gndr' into 0/1 and handle other cases as NAs\n    gndr = case_when(\n      gndr == 1 ~ 0,\n      gndr == 2 ~ 1,\n      TRUE ~ NA_integer_\n    ),\n    # Recode 'vote' as binary 1 (voted) & 0 (abstention)\n    vote = case_when(\n      vote == 1 ~ 1,\n      TRUE ~ 0\n    )\n  )\n\n\n2.3.0.2 Optional quick/fancy data cleaning\nEdit: I have added a more sophisticated way of cleaning all your variables in one go. I have realized during the first session of session 2 that this might be too overwhelming but I would still like to keep this way of doing things to show you how you can do it in the future. If you wish to stick to the slightly longer but maybe also clearer way above, that is absolutely fine with me! Chose the way that you understand and feel comfortable with. As always, there are a lot of different paths to the same goal in R!\n\n# specify a string of numbers we are absolutely certain we won't need\nunwanted_numbers &lt;- c(66, 77, 88, 99, 7777, 8888, 9999)\n\n# make sure to create a new object/data frame; if you don't and re-run your code\n# a second time, it will transform some mutated values again!\ness_final &lt;- ess |&gt; \n  # filtering the dependent variable to get rid of any unnecessary rows\n  filter(!vote %in% c(3, 7, 8, 9)) |&gt; \n  # mutate allows us to transform values within variables into other \n  # values or NAs\n  # vote as binary 1 (voted) & 0 (abstention)\n  mutate(across(c(polintr, clsprty), ~replace(., . %in% c(7:9), NA)),\n         across(everything(), ~replace(., . %in% unwanted_numbers, NA)),\n         vote = ifelse(vote == 1, 1, 0),\n         # recode the variable to 0 and 1\n         clsprty = recode(clsprty, `1` = 1, `2` = 0),\n         # same for gender\n         gndr = recode(gndr, `1` = 0, `2` = 1))\n\n\n2.3.1 Constructing the logit-model\nIf you have made it this far and still bear with me, you have made it to the fun part! Specifying the model and running it, literally only takes one line of code (or two depending on the amount of independent variables). And as you can see, it is really straightforward. The glm() function stands for generalized linear model and comes with Base R.\nIn Professor Rovny’s lecture, we have seen that for a Maximum Likelihood Estimation (MLE) you need to know or have an assumption about the distribution of your dependent variable. And according to this distribution, you need to find the right linear model. If you have a binary outcome, your distribution is binomial. Within the function, we thus specify the family of the distribution as such. Note that you could also specify other families such as Gaussian, poisson, gamma or many more. We are not going to touch further on that but the glm() function is quite powerful. We can specify, within the family = argument, that we are doing a logistic regression. This can be done by adding link = logit to the argument. If ever you wanted to be precise and call a probit or cauchy link, it is here that you can specify this. The standard, however, is set to logit, so we would technically not be forced to specify it in this case.\nIn terms of the model we are building right now, it follows the idea that voting behavior (voted/not-voted) is a function of political interest, trust in politicians, trust in parties, feeling close to a specific party, as well as usual control variables such as gender, age, and education:\nBy no means is this regression just extensive enough to be published. It is just one example in which I suspect that political interest, trust in politics and politicians, and party affiliation are explanatory factors.\n\nlogit &lt;- glm(\n  vote ~ polintr + trstplt + trstprt + clsprty + gndr +\n    yrbrn + eduyrs,\n  data = ess_final,\n  family = binomial(link = logit)\n)\n\nThe object called logit contains our model with its coefficients, confidence intervals and many more things that we will play with! But as you can see, the actual construction of the model is more than simple…\n\nlibrary(broom)\ntidy(logit)\n\n# A tibble: 8 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  3.71      0.0835       44.4   0        \n2 polintr     -0.694     0.0193      -35.9   3.37e-282\n3 trstplt      0.00546   0.00253       2.16  3.08e-  2\n4 trstprt     -0.00521   0.00220      -2.37  1.78e-  2\n5 clsprty     -0.906     0.0347      -26.1   3.80e-150\n6 gndr         0.119     0.0307        3.88  1.07e-  4\n7 yrbrn       -0.0000156 0.0000262    -0.595 5.52e-  1\n8 eduyrs       0.00786   0.00162       4.85  1.27e-  6\n\n\nYou have seen both the broom package as well as stargazer in the last session.\n\nstargazer::stargazer(\n  logit,\n  type = \"text\",\n  dep.var.labels = \"Voting Behavior\",\n  dep.var.caption = c(\"Voting turnout; 0 = abstention | 1 = voted\"),\n  covariate.labels = c(\n    \"Political Interest\",\n    \"Trust in Politicians\",\n    \"Trust in Parties\",\n    \"Feeling Close to a Party\",\n    \"Gender\",\n    \"Year of Birth\",\n    \"Education\"\n  )\n)\n\n\n===================================================================\n                         Voting turnout; 0 = abstention | 1 = voted\n                         ------------------------------------------\n                                      Voting Behavior              \n-------------------------------------------------------------------\nPolitical Interest                       -0.694***                 \n                                          (0.019)                  \n                                                                   \nTrust in Politicians                      0.005**                  \n                                          (0.003)                  \n                                                                   \nTrust in Parties                          -0.005**                 \n                                          (0.002)                  \n                                                                   \nFeeling Close to a Party                 -0.906***                 \n                                          (0.035)                  \n                                                                   \nGender                                    0.119***                 \n                                          (0.031)                  \n                                                                   \nYear of Birth                             -0.00002                 \n                                         (0.00003)                 \n                                                                   \nEducation                                 0.008***                 \n                                          (0.002)                  \n                                                                   \nConstant                                  3.709***                 \n                                          (0.084)                  \n                                                                   \n-------------------------------------------------------------------\nObservations                               28,347                  \nLog Likelihood                          -13,535.140                \nAkaike Inf. Crit.                        27,086.280                \n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n2.3.2 Interpretation of a logistic regression\nInterpreting the results of a logistic regression can be a bit tricky because the predictions are in the form of probabilities, rather than actual outcomes. This sounds quite abstract and you are right, it is abstract. However, with a proper understanding of the coefficients and odds ratios, you can gain insights into the relationship between your independent variables and the binary outcome variable even without transforming your coefficients into more easily intelligible values.\nFirst the really boring and technical definition: The coefficients of a logistic regression model represent the change in the log-odds of the outcome for a one-unit change in the predictor variable (holding all other predictors constant). The sign of the coefficient indicates the direction of the association: positive coefficients indicate that as the predictor variable increases, the odds of the outcome also increase, while negative coefficients indicate that as the predictor variable increases, the odds of the outcome decrease.\nThe odds ratio, which can be calculated from the coefficients and we will see how that works in a second (exponentation is the key word), represents the ratio of the odds of the outcome for a particular value of the predictor variable compared to the odds of the outcome for a reference value of the predictor variable. An odds ratio greater than 1 indicates that the predictor variable is positively associated with the outcome, while an odds ratio less than 1 indicates that the predictor variable is negatively associated with the outcome.\nIt’s also important to keep in mind that a logistic regression model makes assumptions about the linearity, independence and homoscedasticity of the data, if these assumptions are not met it can affect the model’s performance and interpretation. We will see the diagnostics of logistic regression models again next session.\nIs this really dense and did I lose you? It is dense but I hope you bear with me because we will see that it becomes much clearer once we apply this theory to our model but also once we exponentiate the coefficients (reversing the logarithm so to speak) and interpret them as odds-ratios.\nBut from a first glimpse at our model summary we can see that political interest, trust in politicians, closeness to a party, age and education are all statistically significant, meaning that their p-value is &lt;.05! I will not regard any other variable that is not statistically significant as you do not usually interpret non-significant variables.\nNext, we can already say that the association of interest, closeness to party and age with voting behavior is negative. This is quite logical and makes sense in our case. If we look at the scales on which these variables are coded (code book!), we can see that the higher the value of the variable, the less interested, close or aged the respondents were. Thus, it decreases their likelihood to vote on voting day. Trust in politicians is coded the other way around. If I had been a little more thorough, it would have been good to put each independent variable on the same scale… But it means that trust in politicians (in fact meaning that they trust them less) raises the likelihood of not voting somehow (positive association).\n\n2.3.2.1 Odds-ratio\nIf you exponentiate the coefficients of your model, you can interpret them as odds-ratios. Odds ratios (ORs) are often used in logistic regression to describe the relationship between a predictor variable and the outcome. ORs are easier to interpret than the coefficients of a logistic regression because they provide a measure of the change in the odds of the outcome for a unit change in the predictor variable.\nAn OR greater than 1 indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome, and an OR less than 1 indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome.\nThe OR can also be used to compare the odds of the outcome for different levels of the predictor variable. For example, an OR of 2 for a predictor variable means that the odds of the outcome are twice as high for one level of the predictor variable compared to another level. Therefore, odds ratios are often preferred to coefficients for interpreting the results of a logistic regression, especially in applied settings.\nI will try to rephrase this and make it more accessible so that odds-ratios maybe become more intelligible (they are really nasty statistical stuff):\nImagine you’re playing a game where you have to guess whether a coin will land on heads or tails. If the odds of the coin landing on heads is the same as the odds of it landing on tails, then the odds-ratio would be 1. This means that the chances of getting heads or tails are the same. But if the odds of getting heads is higher than the odds of getting tails, then the odds-ratio would be greater than 1. This means that the chances of getting heads is higher than the chances of getting tails. On the other hand, if the odds of getting tails is higher than the odds of getting heads, then the odds-ratio would be less than 1. This means that the chances of getting tails is higher than the chances of getting heads. In logistic regression, odds-ratio is used to understand the relationship between a predictor variable (let’s say “X”) and an outcome variable (let’s say “Y”). Odds ratio tells you how much the odds of Y happening change when X changes.\nSo, for example, if the odds ratio of X is 2, that means that if X happens, the odds of Y happening are twice as high as when X doesn’t happen. And if the odds ratio of X is 0.5, that means that if X happens, the odds of Y happening are half as high as when X doesn’t happen.\n\n# simply use exp() on the coefficients of the logit\nexp(coef(logit))\n\n(Intercept)     polintr     trstplt     trstprt     clsprty        gndr \n 40.8120705   0.4997134   1.0054777   0.9948009   0.4040527   1.1261108 \n      yrbrn      eduyrs \n  0.9999844   1.0078943 \n\n# here would be a second way of doing it\nexp(logit$coefficients)\n\n(Intercept)     polintr     trstplt     trstprt     clsprty        gndr \n 40.8120705   0.4997134   1.0054777   0.9948009   0.4040527   1.1261108 \n      yrbrn      eduyrs \n  0.9999844   1.0078943 \n\n\nWe can also, and should, add the 95% confidence intervals (CI). As a quick reminder, the CI is a range of values that is likely to contain the true value of a parameter (the coefficients of our predictor variables in our case). This comes at a certain level of confidence. The most commonly used levels (attention, this is only a statistical convention!) of confidence are 95% and sometimes 99%.\nA 95% CI for a parameter, for example, means that if the logistic regression model were fitted to many different samples of data, the true value of the parameter would fall within the calculated CI for 95% of those samples.\n\n# most of the times the extra step in the next lines is not necessary and this \n# line of code is enough\nexp(cbind(OR = coef(logit), confint(logit)))\n\nWaiting for profiling to be done...\n\n\n                    OR      2.5 %     97.5 %\n(Intercept) 40.8120705 34.6333002 48.0596000\npolintr      0.4997134  0.4810885  0.5189496\ntrstplt      1.0054777  1.0005481  1.0105282\ntrstprt      0.9948009  0.9905477  0.9991390\nclsprty      0.4040527  0.3773609  0.4323782\ngndr         1.1261108  1.0604502  1.1958345\nyrbrn        0.9999844  0.9999340  1.0000369\neduyrs       1.0078943  1.0047438  1.0111607\n\n# here, however, we must combine both the exponentiate coefficients with the 95% confidence intervals\n# the format() function, helps me to show the numbers without the exponentiated \n# \"e\" and without scientific notation; the round() function within this function gives me values which are rounded on the 5th decimal place.\nformat(round(exp(cbind(\n  OR = coef(logit), confint(logit)\n)), 5),\nscientific = FALSE, digits = 4)\n\nWaiting for profiling to be done...\n\n\n            OR        2.5 %     97.5 %   \n(Intercept) \"40.8121\" \"34.6333\" \"48.0596\"\npolintr     \" 0.4997\" \" 0.4811\" \" 0.5190\"\ntrstplt     \" 1.0055\" \" 1.0006\" \" 1.0105\"\ntrstprt     \" 0.9948\" \" 0.9906\" \" 0.9991\"\nclsprty     \" 0.4041\" \" 0.3774\" \" 0.4324\"\ngndr        \" 1.1261\" \" 1.0604\" \" 1.1958\"\nyrbrn       \" 1.0000\" \" 0.9999\" \" 1.0000\"\neduyrs      \" 1.0079\" \" 1.0047\" \" 1.0112\"\n\n\nThis exponentiated value, the odds ratio (OR), now allows us to say that for a one unit increase in political interest, for example, the odds of voting (versus not voting) decrease. The same goes for the other variables.\nThe last thing about odds-ratio and I hope that this is the easiest to interpret, is when you try to make percentages out of it:\n\n# the [-1] drops the value of the intercept as it is statistically meaningless\n# we put another minus one to get rid of 1 as a threshold for interpreting the\n# odds-ratio\n# we multiply by 100 to have percentages\n100*(exp(logit$coefficients[-1])-1)\n\n      polintr       trstplt       trstprt       clsprty          gndr \n-50.028658955   0.547769108  -0.519908142 -59.594726196  12.611084187 \n        yrbrn        eduyrs \n -0.001560893   0.789429192 \n\n\nThis allows us to say that being politically uninterested decreases the odds of voting by 35%. Much more straightforward right?\n\n2.3.2.2 Predicted Probabilities\nPredicted probabilities also allow us to understand our logistic regression. In logistic regressions, the predicted probabilities and ORs are two different ways of describing the relationship between the predictor variables and the outcome. Predicted probabilities refer to the probability that a specific outcome will occur, given a set of predictor variables. They are calculated using the logistic function, which maps the linear combination of predictor variables (also known as the log-odds) to a value between 0 and 1.\nThe importance here is that we chose the predictor variables and at which values of those we are trying to predict the outcome. This is what we call “holding independent variables constant” while we calculate the predicted probability for a specific independent variable of interest.\nI will repeat this to make sure that everybody can follow along. With the predicted probabilities, we are trying to make out the effect of one specific variable of interest on our dependent variable, while we hold every other variable at their mean, median in some cases or, in the case of a dummy variable, at one of the two possible values. By holding them constant, we can be sure to see the singular effect of our independent variable of interest.\nIn our case, let “feeling close to a party” (1 = yes; 0 = no) be our independent variable of interest. We take our old ess_final dataframe and create a new one. In the newdata dataframe, we hold all values at their respective means or put our binary/dummy variables to 1. It is an arbitrary choice to put it to one here. We could also put it to 0. The only variable that we allow to alternate freely to find the predicted probabilities is our variable of interest clsprty.\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = mean(polintr),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = c(0, 1),\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n    ))\n\nIf that all worked out, we can predict the values for this specific independent variable by using the Base R predict() function:\n\nnewdata$preds &lt;- predict(logit, newdata = newdata, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata, aes(x = clsprty, y = preds)) +\n  geom_line() +\n  ylab(\"Likelihood of Voting\") + xlab(\"Feeling Close to a Party\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nWe can also do the same thing to see the predicted probability of political interest on voting behavior. This is a bit more interesting as the variable is not binary like ess$clsprty:\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata_1 &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = c(1:4),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = 1,\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n  )\n)\n\n\nnewdata_1$preds &lt;- predict(logit, newdata = newdata_1, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata_1, aes(x = polintr, y = preds)) +\n  geom_line() +\n  ylab(\"predicted probability\") + xlab(\"political interest\")\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n#combines value data frame created above with predicted probabilities evaluated \n# at the data values\nnewdata_1 &lt;-\n  cbind(newdata_1,\n        predict(\n          logit,\n          newdata = newdata_1,\n          type = \"link\",\n          se = TRUE\n        )) \n\n\nnewdata_1 &lt;- within(newdata_1, {\n  pp &lt;- plogis(fit)                   # predicted probability\n  lb &lt;- plogis(fit - (1.96 * se.fit)) # builds lower bound of CI\n  ub &lt;- plogis(fit + (1.96 * se.fit)) # builds upper bound of CI\n})\n\n\nggplot(newdata_1, aes(x = polintr, y = pp)) +\n  geom_line(aes(x = polintr, y = pp, color = as.factor(gndr))) +\n  geom_ribbon(aes(ymin = lb, ymax = ub), alpha = 0.3) +\n  theme(legend.position = \"none\") +\n  ylab(\"predicted probability to abstain from voting\") +\n  xlab(\"political interest\")\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\n\n2.3.3 Making life easiest\nYou are going to hate me if I tell you that all these steps which we just computed by hand… can be done by using a package. This is only 0.01% of me trying to be mean but mostly because it is extremely helpful and necessary to understand what is going on under the hood of predicted probabilities. The interpretation of logistic regressions is tricky and if you do not know what you are computing, it is even more complicated.\nWorking with packages is great, and I am aware that I always encourage you to use packages that make your life easier. But and this is an important “but” we do not always understand what is going on under the hood of a package. It is like putting your logistic regression into a black box, shaking it really well, and then taking a look at the output and putting it on shaky interpretational terms.\nBut enough of personal defense, as to why I made you suffer through all this. Here is my code to do most of the steps at once:\n\n# this package contains everything we need craft predicted probabilities and\n# visualize them as well\nlibrary(ggeffects)\n\n# like the predcit() function of Base R, we use ggpredict() and specify\n# our variable of interest\ndf &lt;- ggpredict(logit, terms = \"polintr\")\n\n# this is the simplest way of plotting this\nggplot(df, aes(x = x, y = predicted)) +\n  # our graph is more or less a line, so geom_line() applies\n  geom_line() +\n  # geom_ribbon() with the values that ggpredict() provided for the confidence\n  # intervals then gives\n  # us a shade around the geom_()line as CIs\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1)\n\n\n\n\n\n\n\nAnd voilà, your output it less than 10 lines of code.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#automating-things-in-r-optional",
    "href": "session2/session2.html#automating-things-in-r-optional",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.4 Automating things in R (OPTIONAL!)",
    "text": "2.4 Automating things in R (OPTIONAL!)\nWhen programming, it usually takes time to understand and apply things. The next step should often be to think about how to automate something in order to make processes faster and more elegant. Once we have understood a process relatively well, we can apply it to other areas in an automated way relatively easily.\nFor example, let’s think about our logistical regressions today. We worked with the ESS and looked at the 10th round of the data set for France. Our model aimed to investigate which variables can predict abstention. But I can also ask myself this question over a certain period of time, i.e. over several waves of the ESS, or for several countries. If I now tell you that you should do the logit for all countries of the ESS, the first step would be to run my code for n = countries of the ESS. However, this would result in a very long script, would not be very elegant and the longer the code, the higher the probability of errors.\nIf you are programming and realize that you have to do the same steps several times and it is actually the same step, only that a few parameters (such as the names of variables) change, then you can be sure that this could also be automated. At a certain point in programming, the goal should always be to create a pipeline for the work steps, which has the goal of making our code run as automatically as possible.\nIn RStudio we have several ways to automate this. For example, you can write so-called for-loops, which perform certain operations one after the other for certain list entries. Or you can write your own functions. At some point, someone decided to write the function glimpse() or read_csv2(), for example. As private users of R, we can do the same. In this way, we can, for example, accommodate several operations within a function that we write ourselves, which can then be applied simultaneously to several objects or a data set with different countries.\nI am aware that this is only the second session and that may sound like a lot. Everything from here on is optional, but I think it’s important that you see this as early as possible. Some of you may already feel comfortable enough to try something like this out for yourselves. If you don’t, that’s okay too. You will get there, trust me! I just want to show you what is possible and what you can do with R.\n\n\n\n\n\n2.4.1 Writing your function\nFunctions in R are incredibly powerful and essential for efficient and automated programming. A function in R is defined using the function keyword. The basic structure includes the name of the function, a set of parameters, and the body where the actual computations are performed.\nThe basic syntax is as follows:\nThere are some minor conventions in RStudio when writing functions. Some of them also apply to other parts than just functions.\n\nYou should give them some “breathing” space. When you write the accolades, put a space bar in between.\n\n\n# this is bad\nfunction(x){\n  x + 1\n}\n\n\n# this is good\nfunction(x) {\n  x + 1\n}\n\n\nYou should always put a space between the equal sign and the value you assign to a variable. Place spaces around all infix operators (=, +, -, &lt;-, etc.)\n\n\n# this is bad\ndata&lt;-read.csv(\"data.csv\")\n\n\ndata &lt;- read.csv(\"data.csv\")\n\n\n“stretch” your code when possible ctrl + shift + a can be of help for that\n\n\n# this is bad but in a longer example\ncountry_model &lt;- function(df) {glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, family = binomial(link = \"logit\"), data = df)\n}\n\n\n# this is better\ncountry_model &lt;- function(df) {\n  glm(\n    vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs,\n    family = binomial(link = \"logit\"),\n    data = df\n  )\n}\n\n\nWe usually assign verbs to functions. This means that the name of the function should be a verb that describes what the function does. If we want to create a function that reads the ESS files and does several operations at once, we should call it something like read_ess().\n\n2.4.2 The purrr package\nThe purrr package in R is a powerful tool that helps in handling repetitive tasks more efficiently. It’s part of the Tidyverse, a collection of R packages designed to make data science faster, easier, and more fun!\nIn simple terms, purrr improves the way you work with lists and vectors in R. It provides functions that allow you to perform operations, i.e. pre-existing functions or functions you will write yourselves, on each element of a list or vector without writing explicit loops. This concept is known as functional programming.\n\n\n\n\n\n\nWhy use purrr instead of for-loops?\n\n\n\n\n\n\nSimplifies Code: purrr makes your code cleaner and more readable. Instead of writing several lines of loop code, you can achieve the same with a single line using purrr.\nConsistency and Safety: purrr functions are consistent in their behavior, which reduces the chances of errors that are common in for loops, like mistakenly altering variables outside the loop.\nHandles Complexity Well: When working with complex or nested lists, purrr offers tools that make these tasks much simpler compared to traditional loops.\nIntegration with tidyverse: Since purrr is part of the tidyverse, it integrates smoothly with other Tidyverse packages, making your entire data analysis workflow more efficient.\n\n\n\n\nPut simply, we can use the functions of the purrr package to apply a function to each element of a list or vector. Depending on the specific purrr-function, our output can be different but we can also specify the output in our manually written function which we feed into purrr.\n\n2.4.3 The map() function\nThe map() function, part of the purrr package in R, is built around a simple yet powerful concept: applying a function to each element of a list or vector and returning a new list with the results. This concept is known as “mapping,” hence the name map().\n\n\nThe logic is simple. You take map(a list of your choice + your function) which then creates an output that behaves as if you had applied that function to each list entry individually.\n\nHere’s a breakdown of the logic behind map():\n\nInput: The primary input to map() is a list or a vector. This could be a list of numbers, characters, other vectors, or even more complex objects like data frames.\nFunction Application: You specify a function that you want to apply to each element of the list. This function could be a predefined function in R, or a custom function you’ve written. The key is that this function will be applied individually to each element of your list/vector.\nIteration: map() internally iterates over each element of your input list/vector. You don’t need to write a loop for this; map() handles it for you. For each element, map() calls the function you specified.\nOutput: For each element of the list/vector, the function’s result is stored. map() then returns a new list where each element is the result of applying the function to the corresponding element of the input.\nFlexibility in Output Type: The purrr package provides variations of map() to handle different types of output. For example, map_dbl() if your function returns doubles, map_chr() for character output, and so on. This helps in ensuring that the output is in the format you expect.\n\n2.4.4 Automatic regressions for several countries\nThis is absolutely only optional. I do not ask you to reproduce anything of this at any point in this class. I simply wanted to show you what you can do in R and what I mean when I say that automating stuff makes life easier.\nI present you here with a code that does the logistic regression we have been doing but on all countries of the ESS at the same time and then plots us the odds-ratio of our variable of interest “political interest”, as well as comparing McFadden pseudo R2 (we’ll see this term next session again).\nI will combine things from the optional section of Session 1 and the purrr package to do this. The idea is to build one model per country of the ESS, nest() it in a new tibble (What are tibbles again?) 1 where each row contains the information necessary for one country-model and to then use map() to apply the function tidy() from the broom package to each of these models.\nBelow, you can find a simple function that takes a dataframe as input and returns a logistic regression model. Within, I only specify the glm() function which I have shown you above. Within the parentheses of function() I specify the name of the input object. This name is arbitrary and can be anything you want. I chose df for dataframe. It has to appear somewhere within your function later on; usually there where your operation on the input object is supposed to be performed. In my case this is data = df.\n\ncountry_model &lt;- function(df) {\n  glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, \n      family = binomial(link = \"logit\"), data = df)\n}\n\n\nI will first talk you through the different steps and then provide you one long pipeline that does all this in one step.\n\nFirst, I import the ESS data and select the variables I want to use. I also clean the data a bit and get rid of unwanted observations or values.\n\n# Recoding the 'vote' variable to binary (1 or 0)\n# and filtering the dataset based on specified criteria\nprepared_ess &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt; \n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  )\n\nThen we will nest() the data as described here where I explain the logic of nesting.\n\n# library for McFadden pseudo R2\nlibrary(pscl)\nlibrary(broom)\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  select(cntry,\n         vote,\n         polintr,\n         trstplt,\n         trstprt,\n         clsprty,\n         gndr,\n         yrbrn,\n         eduyrs)\n\ness_model &lt;- ess |&gt; \n  as_tibble() |&gt;  # Convert the data frame to a tibble\n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt;\n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  ) |&gt;\n  group_by(cntry) |&gt;\n  nest() |&gt;\n  mutate(\n    model = map(data, country_model),\n    tidied = map(model, ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE)),\n    glanced = map(model, glance),\n    augmented = map(model, augment),\n    mcfadden = map(model, ~ pR2(.x)[4])\n  )\n\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\n\n\n\n# Comparing AICs\npR2(logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1.353514e+04 -1.520203e+04  3.333773e+03  1.096490e-01  1.109536e-01 \n         r2CU \n 1.686556e-01 \n\ness_model |&gt;\n  unnest(mcfadden) |&gt;\n  ggplot(aes(fct_reorder(cntry, mcfadden), mcfadden)) +\n  geom_col() + coord_flip() +\n  scale_x_discrete(\"Country\")\n\n\n\n\n\n\n\n\n# Comparing coefficients\ness_model |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"polintr\") |&gt;\n  ggplot(aes(\n    reorder(cntry, estimate),\n    y = exp(estimate),\n    color = cntry,\n    ymin = exp(conf.low),\n    ymax = exp(conf.high)\n  )) +\n  geom_errorbar() +\n  geom_point() +\n  scale_x_discrete(\"Country\") +\n  ylab(\"Odds-Ratio of political interest\") +\n  xlab(\"Country\")",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#footnotes",
    "href": "session2/session2.html#footnotes",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "The quick answer is that tibbles are a modern take on data frames in R, offering improved printing (showing only the first 10 rows and fitting columns to the screen), consistent subsetting behavior (always returning tibbles), tolerance for various column types, support for non-standard column names, and no reliance on row names. They represent a more adaptable, user-friendly approach for handling data in R, especially suited for large, complex datasets.↩︎",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session3/session3.html",
    "href": "session3/session3.html",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "3.1 Introduction\nIn this script, I will show you how to construct a multinomial logistic regression in R. For this, we will work on the European Social Survey (ESS) again. These are the main points that are covered in this script:",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#introduction",
    "href": "session3/session3.html#introduction",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "The logic of multinomial (logistic) regressions\nAdvanced Data Management\nInterpretation of a multinomial Model\nModel Diagnostics\nGoodness of Fit\nAPIs and Data Visualization (OPTIONAL!)",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#the-logic-of-multinomial-logistic-regressions",
    "href": "session3/session3.html#the-logic-of-multinomial-logistic-regressions",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.2 The Logic of multinomial (logistic) Regressions",
    "text": "3.2 The Logic of multinomial (logistic) Regressions\nI have chosen four countries out of which you will be able to choose one later one when I ask you to work on some exercises. For now, I will mainly work on Germany. One of the classic applications of multinomial models in political science is the question of voting behavior, more precisely vote choice. Last week, we have seen models of a logistic regression (logit). It is used in cases when our dependent variable (DV) is binary (0 or 1; true or false; yes or no) which means that we are not allowed to use OLS. The idea of logit can be extended to unordered categorical or nominal variables with more than two categories, e.g.: Vote choice, Religion, Brands…\nInstead of one equation modelling the log-odds of \\(P(X=1)\\), we do the same thing but for the amount of categories that we have. In fact, this means that a multinomial model runs several single logistic regressions on something we call a baseline. R will choose this baseline to which the categorical values of our DV will then relate. But we can also change it (this is called releveling). This allows us to make very interesting inferences with categorical (or ordinal) variables. If this sounds confusing, you should trust me when I tell you that this will become more straightforward in a second!\nHowever, this also makes the interpretation of these models a bit intricate and opaque at times. Nevertheless, you will see that once you have understood the basic idea of a multinomial regression and how to interpret the values in accordance to the baseline, it is not much different from logistic regressions on binary variables (and in my eyes even a bit simpler…). If the logic of logit is not 100% clear at this point, I recommend you go back to last session’s script on logit and work through my explanations. And if that does not help, try to follow this lecture attentively. As I said, the logic is the same, so I will repeat myself :) And if it is still unclear, you can always ask in class or come see me after the session!\nBut enough small talk, let’s first do some data wrangling which you all probably dread at this point…",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#data-management-for-multinomial-regression",
    "href": "session3/session3.html#data-management-for-multinomial-regression",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.3 Data Management for Multinomial Regression",
    "text": "3.3 Data Management for Multinomial Regression\nAs I have said, we will work on voting choice in four different countries. I selected Denmark and Germany. Germany I have chosen because I was working on this model a couple of months ago and Denmark is for fun.\nThe data which we will use for this session is the 9th round of the ESS published in 2018. The goal of this session is to understand predictors that tell us more about why people vote for Populist Radical-Right Parties, henceforth called PRRP (Mudde 2007). For this I have two main hypotheses in mind, as well as some predictors which I know are important based on the literature. Finally we also need some control variables which we need to control for in almost any regression analysis using survey data.\nMy two hypotheses (H1) and H2) are as follows:\n\nH1: Thinking that immigrants enrich a country’s culture decreases the likelihood of voting for PRRPs.\nH2: Having less trust in politicians increases the likelihood of voting for PRRPs than voting for other parties.\n\nNow you might notice two things. First, my hypotheses are relatively self-explanatory and you are absolutely right. They are more than that, they are perhaps even self-evident. But to this, I would just reply that this is supposed to be an easy exercise which is supposed to expose you to a multinomial regression and the logic of it. Second, you might see that my hypotheses are relatively broadly formulated. This is because I would like you, later in class, to choose one of the countries of the 9th wave of the ESS and build a model yourselves. By giving you broad hypotheses, you can do this ;)\n\n# read_csv from the tidyverse package\ness &lt;- read_csv(\"ESS9e03_1.csv\") |&gt; \n  # dplyr allows me to select only those variables I want to use later\n  select(cntry, \n         prtvtdfr, \n         prtvede1, \n         prtvtddk, \n         prtvtdpl, \n         imueclt, \n         yrbrn, \n         eduyrs, \n         hinctnta, \n         stflife, \n         trstplt, \n         blgetmg, \n         gndr) |&gt; \n  # based on the selected variables, I filter the dataframe so that I am only\n  # left with the data for Germany and Denmark\n  filter(cntry %in% c(\"DE\", \"DK\"))\n\nRows: 49519 Columns: 572\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): name, proddate, cntry, ctzshipd, cntbrthd, lnghom1, lnghom2, fbrn...\ndbl (562): essround, edition, idno, dweight, pspwght, pweight, anweight, pro...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAgain, every transformation and mutation of variables which you see below is done based on my knowledge of the dataset which I solely gained from looking at the code book. The code book can be found on the Moodle page (or the ESS’ website). It is highly important that you get used to reading a code book in general but especially to familiarize yourselves with the data which you will use by looking at the way that the variables are coded in the code book. There, for example, you will find information on the numeric values which are stored in the variables prtvtdfr, prtvede1, prtvtddk and prtvtdpl. They all stand for a category or, in our case, a party name which you can only identify if you open the code book. You will see that I only selected some parties in the mutate() function below. This is more or less to get rid of those parties that did not make it into the national parliament at the last national election of each country.\nYou have seen a similar chunk of code in the last script. See how, once you have a code that works for one dataset, you can use it again?\n\n# cleaning the dependent variables all over the dataframe\ness_clean &lt;- ess |&gt;\n    mutate(across(where(is.numeric), ~case_when(\n           . %in% c(66, 77, 88, 99, 7777, 8888, 9999) ~ NA_integer_,\n           TRUE ~ .)),\n      prtvtdfr = replace(prtvtdfr, prtvtdfr %in% c(1, 2, 10, 12:99), NA),\n           prtvede1 = replace(prtvede1, !prtvede1 %in% c(1:6), NA),\n           prtvtddk = replace(prtvtddk, !prtvtddk %in% c(1:10), NA),\n           prtvtdpl = replace(prtvtdpl, !prtvtdpl %in% c(1:8), NA),\n           # get rid of unwanted values indicating no response etc\n           blgetmg = replace(blgetmg, !blgetmg %in% c(1:2), NA),\n           # gender recoded to 1 = 0, 2 = 1 (my personal preference)\n           gndr = recode(gndr, `1` = 0, `2` = 1))\n\nIn fact, you could already build the model now and start the multinomial regression. However, I add an additional data management step by placing the numeric values of the election variable in a new variable called vote_de, where I convert the numeric values to character values and at the same time give them the names of the parties. This will automatically transform NAs in all the rows in which the country is not that in which the person has voted.\nBut more importantly, once I run the regression, it will display the parties’ names instead of the numbers. This means that I won’t have to go back to the code book every time to check what the 1s or 2s correspond to.\n\n# this is simple base R creating a new column/variable with character\n# values corresponding to the parties' names behind the numeric values\ness_clean$vote_de[ess_clean$prtvede1==1]&lt;-\"CDU/CSU\"\ness_clean$vote_de[ess_clean$prtvede1==2]&lt;-\"SPD\"\ness_clean$vote_de[ess_clean$prtvede1==3]&lt;-\"Die Linke\"\ness_clean$vote_de[ess_clean$prtvede1==4]&lt;-\"Grüne\"\ness_clean$vote_de[ess_clean$prtvede1==5]&lt;-\"FDP\"\ness_clean$vote_de[ess_clean$prtvede1==6]&lt;-\"AFD\"\n\nHere is a way to mutate all the variables at once. However, this somehow creates conflicts with a package used further below.\n\ness_clean &lt;- ess_clean |&gt; \n  mutate(\n    vote_dk = case_when(prtvtddk == 1 ~ \"Socialdemokratiet\",\n                        prtvtddk == 2 ~ \"Det Radikale Venstre\",\n                        prtvtddk == 3 ~ \"Det Konservative Folkeparti\",\n                        prtvtddk == 4 ~ \"SF Socialistisk Folkeparti\",\n                        prtvtddk == 5 ~ \"Dansk Folkeparti\",\n                        prtvtddk == 6 ~ \"Kristendemokraterne\",\n                        prtvtddk == 7 ~ \"Venstre\",\n                        prtvtddk == 8 ~ \"Liberal Alliance\",\n                        prtvtddk == 9 ~ \"Enhedslisten\",\n                        prtvtddk == 10 ~ \"Alternativet\",\n                        TRUE ~ NA_character_),\n    vote_de = case_when(prtvede1 == 1 ~ \"CDU/CSU\",\n                        prtvede1 == 2 ~ \"SPD\",\n                        prtvede1 == 3 ~ \"Die Linke\",\n                        prtvede1 == 4 ~ \"Grüne\",\n                        prtvede1 == 5 ~ \"FDP\",\n                        prtvede1 == 6 ~ \"AFD\"))",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#constructing-the-model",
    "href": "session3/session3.html#constructing-the-model",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.4 Constructing the Model",
    "text": "3.4 Constructing the Model\nNow that the data management process is finally over, we can specify our model. For this, you need to install the nnet package and load it to your library. Once this is done, we will take the exact same steps as you would do for an OLS or logit model. You specify your DV followed by a ~ and then you only need to add all your IVs. Lastly, you need to specify the data source. Hess = TRUE will provide us with a Hessian matrix that we need for a package later. If you don’t know what that is… that is absolutely fine!\n\nlibrary(nnet)\nmodel_de &lt;- multinom(vote_de ~ imueclt  + stflife + trstplt + blgetmg + \n                    gndr + yrbrn + eduyrs + hinctnta,\n                     data = ess_clean,\n                     Hess = TRUE)\n\n# weights:  60 (45 variable)\ninitial  value 2512.046776 \niter  10 value 2043.014063\niter  20 value 2023.586615\niter  30 value 1980.080494\niter  40 value 1938.289705\niter  50 value 1927.868641\niter  60 value 1926.043042\niter  70 value 1925.949503\niter  80 value 1925.873772\nfinal  value 1925.814902 \nconverged\n\nmodel_dk &lt;- multinom(vote_dk ~ imueclt  + stflife + trstplt + blgetmg + gndr +\n                     yrbrn + eduyrs + hinctnta,\n                     data = ess_clean,\n                     Hess = TRUE)\n\n# weights:  100 (81 variable)\ninitial  value 2525.935847 \niter  10 value 2265.701974\niter  20 value 2136.112054\niter  30 value 2056.238932\niter  40 value 1999.847654\niter  50 value 1950.379411\niter  60 value 1938.124756\niter  70 value 1934.172909\niter  80 value 1915.231553\niter  90 value 1908.197162\niter 100 value 1906.641552\nfinal  value 1906.641552 \nstopped after 100 iterations\n\n\n\n3.4.1 Re-leveling your DV\nIn my case, the German PRRP is called Alternative für Deutschland meaning it starts with an “A”. R tends to take the alphabetical order as a criterion for the baseline meaning that the baseline for your multinomial model is chosen based on the party which comes first in alphabetical order. Depending on what you want to show, you might want to change the baseline which we can do with the relevel() function. Let’s say we are not interested in vote choice regarding the PRRP but conservative parties and thus want to put the German Christian conservative party, the CDU/CSU, as a baseline. Here is how we could do this in R:\n\n# don't run this code chunk\n#| eval: false\n# you need to specify your DV as a factor for this; further, the ref must \n# contain the exact character label of the party\ness_clean$vote_de &lt;- relevel(as.factor(ess_clean$vote_de), ref = \"CDU/CSU\")",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#interpreting-a-multinomial-model",
    "href": "session3/session3.html#interpreting-a-multinomial-model",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.5 Interpreting a Multinomial Model",
    "text": "3.5 Interpreting a Multinomial Model\nYou already know that I like the stargazer package for displaying a regression table. This time I paid attention to what level of statistical significance leads to a star (*). I changed it so that, like in the summary() function, p-values below 0.05 will be used as the minimum level of statistical significance instead of 0.1. dep.var.caption = allows be to specify a caption for our DV and we can use our own labels for the IVs instead of the variables’ names by using the covariate.labels = argument.\nI have specified in the first chunk of code which arguments concern the generated output in LaTeX. I still recommend you start learning how to write papers in LaTeX. This is just to say that some arguments are not useful at all when type = \"text. But LaTeX generates more beautiful tables ;)\n\n# specifying the object in which the model is stored\nstargazer::stargazer(\n  model_de,\n  # adding a title to the table\n  title = \"Multinomial Regression Results Germany\",\n  # change this to the desired output format; either\n  # LaTeX, html, or text (depending on your document)\n  # editor\n  type = \"html\",\n  # some LaTeX information\n  float = TRUE,\n  # font size of the LaTeX table\n  font.size = \"small\",\n  # column width in final LaTeX table\n  column.sep.width = \"-10pt\",\n  # specifying the p-values which lead to stars in our\n  # table\n  star.cutoffs = c(.05, .01, .001),\n  # caption for the DV\n  dep.var.caption = c(\"Vote Choice\"),\n  # labels for our IVs; must be in the same order as our\n  # IVs in the initial model\n  covariate.labels = c(\n    \"Positivity Immigration\",\n    \"Satisfaction w/ Life\",\n    \"Trust in Politicians\",\n    \"Ethnic Minority\",\n    \"Gender\",\n    \"Age\",\n    \"Education\",\n    \"Income\"\n  )\n)\n\n\nMultinomial Regression Results Germany\n\n\n\n\n\n\n\n\n\nVote Choice\n\n\n\n\n\n\n\n\n\n\n\n\nCDU/CSU\n\n\nDie Linke\n\n\nFDP\n\n\nGrüne\n\n\nSPD\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n\n\n\n\n\n\nPositivity Immigration\n\n\n0.350***\n\n\n0.617***\n\n\n0.326***\n\n\n0.784***\n\n\n0.493***\n\n\n\n\n\n\n(0.063)\n\n\n(0.080)\n\n\n(0.076)\n\n\n(0.075)\n\n\n(0.065)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatisfaction w/ Life\n\n\n0.029\n\n\n-0.171*\n\n\n0.095\n\n\n-0.094\n\n\n-0.109\n\n\n\n\n\n\n(0.067)\n\n\n(0.083)\n\n\n(0.093)\n\n\n(0.079)\n\n\n(0.067)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust in Politicians\n\n\n0.513***\n\n\n0.281**\n\n\n0.424***\n\n\n0.352***\n\n\n0.398***\n\n\n\n\n\n\n(0.077)\n\n\n(0.092)\n\n\n(0.090)\n\n\n(0.085)\n\n\n(0.078)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthnic Minority\n\n\n-0.054***\n\n\n-0.443***\n\n\n-1.086***\n\n\n-0.382***\n\n\n-0.385***\n\n\n\n\n\n\n(0.002)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.002)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n0.859***\n\n\n0.240*\n\n\n0.573***\n\n\n0.928***\n\n\n0.395***\n\n\n\n\n\n\n(0.103)\n\n\n(0.096)\n\n\n(0.095)\n\n\n(0.137)\n\n\n(0.112)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n-0.013***\n\n\n0.007***\n\n\n-0.010***\n\n\n-0.002***\n\n\n-0.016***\n\n\n\n\n\n\n(0.0004)\n\n\n(0.0005)\n\n\n(0.001)\n\n\n(0.0005)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation\n\n\n0.014\n\n\n0.081\n\n\n0.036\n\n\n0.091\n\n\n0.040\n\n\n\n\n\n\n(0.051)\n\n\n(0.059)\n\n\n(0.060)\n\n\n(0.055)\n\n\n(0.052)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n0.116*\n\n\n-0.032\n\n\n0.114\n\n\n0.136*\n\n\n0.047\n\n\n\n\n\n\n(0.055)\n\n\n(0.066)\n\n\n(0.067)\n\n\n(0.062)\n\n\n(0.056)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n23.264***\n\n\n-16.715***\n\n\n17.583***\n\n\n-1.578***\n\n\n30.969***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.00005)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\n\n# the annotations of the above model would be the same for this model\nstargazer::stargazer(\n  model_dk,\n  title = \"Multinomial Regression Results Denmark\",\n  type = \"html\",\n  float = TRUE,\n  font.size = \"tiny\",\n  star.cutoffs = c(.05, .01, .001),\n  dep.var.labels = c(\"Germany\"),\n  dep.var.caption = c(\"Vote Choice\"),\n  covariate.labels = c(\n    \"Positivity Immigration\",\n    \"Satisfaction w/ Life\",\n    \"Trust in Politicians\",\n    \"Ethnic Minority\",\n    \"Gender\",\n    \"Age\",\n    \"Education\",\n    \"Income\"\n  )\n)\n\n\nMultinomial Regression Results Denmark\n\n\n\n\n\n\n\n\n\nVote Choice\n\n\n\n\n\n\n\n\n\n\n\n\nGermany\n\n\nDet Konservative Folkeparti\n\n\nDet Radikale Venstre\n\n\nEnhedslisten\n\n\nKristendemokraterne\n\n\nLiberal Alliance\n\n\nSF Socialistisk Folkeparti\n\n\nSocialdemokratiet\n\n\nVenstre\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n(7)\n\n\n(8)\n\n\n(9)\n\n\n\n\n\n\n\n\nPositivity Immigration\n\n\n-0.667***\n\n\n-0.440***\n\n\n-0.018\n\n\n0.162*\n\n\n-0.169\n\n\n-0.403***\n\n\n0.028\n\n\n-0.238***\n\n\n-0.400***\n\n\n\n\n\n\n(0.066)\n\n\n(0.083)\n\n\n(0.084)\n\n\n(0.081)\n\n\n(0.133)\n\n\n(0.087)\n\n\n(0.084)\n\n\n(0.059)\n\n\n(0.060)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatisfaction w/ Life\n\n\n0.153*\n\n\n0.238\n\n\n0.132\n\n\n-0.043\n\n\n0.174\n\n\n0.164\n\n\n0.071\n\n\n0.081\n\n\n0.249***\n\n\n\n\n\n\n(0.070)\n\n\n(0.131)\n\n\n(0.109)\n\n\n(0.082)\n\n\n(0.176)\n\n\n(0.125)\n\n\n(0.098)\n\n\n(0.059)\n\n\n(0.069)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust in Politicians\n\n\n0.180**\n\n\n0.349***\n\n\n0.300***\n\n\n0.003\n\n\n0.334*\n\n\n0.298**\n\n\n0.103\n\n\n0.264***\n\n\n0.451***\n\n\n\n\n\n\n(0.068)\n\n\n(0.093)\n\n\n(0.086)\n\n\n(0.076)\n\n\n(0.144)\n\n\n(0.095)\n\n\n(0.082)\n\n\n(0.061)\n\n\n(0.064)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthnic Minority\n\n\n0.557***\n\n\n23.666***\n\n\n-0.386***\n\n\n-0.389***\n\n\n11.010***\n\n\n0.596***\n\n\n0.179***\n\n\n-0.430***\n\n\n0.494***\n\n\n\n\n\n\n(0.001)\n\n\n(0.0003)\n\n\n(0.002)\n\n\n(0.006)\n\n\n(0.0004)\n\n\n(0.001)\n\n\n(0.002)\n\n\n(0.004)\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n-0.617***\n\n\n-0.128**\n\n\n-0.378***\n\n\n-0.030\n\n\n-0.706***\n\n\n-0.507***\n\n\n0.204\n\n\n-0.164\n\n\n-0.182\n\n\n\n\n\n\n(0.185)\n\n\n(0.047)\n\n\n(0.106)\n\n\n(0.204)\n\n\n(0.007)\n\n\n(0.029)\n\n\n(0.119)\n\n\n(0.130)\n\n\n(0.138)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n-0.023***\n\n\n-0.024***\n\n\n-0.001\n\n\n-0.008***\n\n\n-0.005***\n\n\n0.054***\n\n\n-0.010***\n\n\n-0.024***\n\n\n-0.025***\n\n\n\n\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.0005)\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation\n\n\n-0.144***\n\n\n-0.035\n\n\n-0.017\n\n\n-0.052\n\n\n-0.083\n\n\n-0.036\n\n\n-0.042\n\n\n-0.103***\n\n\n-0.105***\n\n\n\n\n\n\n(0.033)\n\n\n(0.038)\n\n\n(0.035)\n\n\n(0.034)\n\n\n(0.060)\n\n\n(0.045)\n\n\n(0.036)\n\n\n(0.029)\n\n\n(0.030)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n-0.023\n\n\n0.216**\n\n\n0.118\n\n\n-0.067\n\n\n-0.165\n\n\n0.162*\n\n\n-0.050\n\n\n-0.008\n\n\n0.070\n\n\n\n\n\n\n(0.064)\n\n\n(0.081)\n\n\n(0.071)\n\n\n(0.065)\n\n\n(0.115)\n\n\n(0.080)\n\n\n(0.069)\n\n\n(0.057)\n\n\n(0.059)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n48.626***\n\n\n-1.783***\n\n\n0.004***\n\n\n17.170***\n\n\n-12.969***\n\n\n-107.485***\n\n\n18.189***\n\n\n51.982***\n\n\n50.057***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\nThe format of the regression table on our Danish model is not ideal since the names of the parties are quite long and overlap. Blame this on my lack of knowledge of abbreviations of Danish parties…",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#interpreting-a-multinomial-regression-table",
    "href": "session3/session3.html#interpreting-a-multinomial-regression-table",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.6 Interpreting a Multinomial Regression Table",
    "text": "3.6 Interpreting a Multinomial Regression Table\nWe can see that many many things are going on in this regression table. Let us try to analyze our results step by step.\nFirst of all, we can see that we have many variables that are statistically significant (lots of stars yay!). This is always a good sign. Note also that the baseline was the party AFD. You can see this based on the fact that the category AFD which our DV can take on is not given in our table. This means that whenever we see the results where the DV is one of the parties, R has calculated the coefficients based on the logic that the respondent would have voter for either the party in the dependent variable or the party of the baseline, which in our case is that of the AFD. In more mathematical terms these are several single logistic regressions always with regards to the baseline AFD which are then aggregated to a multinomial regression. And to be slightly more mathematical, this means our DV is technically: \\(1 = DV\\) and then \\(0 = AFD\\).\nTherefore, we can interpret the results exactly like we would for a logistic regression. Last week it was about the likelihood of voting abstention, this week it is the likelihood of voting for the CDU/CSU instead of the AFD, or voting for the SPD instead of the AFD, or voting for Die Linke instead of the AFD, and so on. You get the idea hopefully.\nRemember that these are the coefficients of logistic regressions. We cannot interpret them linearily like in OLS. For now, the regression table tells us something about the statistical significance of our predictors and the direction of association: whether or not a statistically significant predictor increases or decreases the likelihood of voting for either or.\n\n3.6.1 The Hypotheses\nAs a reminder, these were my initial (frankly also bad) hypotheses:\n\nH1: Thinking that immigrants enrich a country’s culture decreases the likelihood of voting for PRRPs.\nH2: Having less trust in politicians increases the likelihood of voting for PRRPs than voting for other parties.\n\nI am now interested to see the effect of positivity toward migration and trust in politicians on the vote choice for each party instead of the AFD. What we can see is that a one-unit increase in positive attitudes toward migration (thinking that immigrants culturally enrich the respondents’ country) raises the likelihood for voting for all other parties instead of voting for the AFD. In the case of the first column, in which the vote was either for the CDU/CSU or the AFD, a one unit increase in stances on immigration results in a higher likelihood of voting of voting for the CDU/CSU than the AFD.\nIf we now turn to trust in politicians and this variable’s effect on vote choice for the different German parties, we can see that overall there is a statistically significant a positive association with having more trust in politicians and also voting for other parties than the AFD. In return, this also means that low trust in politicians raises the likelihood of voting for the AFD.\nYou could obviously exponentiate the values that we have here in order to get the odds-ratio. But I have tortured you enough with ORs and predicted probabilities are much more intuitively interpreted. Therefore, we will calculate them in the next section.",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#predicted-probabilities",
    "href": "session3/session3.html#predicted-probabilities",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.7 Predicted Probabilities",
    "text": "3.7 Predicted Probabilities\nYou all hopefully still remember the idea of predicted probabilities which we have already seen last time for a simply logistic regression. You hold all but one predictor variables (IVs) constant at their mean or another logical value. The one predictor which you do not hold constant you let alternate/vary to estimate the predicted probabilities of this specific variable of interest and the different values it can take on (on your dependent variable). The predicted probabilities can be tricky to code manually and we are not going to do this again but we will use a package that can do this for us.\nThe package is called MNLpred and allows us to specify the variable of interest. This packages makes draws from our posterior distribution (hello Bayesian statistics) and simulates our coefficients n-times (we tell it how many times to run the simulation) and then takes the mean value of all of our simulations. This way, we end up more or less with the same predicted probabilities that we have seen last week. These are much more easily interpreted than relative risk ratios (the odds-ratios of multinomial regressions) and can be plotted.\n\nlibrary(MNLpred)\npred1 &lt;- mnl_pred_ova(\n  model = model_de,\n  # specify data source\n  data = ess_clean,\n  # specify predictor of interest\n  x = \"imueclt\",\n  # the steps which should be used for the simulated prediction\n  by = 1,\n  # this would be for replicability, we do not care about it\n  # here\n  seed = \"random\",\n  # number of simulations\n  nsim = 100,\n  # confidence intervals\n  probs = c(0.025, 0.975)\n)\n\nMultiplying values with simulated estimates:\n================================================================================\nApplying link function:\n================================================================================\nDone!\n\n\nThe pred1 object now contains the simulated means for each party at each step of our predictor of interest meaning that there are 10 simulated mean values for each value that imueclt can take on for each party:\n\npred1$plotdata |&gt; head()\n\n  imueclt vote_de       mean      lower      upper\n1       0 CDU/CSU 0.24577468 0.17933183 0.31697670\n2       1 CDU/CSU 0.19087444 0.14598554 0.24115988\n3       2 CDU/CSU 0.14345658 0.11489843 0.17606513\n4       3 CDU/CSU 0.10421451 0.08557876 0.12364891\n5       4 CDU/CSU 0.07315527 0.05955206 0.08559652\n6       5 CDU/CSU 0.04965030 0.03889730 0.05969353\n\n\nLet’s simulate the exact same thing for our second hypothesis regarding the trust in politicians:\n\npred2 &lt;- mnl_pred_ova(\n  model = model_de,\n  data = ess_clean,\n  x = \"trstplt\",\n  by = 1,\n  seed = \"random\",\n  nsim = 100,\n  probs = c(0.025, 0.975)\n)\n\nMultiplying values with simulated estimates:\n================================================================================\nApplying link function:\n================================================================================\nDone!\n\n\nThe results, which we have both stored respectively in the objects pred1 and pred2 can be used for a visualization with ggplot().\n\nlibrary(ggplot2)\nggplot(data = pred2$plotdata, aes(\n  x = trstplt,\n  y = mean,\n  ymin = lower,\n  ymax = upper\n)) +\n  # this gives us the confidence intervals\n  geom_ribbon(alpha = 0.1) +\n  # taking the mean of the values\n  geom_line() +\n  # here we display the predicted probabilities for all parties in one plot\n  facet_wrap(. ~ vote_de, ncol = 2) +\n  # putting the values of the y-axis in percentages\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  # the x-axis follows the 0-10 scale of the predictor\n  scale_x_continuous(breaks = c(0:10)) +\n  # specifying the ggplot theme\n  theme_minimal() +\n  # lastly you only need to label your axes; Always label your axes ;)\n  labs(y = \"Predicted probabilities\",\n       x = \"Trust in Politicians\") \n\n\n\n\n\n\n\nHere we can see very well by how many percent the likelihood increases or decreases for each party given that our independent variable, our predictor, of trust in politicians increases (increasing values mean more trust in politicians).\nWe can also visualize our predicted probabilities in one single plot. I made the effort of coordinating the colors so that they would be displayed in the colors of the parties. If you want to have a color selector to get the HEX color codes, you can click on this link: (it will say Google Farbwähler, which is not a scam but German…). As by recently, R will also display the color you have selected.\n\nggplot(data = pred2$plotdata, aes(\n  x = trstplt,\n  y = mean,\n  color = as.factor(vote_de)\n)) +\n  geom_smooth(aes(ymin = lower, ymax = upper), stat = \"identity\") +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_x_continuous(breaks = c(0:10)) +\n  scale_color_manual(\n    values = c(\n      \"#03c2fc\",\n      \"#000000\",\n      \"#f26dd5\",\n      \"#FFFF00\",\n      \"#00e81b\",\n      \"#fa0000\"\n    ),\n    name = \"Vote\",\n    labels = c(\"AFD\", \"CDU\", \"DIE LINKE\", \"FDP\",\n               \"GRUENE\", \"SPD\")\n  ) +\n  ylab(\"Predicted Probability Vote\") +\n  xlab(\"Trust in Politicians\") +\n  theme_minimal()\n\n\n\n\n\n\n\nThis here is the plot for our first hypothesis for which we have stored the predicted probabilities in the object pred1:\n\nlibrary(ggplot2)\nggplot(data = pred1$plotdata, aes(\n  x = imueclt,\n  y = mean,\n  ymin = lower,\n  ymax = upper\n)) +\n  geom_ribbon(alpha = 0.1) + # Confidence intervals\n  geom_line() + # Mean\n  facet_wrap(. ~ vote_de, ncol = 2) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + # % labels\n  scale_x_continuous(breaks = c(0:10)) +\n  theme_minimal() +\n  labs(y = \"Predicted probabilities\",\n       x = \"Positivity towards Immigrants\") # Always label your axes ;)\n\n\n\n\n\n\n\nAnd here the code which puts all the predicted probabilities in one plot:\n\nggplot(data = pred1$plotdata, aes(\n  x = imueclt,\n  y = mean,\n  color = as.factor(vote_de)\n)) +\n  geom_smooth(aes(ymin = lower,\n                  ymax = upper),\n              stat = \"identity\") +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_x_continuous(breaks = c(0:10)) +\n  scale_color_manual(\n    values = c(\n      \"#03c2fc\",\n      \"#000000\",\n      \"#f26dd5\",\n      \"#FFFF00\",\n      \"#00e81b\",\n      \"#fa0000\"\n    ),\n    name = \"Vote\",\n    labels = c(\"AFD\", \"CDU\", \"DIE LINKE\", \"FDP\",\n               \"GRUENE\", \"SPD\")\n  ) +\n  ylab(\"Predicted Probability Vote\") +\n  xlab(\"Positivity towards Immigration\")",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#diagnostics-of-multinomial-models",
    "href": "session3/session3.html#diagnostics-of-multinomial-models",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.8 Diagnostics of Multinomial Models",
    "text": "3.8 Diagnostics of Multinomial Models\nI have talked about diagnostics of models before. This will be the first time that we really touch upon that in models that are not linear like OLS. Usually this is a step which you should take between the building and the final interpretation of your model.\nThe estimates of your model change depending on several influences. The number of predictors, the scaling of your predictors, the scaling of your dependent variable or the coding of your dependent variable. All these kind of things (and many more) will have an effect on your model’s results. We need to be sure that we have a good amount of variables to account for enough variance. But we also need to make sure that we do not overfit our model, meaning that we put in too many predictors for example. We also need to make sure that our model is not biased by the scaling of our variables. This is why we need to check for multicollinearity, heteroskedasticity and other things.\nWe are firstly concerned with the goodness of fit of our model. In a linear model using the OLS method, we have looked at the \\(R^2\\) and adjusted \\(R^2\\) of the models. This tells us something about how much variance of the DV is explained by our IVs. Unfortunately, this measure does not exist for logistic or multinomial models. But the good news is that we can calculate something that is called McFadden’s Pseudo \\(R^2\\). It is interpreted in a similar way as you would do it with a normal \\(R^2\\) meaning that anything ranging between 0.2 and 0.4 is a result that should make us happy.\nThis is how you do this in R:\n\n# you obviously need to install the package first\nlibrary(pscl)\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\npR2(model_de)\n\nfitting null model for pseudo-r2\n# weights:  12 (5 variable)\ninitial  value 2512.046776 \niter  10 value 2158.240962\niter  10 value 2158.240953\niter  10 value 2158.240953\nfinal  value 2158.240953 \nconverged\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1925.8149021 -2158.2409526   464.8521011     0.1076924     0.2821995 \n         r2CU \n    0.2958110 \n\n\n\n3.8.1 Hetereoskedasticity and Multicollinearity\nThen there are issues of scary words like multicollinearity or heteroskedasticity (oftentimes also refered to as “heteroske-something”). These two things describe two phenomena that can skew our estimations and, in the worst case scenario, will lead to wrong inferences. Therefore, we must check for them in all different kinds of models, be it a simple model using the OLS method, or a logistic regression or a multinomial regression. There are ways to test for potential problems that might arise and also ways to work our way around them if ever we encounter them.\n\n3.8.2 Multicollinearity and how to eliminate it\nFor now, we will only look at the potential issue of multicollinearity. It occurs when your independent variables are correlated among each other. This means that they vary very similarly in their values and measure either similar things or measure things the same way. The higher the multicollinearity within your model, the less reliable are your statistical inferences.\nWe can detect the amount and measure of (multi)collinearity by calculating the Variance Inflation Factor (VIF). It measures the amount of correlation between our predictors. The VIF should be below 10. If it is below 0.2, this is a potential problem. Anything below 0.1 should have us really worried. To do this in R, we use the vif() function of the car package. However, it does not work on the object of a multinomial model. Thus, we cheat our way around it and build a GLM model (glm()) in which we set our DV as factors and pretend that they are binomially distributed. This way, R sort of manually calculates the individual logistic regressions according to a baseline and we can calculate the VIF for the IVs individually.\n\nmodel_vif &lt;-\n  glm(\n    as.factor(vote_de) ~ imueclt  + stflife + trstplt + blgetmg +\n      gndr + yrbrn + eduyrs + hinctnta,\n    data = ess_clean,\n    family = binomial()\n  )\n\ncar::vif(model_vif)\n\n imueclt  stflife  trstplt  blgetmg     gndr    yrbrn   eduyrs hinctnta \n1.350982 1.141501 1.274526 1.013258 1.037425 1.112390 1.260009 1.202217 \n\n\nBased on the results, we can see that our variance is not inflated since all values are below 10. That is great news! A VIF of 1 means that there is no correlation within our predictors, a VIF between 1 and 5 (which is quite normal) indicates slight correlation, and a VIF between 5 and 10 shows a strong correlation.\nIf, however, you should encounter issues of multicollinearity, you should test the VIFs for different versions of your model by starting to drop the IV with the highest VIF and see how that affects your VIFs overall. Or you check the variables which have high values, see if theoretically speaking they measure similar things, and combine them into a single measure.\nIt is important to do this step in order to test the validity and reliability of our models!",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#goodness-of-fits-and-its-other-measures",
    "href": "session3/session3.html#goodness-of-fits-and-its-other-measures",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.9 Goodness of Fits and its other measures",
    "text": "3.9 Goodness of Fits and its other measures\nYou have seen me use the term goodness of fit before and that this becomes very important in quantitative research when you try to model statistical relationships. Until now, we have always only modeled one model and then interpreted its coefficients and model values. We have seen the \\(R^2\\) and adjusted \\(R^2\\) and we have mostly seen bad OLS models which showed very low values in both these measures. However, this measure does not always exist for generalized linear models. Thus, statisticians have come up with other ways to compare models and their goodness of fit. As a rule of thumb, we should always favor models which explain as much as possible by not making too many (strong) assumptions and overfitting our predictors, e.g. adding too many in one regression etc. In one of your introduction to (political) science classes, you might have heard of Ockham’s razor; this is the same idea but for statistical models.\nGoodness of fit in our case refers to how well the model which we have constructed, fits the set of our made observations. Thus, goodness of fit somewhat measures the discrepancy between our observed and expected values given our model. If we do not have an adjusted \\(R^2\\), we need to use other information criteria to determine which model fits best our data. There is quite an abundance of criteria which come to mind. Some of them are related to specific kinds of statistical models, whereas some are more general. The two which I would like to mention here are the AIC and the BIC.\n\n3.9.1 AIC (Akaike Information Criterion)\nDon’t be like me and think for years AIC was a bad abbreviation of Akaike. It actually stands for Akaike Information Criterion. It is calculated based on the number of predictors of our model and how well it reproduces our data (the likelihood estimation). If you go back to our multinomial regressions above, you can see that the the last line of our table shows the AIC for this model. Individually, this information criterion is meaningless. It becomes important when we compare it to an AIC of a similar model and check which one indicates a better fit.\nWhat would a similar model look like? Well, if we dropped one of our IVs for example, we would alter the model a bit but keep its global structure. In that case, we would generate a second but different AIC. Comparing the AIC then tells us something about which model (meaning which composition of model) indicates a better fit.\nWhat is a better AIC? The lower AIC indicates that the model fits our data better than the model with the higher AIC. This is simply a mathematical measure. Stand-alone values of the AIC do not tell us much. They need to be considered in comparison to other values.\n\n3.9.2 BIC (Bayesian Information Criterion)\nBayesian Statistics 1 are super fascinating and I will include them wherever I can. Luckily, the BIC is very common and is to the AIC what the adjusted \\(R^2\\) is to the \\(R^2\\). This means that it is a “stricter” measure of goodness of fit than the AIC. It is quite similar to the AIC but differs in that it penalizes you for adding “less useful” variables to your model (potentially overfitting or overcomplexifing your model). Thus, similarly to the AIC, we should also favor the lower BIC!",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#apis-and-fun-data-visualization-optional",
    "href": "session3/session3.html#apis-and-fun-data-visualization-optional",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.10 APIs and Fun Data Visualization (OPTIONAL!)",
    "text": "3.10 APIs and Fun Data Visualization (OPTIONAL!)\nIn this session’s optional section, I will introduce you to the logic of APIs and how to use them in R. Further, we will use data from Spotify (which we will collect through their API) to make fun data visualizations!\n\n3.10.1 What is an API?\nAPI stands for Application Programming Interface. It is a way for two applications to communicate with each other. This is a very technical way of saying that an API allows us to carefully and gently collect data from a server, a website or a database. APIs are a very common way of collecting data from websites. Most websites use internal APIs to communicate with their own databases 2, whereas some websites also offer public APIs which allow you to collect data from their website.\nThink about APIs like this: You go to a restaurant and you want to eat. You could be very rude, ignore every other customer, ignore every social norm you have ever learned and go to the kitchen to tell the chef what you want. Now, if only one person did this, nothing would happen (even if I were surprised if the chef would take your order). But if now every customer did this, started yelling their order at the chef, he would probably just stop working and throw in the towel, or worse, throw you out of the restaurant. To avoid that, there are waiters and waitresses that usually come to your table, take your order, deliver that to the kitchen and once your dish is ready, you will get it to your table. Transposing this onto APIs, the waiter/waitress is our API. They make sure that we can communicate with the kitchen (the database) without disrupting the work of the chef (the server).\nUsually APIs are win-win situations for us and for them. They can control how much data we collect from them, they can dictate the rules and have traces of what we were doing when and where. And we can collect data without disrupting their work. However, sometimes APIs are not public and we need to find other ways to collect data from websites. This is called webscraping and is a bit more complicated and in the appendix.\nSome APIs are more useful than others. The New York Times’ one is unfortunately quite useless. Spotify’s API is more fun, as we will see in a bit. Twitter used to have one of the best developer’s accesses to their data through a wonderful API until he-who-must-not-be-named destroyed the platform. But also government websites have APIs which make it a bit more regulated to access their data.\n\n3.10.2 Spotify API\nFor the sake of the example, we will use the Spotify API to communicate with the Spotify servers and ask them for data. You usually have to sign up for an API on the respective website. If you want to reproduce my code, you will have to sign up for a Spotify API here. Once you have signed up, you will be able to create a new app. This will give you a client ID and a client secret. These are your credentials to access the API. You will need them to access the API.\n\nneeds('spotifyr',\n      'tidyverse',\n      'plotly',\n      'ggimage',\n      'httpuv',\n      'httr', \n      'usethis')\n\n\n3.10.3 System Enivronments and API keys\nWe will get a bit more technical and speak about good practices in R for a second. When you apply to an API or sign up for an access, you will usually get a sort of key, clientID or something to authenticate yourself whenever you make requests to the API. This way, they know that it is you who is making requests and that you are validated by them. This information is sensitive! You do not share it with anyone, and you never show this to anyone. This is why you need to store it somewhere safe. One of the options we have in R is to save them in our system environment. It is a file in which we can store these things. Honestly, the keys to APIs are really long and complicated and you do not want to type them in every time you want to use them. So, we will store them in our system environment. This is a bit more complicated than just typing them in, but it is a good practice. It is also not the safest way to do it but for simplicity’s sake we will do it like this for now.\n\n\n\n\n\n\nIf you want to follow my code and run it on your end, you will have to first sign up to the Spotify developer’s portal, generate the key and clientID and come back to my script then.\n\n\n\nThis line of code opens up a file called .Renviron in which we can store our keys. If you have never done this before, it will open up a new file. If you have done this before, it will open up the file in which you probably already have things stored.\n\nusethis::edit_r_environ()\n\nOnce you have opened the file, you can add the following lines to it. You will have to replace the XXXX with your own keys. Once you have done that, save the file and restart R! This is necessary for the changes to take effect.\n\nSPOTIFY_CLIENT_ID = \"XXXX\"\nSPOTIFY_CLIENT_SECRET = \"XXXX\"\n# this has to be the same localhost as in indicated in the spotify developer portal\nSPOTIFY_REDIRECT_URI = \"http://localhost:1410/\"\n\nNow that we have added the sensitive information to our R Environment, it will remain there until we decide to delete it. If we need to load it now, we can run these lines of code:\n\nclient_id &lt;- Sys.getenv(\"SPOTIFY_CLIENT_ID\")\nclient_secret &lt;- Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\nredirect_uri &lt;- Sys.getenv(\"SPOTIFY_REDIRECT_URI\")\n\n\n3.10.4 Getting the data from Spotify\nThis is where the technical part is over and we can finally turn to the more fun side of things and collect data from Spotify. We will use the spotifyr package to do so. This package is a wrapper for the Spotify API. It makes it easier for us to collect data from Spotify. It has been developed by Charlie Thompson and is a great example of how to use APIs in R.\nAgain, credit where credit is due: When I first discovered the Spotify API, I got a lot of inspiration from this chapter by Marie-Lou Sohnius and Johanna Mehltretter.\nStore your credentials in an object. This will only work if you have properly saved your Spotify access keys and credentials to your system environment.\n\naccess_token &lt;- get_spotify_access_token()\n\nWe will use the get_playlist_audio_features(). This function will collect all the information of a specific Spotify playlist. There is an insane amount of information that we can already extract with this. We will use the playlist of the top 50 songs in the France You can find the playlist here. The playlist has a unique identifier, called a URI (= Uniform Resource Identifier which is a sequence that identifies resources found on the internet). To find the URI, you simply open the Spotify web browser, log in, and go to any playlist of your choice. The URL for the Top 50 of France is https://open.spotify.com/playlist/37i9dQZEVXbIPWwFssbupI. The URI is the last part of the URL: 37i9dQZEVXbIPWwFssbupI. Below, we simply feed it into the function and can store all the information about the Top 50 songs in an object.\n\ntop50 &lt;- get_playlist_audio_features(playlist_uris = '37i9dQZEVXbIPWwFssbupI') \n\nWe can have a look at the data. It is a tibble with 50 rows and 61 columns. Each row is a song and each column is a variable. We have the name of the song, the artist, the album, the URI, the danceability, the energy, the key, the loudness, the mode, the speechiness, the acousticness, the instrumentalness, the liveness, the valence, the tempo, the duration, the time signature and the date of release. All in all, quite some data.\n\nglimpse(top50)\n\nRows: 50\nColumns: 61\n$ playlist_id                        &lt;chr&gt; \"37i9dQZEVXbIPWwFssbupI\", \"37i9dQZE…\n$ playlist_name                      &lt;chr&gt; \"Top 50 - France\", \"Top 50 - France…\n$ playlist_img                       &lt;chr&gt; \"https://charts-images.scdn.co/asse…\n$ playlist_owner_name                &lt;chr&gt; \"Spotify\", \"Spotify\", \"Spotify\", \"S…\n$ playlist_owner_id                  &lt;chr&gt; \"spotify\", \"spotify\", \"spotify\", \"s…\n$ danceability                       &lt;dbl&gt; 0.472, 0.794, 0.702, 0.545, 0.561, …\n$ energy                             &lt;dbl&gt; 0.471, 0.578, 0.693, 0.524, 0.604, …\n$ key                                &lt;int&gt; 10, 1, 5, 1, 9, 1, 4, 5, 10, 1, 6, …\n$ loudness                           &lt;dbl&gt; -5.692, -5.932, -4.941, -10.072, -4…\n$ mode                               &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0,…\n$ speechiness                        &lt;dbl&gt; 0.0603, 0.2500, 0.1910, 0.0322, 0.0…\n$ acousticness                       &lt;dbl&gt; 0.1510, 0.1030, 0.1390, 0.5850, 0.1…\n$ instrumentalness                   &lt;dbl&gt; 0.00e+00, 1.24e-05, 0.00e+00, 0.00e…\n$ liveness                           &lt;dbl&gt; 0.1400, 0.3630, 0.0702, 0.0744, 0.1…\n$ valence                            &lt;dbl&gt; 0.2190, 0.9670, 0.5720, 0.2760, 0.2…\n$ tempo                              &lt;dbl&gt; 105.029, 126.032, 87.973, 145.876, …\n$ track.id                           &lt;chr&gt; \"6tNQ70jh4OwmPGpYy6R2o9\", \"1eldTykr…\n$ analysis_url                       &lt;chr&gt; \"https://api.spotify.com/v1/audio-a…\n$ time_signature                     &lt;int&gt; 3, 4, 4, 4, 3, 5, 4, 4, 4, 4, 4, 4,…\n$ added_at                           &lt;chr&gt; \"2024-03-18T10:29:48Z\", \"2024-03-18…\n$ is_local                           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ primary_color                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ added_by.href                      &lt;chr&gt; \"https://api.spotify.com/v1/users/\"…\n$ added_by.id                        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ added_by.type                      &lt;chr&gt; \"user\", \"user\", \"user\", \"user\", \"us…\n$ added_by.uri                       &lt;chr&gt; \"spotify:user:\", \"spotify:user:\", \"…\n$ added_by.external_urls.spotify     &lt;chr&gt; \"https://open.spotify.com/user/\", \"…\n$ track.artists                      &lt;list&gt; [&lt;data.frame[1 x 6]&gt;], [&lt;data.fram…\n$ track.available_markets            &lt;list&gt; &lt;\"AR\", \"AU\", \"AT\", \"BE\", \"BO\", \"BR…\n$ track.disc_number                  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ track.duration_ms                  &lt;int&gt; 180304, 217259, 233240, 174106, 210…\n$ track.episode                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ track.explicit                     &lt;lgl&gt; FALSE, TRUE, TRUE, FALSE, FALSE, FA…\n$ track.href                         &lt;chr&gt; \"https://api.spotify.com/v1/tracks/…\n$ track.is_local                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ track.name                         &lt;chr&gt; \"Beautiful Things\", \"Petit génie\", …\n$ track.popularity                   &lt;int&gt; 100, 82, 79, 80, 93, 69, 78, 90, 73…\n$ track.preview_url                  &lt;chr&gt; \"https://p.scdn.co/mp3-preview/e213…\n$ track.track                        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,…\n$ track.track_number                 &lt;int&gt; 1, 1, 11, 1, 2, 4, 3, 1, 4, 6, 5, 1…\n$ track.type                         &lt;chr&gt; \"track\", \"track\", \"track\", \"track\",…\n$ track.uri                          &lt;chr&gt; \"spotify:track:6tNQ70jh4OwmPGpYy6R2…\n$ track.album.album_type             &lt;chr&gt; \"single\", \"single\", \"album\", \"singl…\n$ track.album.artists                &lt;list&gt; [&lt;data.frame[1 x 6]&gt;], [&lt;data.fram…\n$ track.album.available_markets      &lt;list&gt; &lt;\"AR\", \"AU\", \"AT\", \"BE\", \"BO\", \"BR…\n$ track.album.href                   &lt;chr&gt; \"https://api.spotify.com/v1/albums/…\n$ track.album.id                     &lt;chr&gt; \"29aSKB1qPEbN0Qf9OPSQpw\", \"4Ta3fRze…\n$ track.album.images                 &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.fram…\n$ track.album.name                   &lt;chr&gt; \"Beautiful Things\", \"Petit génie\", …\n$ track.album.release_date           &lt;chr&gt; \"2024-01-18\", \"2023-08-04\", \"2023-1…\n$ track.album.release_date_precision &lt;chr&gt; \"day\", \"day\", \"day\", \"day\", \"day\", …\n$ track.album.total_tracks           &lt;int&gt; 1, 1, 12, 1, 10, 16, 17, 1, 12, 7, …\n$ track.album.type                   &lt;chr&gt; \"album\", \"album\", \"album\", \"album\",…\n$ track.album.uri                    &lt;chr&gt; \"spotify:album:29aSKB1qPEbN0Qf9OPSQ…\n$ track.album.external_urls.spotify  &lt;chr&gt; \"https://open.spotify.com/album/29a…\n$ track.external_ids.isrc            &lt;chr&gt; \"USWB12307016\", \"QMBZ92393247\", \"FR…\n$ track.external_urls.spotify        &lt;chr&gt; \"https://open.spotify.com/track/6tN…\n$ video_thumbnail.url                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ key_name                           &lt;chr&gt; \"A#\", \"C#\", \"F\", \"C#\", \"A\", \"C#\", \"…\n$ mode_name                          &lt;chr&gt; \"major\", \"minor\", \"minor\", \"major\",…\n$ key_mode                           &lt;chr&gt; \"A# major\", \"C# minor\", \"F minor\", …\n\n\n\n3.10.5 Preparing the data\nFor any further operation, we will first have to create a rank, or identifier for each song. In the next step, we use a simple for-loop to extract the artist and the image of the album cover. We will use the ggimage package to plot the album covers.\n\ntop50$rank &lt;- seq.int(nrow(top50))\n\n\nfor (i in 1:50) {\n  top50$artist[i] &lt;- top50[[28]][[i]]$name\n  top50$image[i] &lt;- c(top50[[48]][[i]]$url[2], size=10, replace = TRUE)\n}\n\n\n3.10.6 Plotting the data\nUnfortunately, Spotify has no documentation on how they measure or decide which song is more lively, acoustic or danceable than others. But we can probably get the gist of it by listening to the songs. Below we can plot the songs by their danceability and happiness. The happier the song, the more to the right it is. The more danceable the song, the higher it is. We can see that there seems to be a linear relationship between danceability and happiness, which does not necessarily come as a surprise.\n\nggplot(data = top50, aes(x = valence, y = danceability, text = (\n  paste(\"Track:\",\n        track.name,\n        \"&lt;br&gt;\",\n        \"Artist:\",\n        artist)\n))) +\n  geom_image(aes(image = image), asp = 1.5) +\n  theme_minimal() +\n  ylab(\"Danceability\") +\n  xlab(\"Happiness\") \n\n\n\n\n\n\n\nBut we can also see that there are some outliers. There are some songs that are very happy but not very danceable. And there are some songs that are very danceable but not very happy. We can have a look at these songs.\nUsing the ggridges package we can study the distribution of some of our variables. The code below extracts data on all Taylor Swift albums and songs. We can then plot the distribution of the happiness of her songs:\n\nlibrary(ggridges)\n\ntaylor &lt;- get_artist_audio_features('taylor swift')\n\ntaylor_filtered &lt;- taylor |&gt;\n  filter(\n    album_name %in% c(\n      \"Speak Now (Taylor's Version)\",\n      \"evermore\",\n      \"Fearless (Taylor's Version)\",\n      \"folklore\",\n      \"Midnights (3am Edition)\",\n      \"Red (Taylor's Version)\",\n      \"Taylor Swift\",\n      \"reputation\",\n      \"1989 (Taylor's Version)\",\n      \"Lover\"\n    )\n  ) |&gt;\n  arrange(album_release_year)\n\nggplot(taylor_filtered,\n       aes(x = valence, y = album_name, fill = after_stat(x))) +\n  geom_density_ridges_gradient() +\n  scale_fill_viridis_c(name = \"Happiness\", option = \"C\") +\n  labs(title = \"Joyplot of Taylor Swift Albums (Based on Spotify's valence value)\",\n       subtitle = \" Built using the R-package spotifyr\") +\n  theme_minimal() +\n  ylab(\"Album Name\") +\n  xlab(\"Happiness score according to Spotify\")\n\nPicking joint bandwidth of 0.0866\n\n\n\n\n\n\n\n\nHere is the danceability of her albums:\n\nggplot(taylor_filtered,\n       aes(x = danceability, y = album_name, fill = after_stat(x))) +\n  geom_density_ridges_gradient() +\n  scale_fill_viridis_c(name = \"Danceability\", option = \"F\") +\n  labs(title = \"Danceplot of Taylor Swift Albums\",\n       subtitle = \" Built using the spotifyr package\") +\n  theme_minimal() +\n  ylab(\"Album Name\") +\n  xlab(\"Danceability score according to Spotify\")\n\nPicking joint bandwidth of 0.0449",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#footnotes",
    "href": "session3/session3.html#footnotes",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "The second school of doing statistics. What we are doing is called frequentist statistics. Bayesian statistics are a bit more complicated and require a different way of thinking about statistics but they are the more intuitive way of conducting statistics (without p-values but with something you could call certainty). They are also more computationally expensive and thus, have only gained traction over the recent decades. They are gaining more and more popularity. If you are interested in learning more about them, I recommend the book by Richard McElreath (2016) Statistical Rethinking: A Bayesian Course with Examples in R and Stan. He also has lectures on YouTube that follow the book. Definitely worth a try!↩︎\nThis is a fun exercise for webscraping and you can imitate the websites’ calls to their server API to collect data but more on that at a later point.↩︎",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session6/session6.html",
    "href": "session6/session6.html",
    "title": "\n4  Time Series\n",
    "section": "",
    "text": "4.1 Introduction\nThis week’s R session will introduce you to the logic of time series cross section in RStudio. Be aware that just because it sounds very impressive does not mean that we need to be in awe. It is rather straight forward and once you can wrap your head around it, you will see that – as always – the actual coding part is not that complicated (except but… we still have to manage our data).\nThis week’s session will also introduce you to a dataset which you have not seen: the Comparative Manifesto Project (CMP) or also referred to as Manifesto Research on Political Representation. If you are only interested in party positions based on their manifestos, and thus only in intervals of 4-5 years, it is a great source for everything that has to do with party positions and political competition over time, within party families or within several countries – or everything all at once.\nThe idea of the CMP is that individual human-coders split the party manifestos for each party at each election into quasi-sentences and attribute it to a certain topic. The topics are predefined and go back to the saliency theory of Budge and Farlie (1983). In their pre-definition, they are also attributed to either left-wing politics or right-wing politics. The idea of the saliency theory is that parties will only emphasize those issues which are favorable to them. Therefore, we can place parties in a (one- or twodimensional) space. Examining these estimates of party positions over time can give us insights on party competition.\nI could spend hours talking about how to model party competition. But according to Jan, I spend way too much of my time thinking about models and operationalizations,1 which is why I will not bother you with my thoughts on the CMP and its advantages or inconveniences. Let me say this at least: the CMP is a powerful and widely used database; it relies, however, on very very strong assumptions, which can be problematic (imho). If you are interested in party competition and party positions, feel free to contact me or come see me after class. Also, here are some references, which you might want to consult to check out scientific publications using the CMP or articles evaluating the validity of the CMP’s measures; for different uses of the CMP: Green and Hobolt (2008) or Adams and Somer-Topcu (2009); for critical evaluations of the CMP: Dinas and Gemenis (2010), Gemenis (2013), Ruedin and Morales (2019).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html",
    "href": "appendix/rmarkdown.html",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "",
    "text": "A.1 What are Quarto & RMarkdown?\nQuarto is an open source tool that allows you to combine code, its outputs, with text in order to publish reproducible and high quality scripts and documents. You can include several (different) programming languages such as R, Python, Julia or others in either the same or single scripts.\nBefore quarto, Markdown was the standard. RMarkdown is a version that is specifically tailored to R. If you know how to use tools like Obsidian, you might already be familiar with the Markdown syntax. Similarly to Quarto, it is a framework in which you can code in R, document your code, annotate it with text and present your research, graphs and models to others. It generates documents which are not only nice to look at but also the best way to present your quantitative work to others. Quarto acts as a more powerful wrapper around Markdown. Once you have understood the syntax, and believe me when I say that it is not tricky, you might even choose to take your notes in the Markdown format. Every Markdown document needs to be knitted. This will transform your text, code and the commands (which you have told R to do) into either HTML, PDF or Word documents. This is what we call the output of your Markdown document.\nIn the Markdown language, you have to explain to the computer what you want it to show in the output. Unlike how we would see it in Microsoft Word or Pages, you do not have any buttons which allow you to write in italic or bold letters. You need to let R know, via certain commands that I will present to you below, what it is supposed to do with the text and code you have written.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#getting-started",
    "href": "appendix/rmarkdown.html#getting-started",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.2 Getting started",
    "text": "A.2 Getting started\nIn order to get started with Quarto, you first need to install it from here. Then, you will have to open an .qmd file in RStudio. Go to the upper left corner, click on the plus sign on the white page and select Quarto document. A new document will open. It might contain some text which we will go through and you’ll understand in a second. It is possible that you first have to install the R Markdown package from CRAN. To do this, run this line of code in your console:\n\nif(!require(rmarkdown)) \n  install.packages(\"rmarkdown\", repos = \"http://cran.us.r-project.org\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#annotating-text",
    "href": "appendix/rmarkdown.html#annotating-text",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.3 Annotating text",
    "text": "A.3 Annotating text\nHere are the most common things you might want to do with your text. These are commands that must be put before and after the words or sentences that you want to change. If you are used to coding html, this might seem familiar:\n\n\nitalics can be done with two ** in between which you put the words that should be italic\n\nbold words follow the same principle but putting two ** in front and ** two behind your chunk of text\nif you want to itemize or enumerate something simply use hyphens like this - or “1.”, “2.” etc.\nany headers must be preceded by a #, the more # you add the smaller the header will become; this way you can add up to six different sizes of headers\nany mathematical equations or variables can be written in LaTeX style by putting dollar signs around your text: $y = \\alpha + \\beta_1*x + \\epsilon$ becomes \\(y = \\alpha + \\beta_1*x + \\epsilon\\)\n\n\nThese commands that regard the textual output (anything that is not code in your final Markdown document) might not be that interesting for you (yet). You do not need to know how to write LaTeX equations or perfect Markdown documents. It is simply a quick walk through of what is possible. And after all, we are interested in the final code and a little less about what is italic and bold in your text…",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#including-code",
    "href": "appendix/rmarkdown.html#including-code",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.4 Including code",
    "text": "A.4 Including code\nSince this is a class on coding in R, you will have to include code in your files. This can be done in two ways. You might want to include code in your text like this data &lt;- read_csv(data). This can be done by fitting two accent grave as you would say in French around the piece of code. When I say accent grave it is the thing on top of à and è. Usually when we speak about a package, a function, an argument or a line of code that we want to specifically present, we put it as a piece of code. Thus, when I speak of the tidyverse package or the install.packages() function, we put two accents grave around the words so that they appear the way they appear in this sentence.\nOr you might want to include a whole chunk of code which then gives you the result in the final output:\n\nx &lt;- 2\ny &lt;- 2\nx + y\n\n[1] 4\n\n\nA whole chunk can be added by either typing the chunk delimiters ```{r} and ``` in two seperate lines. It is much easier if you use your keyboard hotkeys to do that. For Mac use Cmd + Option + I and for Windows use Ctrl + Alt + I . This will automatically generate a chunk in which you can write your code. Within this field, you can write your code, run it, assign variables as you would usually do. Sometimes you might not want to include its results, or the warnings or the chunk itself. Here are the ways to do that:\n\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\n\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\n\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\n\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\n\neval = FALSE tells knitr to skip the code in the chunk, but still include the results in the finished file. You can use this if a chunk is very computationally intensive, or if you need to knit the document but the code is not working!\n\nI encourage you to use these different functions at different points. It is no problem if you do not. But for example, when we load our packages that we will need for our R script at the beginning of our code (like this library(tidyverse)), R generates an output in the console which will also appear in the final Markdown document. To prevent the document from showing that, we would have to add the line include = FALSE. It is absolutely fine if you decide to show the code and its result as it is. It might just be a longer document and a little less elegant.\nEven before knitting (producing the final document), you can run the chunks of code. This can be either done on the right side of the chunk by clicking on the “play button” or by simply using your keyboard hotkey that you would usually use to run code. R will immediately show you if there is an issue with your code, just like it would do if this was not a RMarkdown file. The warnings and errors are the same and the troubleshooting process would then also be the same.\nYou are required to put the solutions to the exercises of this class in chunks of codes and discuss them outside of the chunks with some text. You will find instructions within the exercises regarding the interpretation of results or as to why you might have done something in a certain way. These comments must be put outside of the chunks of code. However, if you wish to annotate your code within the chunks, simply use the # as you would do in a normal R file.\n\ndata &lt;- read_csv(\"data.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#knitting-your-document",
    "href": "appendix/rmarkdown.html#knitting-your-document",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.5 Knitting your document",
    "text": "A.5 Knitting your document\nI mentioned earlier that you have to “knit” your Quarto document at the end. RStudio compiles the document for you with all the commands and codes you have written. To knit, you need to click on the icon in your top bar above your code where you can see the knitting needle and yarn. The default setting is to produce a document in html format. However, you can also have a PDF document produced that is knitted in LaTeX style. These documents are the ones you have to hand in to me on the Moodle page for this class.\nHowever, please note that any error in your code will result in the document not being able to be knitted. This means that your code must be correct and work in order to create an html or pdf document. This is because in Markdown, most of the time, your results are displayed and based on each other, just like in any other programming. If one place doesn’t work, most of the later ones won’t work either. But R tells you, at the latest during the knitting process, in which line of your code the problem is.\n\n\n\n\n\n\nIf you want to send me your script because you have to hand in your homework or because you cannot find a solution, it might happen that your code does not work. This then also means that R cannot knit the document to an html or pdf output. In that case, the chunks of code which are creating problems must be set to eval = FALSE to be included but not run! This way, I can see what you have done and where the problem is while having a knitted/rendered document at the same time!\n\n\n\nIf you have made it this far, I really hope that this quick setup tutorial for Quarto has helped you. If not, do not hesitate to get in touch and I’ll try to help! I highly encourage you though to play around with it, see what works and what doesn’t. Most of the time a quick Google search solves most of the problems. However, it is my job to help you, so do not hesitate to reach out!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html",
    "href": "appendix/webscraping.html",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "",
    "text": "B.1 What is webscraping?\nWebscraping is a method that allows us to extract data from websites. Ideally we do this in automated fashion, so that we can collect large amounts of data in a short amount of time.\nIf you want to work with text and analyze it quantitatively, webscraping is a very useful tool. In Computational Social Sciences, we are oftentimes interested in analyzing large corpora of text that an organization, political party or individuals have emitted. If these texts are not yet collected, we have to do this.\nNow, we could start out and do this by hand. But frankly, nobody has time for that and programming is all about making life the easiest for the user and automating everything as much as we can. Nobody wants to click through party press releases, copy paste every item of interest into a spreadsheet and do this for potentially millions of documents.\nWebscraping on the other hand allows us to write a script that does this for us. This script will go to any website out there, find the content we are interested in, extract it and store it in a csv file which we can then use for any other analysis. Data from the Web tends to be unstructured and messy, we might have to do some data cleaning on the text afterwards. But the better we write our “scraper” beforehand, the better the data quality.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#how-are-websites-structured",
    "href": "appendix/webscraping.html#how-are-websites-structured",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.2 How are websites structured?",
    "text": "B.2 How are websites structured?\nWebsites are built with a combination of different languages. You access them by putting in a URL into your browser. URL stands for Uniform Resource Locator and it is somewhat the online adress of any website on the Web. In most cases, when you navigate to a website you are looking at a combination of HTML, CSS and JavaScript. HTML is the structure of the website, CSS is the design and JavaScript is the interactivity. HTML stands for Hyper Text Markup Language and it is the standard language for creating documents designed to be displayed in a web browser. It can be assisted by technologies such as Cascading Style Sheets (CSS) and scripting languages such as JavaScript. But for now, let’s only stick to HTML.\nFor scraping websites, it is essential that we understand the fundamentals of HTML code. Now, please do not be afraid, HTML code is not hard to understand, there are patterns and regularities like in any other programming language and HTML is no C++. The only thing we need to do, is to find out where the content we are interested in is located in the HTML code. If you want to check out any website’s HTML structure (also called a tree), you can do so by right-clicking on the website and selecting “Inspect”. This will open a new window in your browser that shows you the HTML code of the website. Why don’t we take a look at Jan Rovny’s SciencesPo profile. If you go on the website, right-click and go on inspect, you should see something like this:\n\nIf you now wanted, to find something specific on this website, you could either look in the HTML code for it. Or you highlight it on the website and then right-click and select “Inspect”. This will automatically take you to the part of the HTML code that is responsible for the content you have highlighted!\n In this picture, I have first highlighted Jan’s name with my cursor and then inspect the website. Note how the &lt;h1 class=\"title\"&gt; Jan Rovny &lt;/h1&gt; part is highlighted in blue. This is where his name is stored in the HTML code.\nThis is the basic principle of webscraping. We find the content we are interested in and then we write a script that tells the computer to go to the website, find the content and extract it. We will come back to this in a second.\n\nB.2.1 Dynamic and static websites\nGenerally speaking, we can divide websites into two different categories: dynamic and static websites. Static websites are our friends because they are easily scraped. They are built with HTML and CSS and the content is always the same. Dynamic websites on the other hand are a bit more tricky. They are built with HTML, CSS and JavaScript and the content is not always the same.\nLet’s start with static websites. Whatever you do, wherever you scroll on a dynamic website, there are no new panels that appear, no more articles that can be loaded by clicking on a button, the content is always the same. It means that there is no JavaScript running somewhere that makes the website interactive. Static websites are also nice to scrape since they do not require a lot of communication between the website and the websites server where they store their information and data. As a general rule of thumb, scraping processes are usually slowed down on the server’s end, not on ours (provided that our code is efficient of course). Let’s look at the CEE’s website; specifically at that of the doctoral students of my lab. If you go on the website and scroll all the way down, you do not see that it is changing in any way; there are no new elements that appear all of the sudden. This is an example for a static website.\nDynamic websites are a bit more tricky. They are built with HTML, CSS and JavaScript and the content is not always the same. The content can change. This is a problem for us because we want to scrape the content and if the content changes, we have to write a script that can handle this. This is where the RSelenium package comes in. It allows us to scrape dynamic websites. 1 Dynamic websites can change the content displayed to the user based on interactions, user behavior, or inputs without the need to reload the entire page. This interactivity is often powered by AJAX (Asynchronous JavaScript and XML) and APIs that fetch data on demand. If you do not know what this is, simply skip all the technical parts. I just want you to retain that there are ways to scrape these websites as well, but it is a bit more complicated. Imitating APIs that fetch data on demand is not and I will show you further below how to do that. We will leave dynamic website and their annoying JavaScript aside for now and focus on static websites first by looking at the webscraping workflow.\n\nB.2.2 The different html elements\nA node, in web development, refers to any single point in the document tree. This tree represents the structure of a webpage. HTML documents are made up of nodes; these can be element nodes (like &lt;div&gt;, &lt;p&gt;, &lt;a&gt;), text nodes (the actual text within those elements), and even attribute nodes (attributes of elements, like href in an &lt;a&gt; tag).\nThe document tree is hierarchical, resembling a family tree, with branches that represent parent-child relationships (these are not my words, it is used in the web design world). For example, if a\n\ncontains a\n\n(paragraph), the\n\n\nis considered the parent node, and the\n\nis its child node. The\n\n\n\nis also a sibling of any other elements that are children of the same parent.\n\n\n\n\n\n\nThe more you start to scrape websites, the more you will get used to reading the HTML source code. It is not as difficult as it seems at first. However, websites can be terribly messy and badly coded. This is why you will often have to try different things and see what works. Troubleshooting, as in every coding setting, is key.\n\n\n\nWhen scraping a website, you’ll often need to identify specific divs or other elements (nodes) containing the data you wish to extract. Here’s how:\nUse the browser’s Developer Tools (usually accessible by right-clicking on the page and selecting “Inspect” or pressing F12 ) to view the source code and structure of the page – this is what I have shown you based on Jan’s CEE website. This tool highlights the tree structure of HTML documents, showing parent-child relationships. You will get used to reading it, or to finding your way around. One easy way to identify the location of the element you are interested in, is – as indicated above – to highlight it and then to rightclick –&gt; inspect. The HTML source code will automatically open and highlight the part of the code that is responsible for the content you have highlighted.\nOnce you know where, let’s say the title of some website or article you wish to scrape, is stored, you can then try to figure out the path to this specific HTML part, in order to use this path for our code. We will have to specify where our code should go look for our element of interest. Generally speaking, there are two ways to do this: CSS Selectors and XPath.\n\nCSS Selectors: Learn to use CSS selectors, which are patterns used to select the elements you want to style. In web scraping, these selectors help you specify the elements you wish to extract from a webpage. For example, div.article-content p selects all &lt;p&gt; elements inside a &lt;div&gt; with a class of article-content.\nXPath: XPath (XML Path Language) is another powerful tool for navigating through elements and attributes in an HTML document. It allows for more complex queries, like selecting elements based on their content or attributes.\n\nNow these two things sound intimidating, they might also be at first sight, but usually CSS selectors (which are much easier to read but less precise) do the trick. And I hardly ever try to figure this out by hand. There are two things you can do. Assuming you have identified the location of your html content of interest within the source code, you simply right click on that line of html code, and then you can copy the CSS selector or the XPath.\nThe easiest way to find the path to our chunk of interest is the SelectorGadget browser extension. This is a plugin allows you to click on the element you are interested in and it will give you the CSS selector. I will show you how to use it in the next section. I recommend you install it and clip it to the right upper corner where, at least in Chrome, your extensions are listed.\nOnce you have installed the extension, click on it. It will change several things. First, your cursor will now probably create different orange boxes around the elements of the website. Second, you will have sort of a search bar in your lower right corner. If you now click on just some element, it will be highlighted in green and some text will appear in the search bar which had opened up before. Let’s take a look at how this looks:\n\nThis is the CEE’s website where they display the doctoral students. I have only clicked on the word “Doctorants” which is the websites title. In the search bar, the SelectorGadget has now suggested a CSS selector. 2 This is the path to the element of interest. We now know, where, if we were interested in the title of the website, it is stored in the HTML code. You can now copy this path and use it in your code to scrape that specific content.\nLet’s look at another example. Let’s say I am interested in all the names of the doctoral students. I can click on one of the names and the SelectorGadget will give me the CSS selector for that specific element. Here it gives us the tag “a”.\n\nBut see also how it has highlighted other things in yellow as well. This means that the CSS selector is not unique to the element I have clicked on. I am not interested in anything else but the names of my colleagues! For now, it shows “a” in the search bar. That is because the name of the doctoral students is stored in an “a” tag but also other information is stored in some “a” tag. We will have to make sure that we only select the names of the doctoral students. For that, you can still use the Selector Gadget and click on any yellow highlighted content which you are not interested in (i.e. in our case that would be the email addresses or the drop down menus called “Recherche”, “Publications” etc, see picture). We can make sure that we only select the element(s) that we want, if by clicking on the yellow elements we do not want. Clicking on one, can already make the other unimportant ones disappear as well. This should be the case here.\n\nNow, the CSS selector is unique to the names of the doctoral students. You can also see that next to the search bar it says “Clear (35)”. This means that we are still selecting 35 elements with our current tag (in the search bar) called .views-field-title a. Given that we should be about 35 doctoral students at the CEE, this is probably the right path that only captures the names of the PhD students here. This step is an important step of verification. In Computational Social Sciences, we often work with large datasets. If you are interested in scraping the entirety of something (party press releases, a newspaper, parliamentary speeches, you name it), you ideally want to make sure that you have scraped the entirety of the content (available online). Selecting a wrong HTML tag, can lead to you either scraping too little resulting in an incomplete dataset or too much resulting in tedious data cleaning work afterwards.\nWas this complicated and a lot? It probably was. And I understand it. The first time I tried to do this on my own, it was horribly complicated and I did not manage at all. Trust me, this will come with time. And it will become much more intuitive. You should play ariund with the SelectorGadget and try to find the CSS selector for different elements on different websites. If that does not work out, try the other method of going into the source code of the HTML structure, right clicking on the element of interest and then selecting “Copy” –&gt; “Copy selector” or “Copy XPath”.\nMaybe just bear with me and check out the code. Everything will become simpler. The coding part is not necessarily hard when it comes to static websites. The hard part is to find the right CSS selector or XPath and then to validate that you have selected the right elements. Once we properly start scraping in this script, it will become much clearer.\n\n\n\nB.3 Extracting content from websites (rvest)\nFor my first example, we will stick withg the CEE’s website where they display the doctoral students. We will use the rvest package to extract the content from the website. The first thing which we have to do, is to copy paste the URL of the website we want to scrape into the read_html function of the package. This will give us the entire HTML code of that belongs to the website whose URL (remember the online address) we fed to the function. When you copy paste, don’t forget the “https://” as well as the quotation marks around the URL text string. If you simply go into your browser, click on the URL once, it will highlight the “https://” automatically (even though you might not see it).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nread_html(\"https://www.sciencespo.fr/centre-etudes-europeennes/fr/doctorants.html\")\n\n{html_document}\n&lt;html lang=\"fr\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" ...\n[2] &lt;body id=\"top\" class=\"not-front not-logged-in page-doctorants sidebar-rig ...\n\n\nThe output suggests that we have retrieved (“scraped” so to say) the html source code of the website. Feel free to go back to the original website, right click and inspect it to see that these are the same things. We could of course also store the HTML code in a variable. This is often useful if you want to scrape multiple websites and store the HTML code of each website in a separate variable.\n\ncee &lt;- read_html(\"https://www.sciencespo.fr/centre-etudes-europeennes/fr/doctorants.html\")\nglimpse(cee)\n\nList of 2\n $ node:&lt;externalptr&gt; \n $ doc :&lt;externalptr&gt; \n - attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n\n\nNow, we want to extract some content from the website. We want to know the names of the doctoral students. We can do this by using the html_nodes function of the rvest package. This function allows us to extract the content of the website based on the HTML node. Remember that we have used the Selector Gadget above to find the node corresponding to the names of the doctoral students. We can now use this information to extract the names of the doctoral students. For the sake of simplicity and to keep the code clean, I will store the URL in a variable called… url. This will be part of the workflow later on when we start scraping multiple websites.\n\nurl &lt;- \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/doctorants.html\"\n\nread_html(url) |&gt; \n  html_nodes(\".views-field-title a\") \n\n{xml_nodeset (35)}\n [1] &lt;a href=\"chercheur/lennard-alke.html\"&gt;Lennard Alke&lt;/a&gt;\n [2] &lt;a href=\"chercheur/marcela-alonso-ferreira.html\"&gt;Marcela Alonso Ferreira ...\n [3] &lt;a href=\"chercheur/simon-audebert.html\"&gt;Simon Audebert&lt;/a&gt;\n [4] &lt;a href=\"chercheur/meryem-bezzaz.html\"&gt;Meryem Bezzaz&lt;/a&gt;\n [5] &lt;a href=\"chercheur/marius-bickhardt.html\"&gt;Marius Bickhardt&lt;/a&gt;\n [6] &lt;a href=\"chercheur/jan-boguslawski.html\"&gt;Jan Boguslawski&lt;/a&gt;\n [7] &lt;a href=\"chercheur/jean-baptiste-bonnet.html\"&gt;Jean-Baptiste Bonnet&lt;/a&gt;\n [8] &lt;a href=\"chercheur/eva-bossuyt.html\"&gt;Eva Bossuyt&lt;/a&gt;\n [9] &lt;a href=\"chercheur/charlotte-boucher.html\"&gt;Charlotte Boucher&lt;/a&gt;\n[10] &lt;a href=\"chercheur/jens-carstens.html\"&gt;Jens Carstens&lt;/a&gt;\n[11] &lt;a href=\"chercheur/jean-baptiste-chambon.html\"&gt;Jean-Baptiste Chambon&lt;/a&gt;\n[12] &lt;a href=\"chercheur/thalia-creach.html\"&gt;Thalia Creac'h&lt;/a&gt;\n[13] &lt;a href=\"chercheur/pablo-cussac.html\"&gt;Pablo Cussac&lt;/a&gt;\n[14] &lt;a href=\"chercheur/soazig-dollet.html\"&gt;Soazig Dollet&lt;/a&gt;\n[15] &lt;a href=\"chercheur/lea-dornacher.html\"&gt;Lea Dornacher&lt;/a&gt;\n[16] &lt;a href=\"chercheur/maxence-dutilleul.html\"&gt;Maxence Dutilleul&lt;/a&gt;\n[17] &lt;a href=\"chercheur/zoe-evrard.html\"&gt;Zoé Evrard&lt;/a&gt;\n[18] &lt;a href=\"chercheur/maxime-gaborit.html\"&gt;Maxime Gaborit&lt;/a&gt;\n[19] &lt;a href=\"chercheur/leo-grillet.html\"&gt;Léo Grillet&lt;/a&gt;\n[20] &lt;a href=\"chercheur/marie-ines-harte.html\"&gt;Marie Inès Harté&lt;/a&gt;\n...\n\n\nHere we can see that there are 35 nodes that all have a similar structure &lt;a href=\"chercheur/lennard-alke.html\"&gt;Lennard Alke&lt;/a&gt;. Since I only want the text corresponding to the names, we will use the html_text function to extract the names of the doctoral students. The text is stored in the a tag.\n\nread_html(url) |&gt; \n  html_nodes(\".views-field-title a\") |&gt; \n  html_text()\n\n [1] \"Lennard Alke\"            \"Marcela Alonso Ferreira\"\n [3] \"Simon Audebert\"          \"Meryem Bezzaz\"          \n [5] \"Marius Bickhardt\"        \"Jan Boguslawski\"        \n [7] \"Jean-Baptiste Bonnet\"    \"Eva Bossuyt\"            \n [9] \"Charlotte Boucher\"       \"Jens Carstens\"          \n[11] \"Jean-Baptiste Chambon\"   \"Thalia Creac'h\"         \n[13] \"Pablo Cussac\"            \"Soazig Dollet\"          \n[15] \"Lea Dornacher\"           \"Maxence Dutilleul\"      \n[17] \"Zoé Evrard\"              \"Maxime Gaborit\"         \n[19] \"Léo Grillet\"             \"Marie Inès Harté\"       \n[21] \"Emilien Houard-Vial\"     \"Malo Jan\"               \n[23] \"Angeliki Konstantinidou\" \"Thomas Laffitte\"        \n[25] \"Claire Morgane Lejeune\"  \"Chiao Li\"               \n[27] \"Arno Lizet\"              \"Mattia Lupi\"            \n[29] \"Francesco Nardone\"       \"Selma Sarenkapa\"        \n[31] \"Luis Sattelmayer\"        \"Viviane Spitzhofer\"     \n[33] \"Théodore Tallent\"        \"Lucien Thabourey\"       \n[35] \"Marta Tramezzani\"       \n\n\nNow, we have extracted all the names on the website. Feel free to check out who these people are. You should know at least two by now.\nI can tell you though that there are other things stored in our initial HTML node .views-field-title a. We have extracted the text but there is also something called href. This is a hyperlink reference and key for webscraping. If you go back to your browser and on the website we are currently scraping, you will realize that by hovering over our names with your cursor, you can click on them and it will take you to a new website. The information of where your Browser should take you when you click on a name has to be stored somewhere. And well it is stored in the HTML source code as well, within the node we have already identified and to be more precise, the corresponding link (URL) is stored in the href attribute of the a tag. We can extract this information as well as such:\n\nread_html(url) |&gt; \n  html_nodes(\".views-field-title a\") |&gt; \n  html_attr(\"href\") \n\n [1] \"chercheur/lennard-alke.html\"           \n [2] \"chercheur/marcela-alonso-ferreira.html\"\n [3] \"chercheur/simon-audebert.html\"         \n [4] \"chercheur/meryem-bezzaz.html\"          \n [5] \"chercheur/marius-bickhardt.html\"       \n [6] \"chercheur/jan-boguslawski.html\"        \n [7] \"chercheur/jean-baptiste-bonnet.html\"   \n [8] \"chercheur/eva-bossuyt.html\"            \n [9] \"chercheur/charlotte-boucher.html\"      \n[10] \"chercheur/jens-carstens.html\"          \n[11] \"chercheur/jean-baptiste-chambon.html\"  \n[12] \"chercheur/thalia-creach.html\"          \n[13] \"chercheur/pablo-cussac.html\"           \n[14] \"chercheur/soazig-dollet.html\"          \n[15] \"chercheur/lea-dornacher.html\"          \n[16] \"chercheur/maxence-dutilleul.html\"      \n[17] \"chercheur/zoe-evrard.html\"             \n[18] \"chercheur/maxime-gaborit.html\"         \n[19] \"chercheur/leo-grillet.html\"            \n[20] \"chercheur/marie-ines-harte.html\"       \n[21] \"chercheur/emilien-houard-vial.html\"    \n[22] \"chercheur/malo-jan.html\"               \n[23] \"chercheur/angeliki-konstantinidou.html\"\n[24] \"chercheur/thomas-laffitte.html\"        \n[25] \"chercheur/claire-morgane-lejeune.html\" \n[26] \"chercheur/chiao-li.html\"               \n[27] \"chercheur/arno-lizet.html\"             \n[28] \"chercheur/mattia-lupi.html\"            \n[29] \"chercheur/francesco-nardone.html\"      \n[30] \"chercheur/selma-sarenkapa.html\"        \n[31] \"chercheur/luis-sattelmayer.html\"       \n[32] \"chercheur/viviane-spitzhofer.html\"     \n[33] \"chercheur/theodore-tallent.html\"       \n[34] \"chercheur/lucien-thabourey.html\"       \n[35] \"chercheur/marta-tramezzani.html\"       \n\n\nNote how this time we are not using the html_text function but the html_attr function. This function allows us to extract the content of a specific attribute of the HTML node. href is such an attribute.\nNow if you look at the list of elements we have extracted, you will see that they are not complete URLs. They are relative URLs. This means that they are not complete URLs but rather URLs that are relative to the current website. Our extracted URLs are of the form chercheur/malo-jan.html. Websites always need “https://www.” in front of the URL to be complete. In our case, we can look at the website of Malo on here to see what is missing. His actual URL is\n\nhttps://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/malo-jan.html\n\nThis means that we have to add https://www.sciencespo.fr/centre-etudes-europeennes/fr/ to our relative URLs. We can do this by using the str_c function from the stringr package that comes with the tidyverse. More on that package further below. Here it simply adds (concatenates which is where this function gets its c from) the two strings together.\n\ncee_phds &lt;- read_html(url) |&gt; \n  html_nodes(\".views-field-title a\") |&gt; \n  html_attr(\"href\") %&gt;%  # if you are wondering why the %&gt;%, check the note below\n  str_c(\"https://www.sciencespo.fr/centre-etudes-europeennes/fr/\", .)\n\nhead(cee_phds, 5)\n\n[1] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/lennard-alke.html\"           \n[2] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/marcela-alonso-ferreira.html\"\n[3] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/simon-audebert.html\"         \n[4] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/meryem-bezzaz.html\"          \n[5] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/marius-bickhardt.html\"       \n\n\n\n\n\n\n\n\nIn some rare cases, you will have to specify something in long pipes (%&gt;% or |&gt;) that is called a placeholder. Usually in pipes, you do not have to specify the object on which you are doing something anymore. Sometimes you do, however. In our case that is to indicate whether the string of the URL that was missing should be added at the beginning of our relative URLs or at the end. The dot . is a placeholder that tells the function to use the object that was passed to the pipe before the placeholder. And for whatever reason, the Base R placeholder does not work in this case and you have to work around it by using the old magrittr %&gt;% pipe and the corresponding placeholder which is a dot.\n\n\n\nWe now have officially scraped the URLs which lead to the individualy profiles of the CEE’s PhD students.\n\nB.4 What is the webscraping workflow?\nNow, you might be wondering why I emphasized the HTML attribute href so much in the section above. Or why we scraped URLs although I was speaking of content before. The reason is that webscraping is not only about extracting content from websites. It is also about extracting the structure of the website. This is important because it allows us to scrape multiple websites in a structured way. What I showed you on the CEE’s website could technically be done manually. It would take longer by hand, if you know how to code, but still… it could have been done manually. But let’s suppose, I would like to have 10 000 press releases of a party, or scrape all parliamentary speeches of the French Assemblée Nationale. This would be doable manually but it would be terribly chiant (excuse my French).\nIn very very broad terms, webscraping is a two step process. You first collect all the URLs behind which your content of interest is stored, and then you scrape the content behind these URLs. This is the workflow we will follow in the next section. To put it differently, we first need to collect a list of URLs (for which we will build one scraper) which serves us then for our next stept during which we scrape the content with a second scraper.\nThis is where we will have more coding fun and where you might need a refresher of how to write a function and the purrr package (Session 2) of this class. But I will try to talk you to my steps as much as possible. Feel free to go back, however, and look at the code from the previous sessions.\n\nB.4.1 Automated collection of URLS\nIn an ideal world, where web designers are nice people, all websites would have a similar structure. This would mean that we could write one scraper and use it for all websites. But we do not live in an ideal world. Websites are different and we usually have to write a new scraper for each website.\nThe typical example for an introduction to webscraping would be to scrape IMDB. But this is a social science oriented class and we will collect political texts. We will start by harvesting some press releases by the German Social Democratic Party, the SPD. There is no reason as to why this party other than their website is well coded and can easily be scraped. The first thing we need to do is to identify their press release archives. Ideally they have something like this, fortunately they do. If you click on here you can check them out yourselves. And no worries, you do not need to be able to speak German for this task. As a matter of fact, the HTML language and coding in general are both universal enough to bridge language barriers – and in some moments deepl does the trick (but I know that this is no news to any of us).\nThe URL of the press release archive of the SPD is https://www.spd.de/service/pressemitteilungen. First we need to understand the website’s structure to write code that will alternate over each page of their archive and retrieve only those URLs that we are interested in, i.e. the URLs of the press releases. If you click on the link and scroll all the way down, you will see that there is a red circle with a one, another with a two, three dots and then a 111 in another red circle, like in the picture below. 3\n\nThis is a typical pagination structure. It means that the press releases are not all on one page but on multiple pages. I know that all of you have come across this before and that we have all already clicked on these things in our digital lives.\nAnother thing that you might realize, while you have scrolled down, is that the page has remained more or less the same throughout and that nothing new appeared while going down. This is a very good and solid indicator that the website is static (yay).\nNow click on the red two in the circle. This will redirect you to the next page. You will see that the URL has changed to https://www.spd.de/service/pressemitteilungen/page/2. This is a very good sign. If you now scroll down, click on the red arrow next to the 111, you will be redirected to the next page. The URL will be https://www.spd.de/service/pressemitteilungen/page/3. This means that whenever we click on to the next page, the URL changes in a predictable way and we can – very – easily reproduce this in R by creating a list of URLs that we can then have our code use to extract the URLs that are stored on each of “page/3” to the last page. This is an ideal scenario. Sometimes you will have to look a bit more closely for the subtle changes in initial URLs that we need to find to automate our process.\nNow, a quick excursion in some sorting and filtering. We want to make sure that the list of initial URLs, over which we will then scrape in a second step, contains all the URLs. Since we know how the URLs are structured and behave, we can also simply go into our search bar and manually change the number from, let’s say, 3 to 100. If you do this, you will approximately land in 2016 and we will see the display of press releases. To speed up the process, you never want to go one by one, meaning to try out first “page/3”, “page/4”, “page/5” and so on. You want to find the last page as quickly as possible. For that, you randomly type in a large number and see what happens. The worst thing that can happen is that you stumble upon a 404 error; which is just the website telling us that the URL we are trying to navigate to looks like it is on their website but does not exist in reality. Then we know that we will have to try a smaller number to approximate the last page on which the press releases are stored. I suggest you do this in half steps meaning that you always take double or half the number and then see what happens. This is a much faster way to sort or filter things in computer science than increasing/decreasing one step at a time. In our case, I put in 100 and did not get a 404 error. If I now put in the double “https://www.spd.de/service/pressemitteilungen/page/200”, you surprisingly do not get a 404 error. But if you scroll down, we can see that we have reached 111. And that this seems to be the last page of available press releases in the SPD’s archives. Oftentimes, you would have gotten an error message somehow on the website. But from this paragraph, I just want you to take away that it is faster to sort and filter in double steps for the final URL than to go one by one.\n\n\n\n\n\n\nThere are plenty of errors that you can get on a website or on the Web. They are called HTTP response status codes. 404 is one of the most frequent ones but you might encounter others as you scrape more. You do not have to know them. If you see one that is not 404, simply google it and then troubleshoot from there. Here is an overview.\n\n\n\nAlright, let’s finally get our hands on some code. We know that the URLs are structured in a way that we can easily predict the next URL. We also know that they alternate by simply changing the last digit of the URL and that there are 111 URLs in total. We can now create an object that contains all the initial URLs. The code below stores a character string with the root of the URL in an object called intial_url. We then use the str_c function from the stringr package to concatenate the root URL with the numbers 1 to 111. The sep argument is set to \"\" to make sure that there is no space between our initial url root and the numbers we want to add.\n\nintial_url &lt;- \"https://www.spd.de/service/pressemitteilungen/page/\" \ninitial_urls_spd &lt;- str_c(intial_url, 1:111)\n\ninitial_urls_spd |&gt; head()\n\n[1] \"https://www.spd.de/service/pressemitteilungen/page/1\"\n[2] \"https://www.spd.de/service/pressemitteilungen/page/2\"\n[3] \"https://www.spd.de/service/pressemitteilungen/page/3\"\n[4] \"https://www.spd.de/service/pressemitteilungen/page/4\"\n[5] \"https://www.spd.de/service/pressemitteilungen/page/5\"\n[6] \"https://www.spd.de/service/pressemitteilungen/page/6\"\n\n\nAlright, now we have a list of URLs that we can use to scrape the URLs of the individual press releases. For that, we will have to find the URLs behind which the individual press release is stored.\n If you want to navigate to a single press release you have to click on the black button that says “MEHR” (more in German). Right-click on it -&gt; go to inspect -&gt; and the HTML source code will already show us in what node the URL is stored.\n\n\nThe HTML source code corresponding to the MEHR button.\n\nAs for the CEE’s website, it is within an “a” tag and the URL is stored in the “href” attribute. You could of course also use the GadgetSelector extension which will yield the same result. Now let’s apply the same logic as above. For the first example, I will only use the first entry of our initial_urls_spd list:\n\nlibrary(rvest)\ninitial_urls_spd[1] |&gt; \n  read_html() |&gt; \n  html_nodes(\"a\") |&gt; \n  html_attr(\"href\") |&gt; \n  head(40)\n\n [1] \"https://www.spd.de/site/datenschutz/#c38250\"                                                                                                           \n [2] \"https://www.spd.de/site/datenschutz/\"                                                                                                                  \n [3] \"?acceptCookiePolicy=1\"                                                                                                                                 \n [4] \"#main\"                                                                                                                                                 \n [5] \"#footer\"                                                                                                                                               \n [6] \"https://meine.spd.de/\"                                                                                                                                 \n [7] \"https://www.spd.de/suche\"                                                                                                                              \n [8] \"/\"                                                                                                                                                     \n [9] \"/programm\"                                                                                                                                             \n[10] \"/programm/europaprogramm\"                                                                                                                              \n[11] \"/programm/stark-gegen-rechts\"                                                                                                                          \n[12] \"/programm/respekt\"                                                                                                                                     \n[13] \"/programm/gesundheit-und-pflege\"                                                                                                                       \n[14] \"/programm/wohnen\"                                                                                                                                      \n[15] \"/programm/familien\"                                                                                                                                    \n[16] \"/programm/klimaschutz\"                                                                                                                                 \n[17] \"/programm/digitalisierung\"                                                                                                                             \n[18] \"https://www.spd.de/stabile-rente\"                                                                                                                      \n[19] \"/programm/beschluesse\"                                                                                                                                 \n[20] \"/programm/grundsatzprogramm\"                                                                                                                           \n[21] \"/programm/zukunftsprogramm\"                                                                                                                            \n[22] \"/partei\"                                                                                                                                               \n[23] \"/partei/geschichte\"                                                                                                                                    \n[24] \"https://www.spd.de/partei#c75377\"                                                                                                                      \n[25] \"https://www.spd.de/partei/#c75398\"                                                                                                                     \n[26] \"https://www.spd.de/partei/#c75461\"                                                                                                                     \n[27] \"/partei/preise\"                                                                                                                                        \n[28] \"/service\"                                                                                                                                              \n[29] \"/site/kontakt\"                                                                                                                                         \n[30] \"https://www.spd.de/service/#c75578\"                                                                                                                    \n[31] \"https://www.spd.de/service/#m75572\"                                                                                                                    \n[32] \"https://www.spd.de/service/#c75584\"                                                                                                                    \n[33] \"/service/finanzen-und-transparenz\"                                                                                                                     \n[34] \"https://www.spd.de/unterstuetzen\"                                                                                                                      \n[35] \"https://www.spd.de/suche\"                                                                                                                              \n[36] \"/\"                                                                                                                                                     \n[37] \"/service\"                                                                                                                                              \n[38] \"/service/pressemitteilungen/detail/news/nord-sued-neu-denken-veranstaltung-des-geschichtsforums-der-spd-mit-dem-vorsitzenden-lars-klingbeil/12/03/2024\"\n[39] \"/service/pressemitteilungen/detail/news/einladung-klausur-des-spd-parteivorstandes/11/03/2024\"                                                         \n[40] \"/service/pressemitteilungen/detail/news/spd-frauen-frueher-hat-verwaehlen-25-pfennige-gekostet-heute-kann-es-frauenrechte-kosten/07/03/2024\"           \n\n\nIf we look at this, we can see that it picked up plenty of more things that are stored in the same way with an a node and that have the href attribute. However, by clicking on the “MEHR” button, i.e. navigating to an individual press release, I can look at what the press release URL individually looks like. They all have /service/pressemitteilungen/detail/news/ in their URL whereas the other, unnecessary, stuff that we picked up as well does not have this. We can use this to filter out the URLs that we do not need.\n\nspd_urls &lt;- initial_urls_spd[1] |&gt; \n  read_html() |&gt; \n  html_nodes(\"a\") |&gt; \n  html_attr(\"href\") |&gt; \n  # this here transforms the output into a tibble on which we can then do\n  # the usual data management operations\n  tibble(url = _) |&gt; \n  # and I filter the column \"url\" for the string that we need\n  filter(str_detect(url, \"/service/pressemitteilungen/detail/news/\"))\n\nspd_urls |&gt; head(10)\n\n# A tibble: 10 × 1\n   url                                                                          \n   &lt;chr&gt;                                                                        \n 1 /service/pressemitteilungen/detail/news/nord-sued-neu-denken-veranstaltung-d…\n 2 /service/pressemitteilungen/detail/news/einladung-klausur-des-spd-parteivors…\n 3 /service/pressemitteilungen/detail/news/spd-frauen-frueher-hat-verwaehlen-25…\n 4 /service/pressemitteilungen/detail/news/katarina-barley-in-erlangen/07/03/20…\n 5 /service/pressemitteilungen/detail/news/saskia-esken-in-nordrhein-westfalen/…\n 6 /service/pressemitteilungen/detail/news/einladung-zur-pressekonferenz/06/03/…\n 7 /service/pressemitteilungen/detail/news/termine-lars-klingbeil-und-katarina-…\n 8 /service/pressemitteilungen/detail/news/spe-kongress-am-01-und-02-maerz-2024…\n 9 /service/pressemitteilungen/detail/news/spe-kongress-am-01-und-02-maerz-in-r…\n10 /service/pressemitteilungen/detail/news/arbeitsgemeinschaft-sozialdemokratis…\n\n\n\n\n\n\n\n\nNow before we automate this, one really really important thing! Always scrape as much information as you later need. This applies both for content extraction as well as simply recovering URLs. In our case, the URLs contain a string that indicates the date. That is awesome, but not the norm. I really suggest you always extract information which let’s you arrange things in a temporal order. This is a really important step to avoid later headaches or having to scrape all over again. What we are looking at, is a relatively easy task of scraping and it would not take too much of our time to do this again with another element. But if we are talking about scrapes that take a day or potentially weeks, you want to make sure beforehand that you have all the necessary elements.\n\n\n\nI suggest we also make use of the date element that comes within each URL and store it in a separate column called date. This will make it easier for us to sort the press releases by date later on, if ever we have to. And I want to get you used to good practices within scraping as early as possible. For that, I will use str_sub() of the stringr package (for a more thorough review of that powerful package see the section on it below). I mutate(), create a column called date and then specify that -10, i.e. the last 10 characters of the URL counting from the back of the character string, should be stored in that column. In R, if you want to specify that something should be counted/displayed/extracted or whatever from the end of something, you do so by putting a minus sign in front of it. The number simply counts the characters of that string.\n\nspd_urls &lt;- initial_urls_spd[1] |&gt; \n  read_html() |&gt; \n  html_nodes(\"a\") |&gt; \n  html_attr(\"href\") |&gt; \n  # this here transforms the output into a tibble\n  tibble(url = _) |&gt; \n  # and I filter the column \"url\" for the string that we need\n  filter(str_detect(url, \"/service/pressemitteilungen/detail/news/\")) |&gt; \n  # now I extract the date from the URL\n  mutate(date = str_sub(spd_urls$url, start = -10),\n  # here I add the root of the URL so that it can be read as an URL by\n  # RStudio later on\n         url = str_c(\"https://www.spd.de\", url))\n\nspd_urls\n\n# A tibble: 10 × 2\n   url                                                                     date \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 https://www.spd.de/service/pressemitteilungen/detail/news/nord-sued-ne… 12/0…\n 2 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-kl… 11/0…\n 3 https://www.spd.de/service/pressemitteilungen/detail/news/spd-frauen-f… 07/0…\n 4 https://www.spd.de/service/pressemitteilungen/detail/news/katarina-bar… 07/0…\n 5 https://www.spd.de/service/pressemitteilungen/detail/news/saskia-esken… 06/0…\n 6 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-zu… 06/0…\n 7 https://www.spd.de/service/pressemitteilungen/detail/news/termine-lars… 06/0…\n 8 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n 9 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n10 https://www.spd.de/service/pressemitteilungen/detail/news/arbeitsgemei… 01/0…\n\n\nThis is all fun and games but we have to automate this process. We could do this by using a for loop but this is not Python, I do not like for loops and the purrr package is (one of) my favorite packages in R. If we feed it a function, we can make it iterate over a list of URLs and apply the function to each element of the list. This is done with the map() function. We can also use map_df() which will return a data frame. However, since I work with tibbles we will write our function in a way that will return a tibble instead.\n\nlibrary(purrr)\n\nscraping_spd_urls &lt;- function(url) {\n  url |&gt; \n    read_html() |&gt; \n    html_nodes(\"a\") |&gt;\n    html_attr(\"href\") |&gt;\n    tibble(url = _) |&gt;\n    filter(str_detect(url, \"/service/pressemitteilungen/detail/news/\")) |&gt;\n    mutate(date = str_sub(url, start = -10),\n           url = str_c(\"https://www.spd.de\", url))\n}\n\nIf you run this on your end, you should now have a function in your environment under the section “Functions” that is called scraping_spd_urls. Now we can use map_df() to apply this function to our list of URLs. What you see me do here is that I only use the first 5 URLs of the list. This is because I want to make sure that the function works as intended. If it does, I can then apply it to the entire list. Then I specify the function that purrr should map over our list. The last element, .progress = TRUE will give us a loading bar that inidicates the progress of the scraping. This is particularly useful for longer scraping processes.\n\nspd_press_releases &lt;- map_df(initial_urls_spd[1:5], scraping_spd_urls,\n         .progress = TRUE)\n\nspd_press_releases\n\n# A tibble: 50 × 2\n   url                                                                     date \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 https://www.spd.de/service/pressemitteilungen/detail/news/nord-sued-ne… 12/0…\n 2 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-kl… 11/0…\n 3 https://www.spd.de/service/pressemitteilungen/detail/news/spd-frauen-f… 07/0…\n 4 https://www.spd.de/service/pressemitteilungen/detail/news/katarina-bar… 07/0…\n 5 https://www.spd.de/service/pressemitteilungen/detail/news/saskia-esken… 06/0…\n 6 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-zu… 06/0…\n 7 https://www.spd.de/service/pressemitteilungen/detail/news/termine-lars… 06/0…\n 8 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n 9 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n10 https://www.spd.de/service/pressemitteilungen/detail/news/arbeitsgemei… 01/0…\n# ℹ 40 more rows\n\n\nIf you are happy with the result, you could now apply the function to the entire list. For reasons of time, I will not do this. Congratulations, you have built your first scraper.\n\nB.4.2 Scraping content\nThis was the first step of the scraping workflow. Now we are going to inspect the structure of the website on which the respective press releases are stored. We will then write a function that will scrape the content of the press releases, put this in a map() and retrieve our information.\nAs already laid out above, you should really put some thoughts into the information you want to scrape. There is nothing worse than either having to scrape all over again or having to wrangle with your data afterwards because you have not tested your code sufficiently enough beforehand.\nIf you look at the press release’s individual website: https://www.spd.de/service/pressemitteilungen/detail/news/einladung-zur-pressekonferenz/02/02/2024, we can see that it has a title, the date, the content. Some other things you might encounter in these settings are sub-titles, sub-headers, other indices and so on. I suggest you always scrape everything. It is not the different elements that take time when scraping, it is navigating to the website, i.e. marginally more elements will not slow down your code.\nYou will have to find the different CSS selectors/XPath elements for the corresponding elements. And you want to make sure that they are unique and stay the same for each URL. You can never be sure of the later unless you do some proper validation before and after. We do not want to check this manually for each URL because that is not what automation is about. But you would want to check this for a sample of URLs. And be smart about it. If your code breaks after a certain amount of URLs or after a while it only returns NAs, you probably have a switch in the websites HTML structure. On well coded and new websites, this is rather rare because they are consistent. But as I have said before, the Web is full of badly coded website – the majority of them are.\nThe logic is the same as for Jan’s profile or the CEE’s website. You want to identify the HTML code blocks that correspond to the information of the title, the date, and the content. Here, I really recommend that you use the Selector Gadget I’ve shown you. This will allow you to click on the parts which you want and also eliminate other unwanted html elements. For the headline for example, I select the SelectorGadget, click on the headline and it gives me .news__headline as the CSS selector.\n\nFor the date:\n\nAnd now for the content:\n\nAnd this, we can now put into a function all together:\n\nscraping_press_spd &lt;- function(url) {\n  page_content &lt;- read_html(url)\n  date &lt;- str_sub(url, c(-10))\n  content &lt;-\n    html_elements(page_content, \".text__body\") |&gt;\n    html_text()\n  head_title &lt;- html_node(page_content,\n                          \"#main &gt; div &gt; section &gt; div.news &gt; div.news__header &gt; div &gt; h1\") |&gt;\n    html_text()\n  spd_pr &lt;- tibble(date, content, head_title)\n}\n\n\nspd_pr &lt;- map_df(spd_press_releases$url[1:5], scraping_press_spd, .progress = TRUE)\n\nspd_pr |&gt; head()\n\n# A tibble: 6 × 3\n  date       content                                                  head_title\n  &lt;chr&gt;      &lt;chr&gt;                                                    &lt;chr&gt;     \n1 12/03/2024 \"\"                                                       \" „Nord-S…\n2 12/03/2024 \"Wenige Tage nach seiner Reise nach Namibia, Südafrika … \" „Nord-S…\n3 11/03/2024 \"\"                                                       \" Einladu…\n4 11/03/2024 \"Zu Beratungen im Rahmen einer Klausur kommt der SPD-Pa… \" Einladu…\n5 07/03/2024 \"\"                                                       \" SPD FRA…\n6 07/03/2024 \"Anlässlich des Internationalen Frauentages 2024 finden… \" SPD FRA…\n\n\nIt seems as if the content column is filled twice; once with an empty string and once with the actual content. This is because the CSS selector I used is not specific enough. For the sake of the example, we will simply filter for an empty string. But you should always make sure that your CSS selectors are specific enough.\n\nspd_pr &lt;- spd_pr |&gt;\n  filter(content != \"\")\n\nspd_pr |&gt; head()\n\n# A tibble: 5 × 3\n  date       content                                                  head_title\n  &lt;chr&gt;      &lt;chr&gt;                                                    &lt;chr&gt;     \n1 12/03/2024 Wenige Tage nach seiner Reise nach Namibia, Südafrika u… \" „Nord-S…\n2 11/03/2024 Zu Beratungen im Rahmen einer Klausur kommt der SPD-Par… \" Einladu…\n3 07/03/2024 Anlässlich des Internationalen Frauentages 2024 finden … \" SPD FRA…\n4 07/03/2024 Die SPD-Spitzenkandidatin für die Europawahl Katarina B… \" Katarin…\n5 06/03/2024 Die SPD-Vorsitzende Saskia Esken kommt nach Nordrhein-W… \" Saskia …\n\n\n\nB.4.3 Speeding up the process with future and furrr\n\n\nB.5 Process text in R\nThe whole purpose of our scraping was to get textual data which we can then use to analyze the text in automated fashion using methods from Computational Social Sciences. However, handling text in R comes with some additional methods that you need to know.\nEven if you do not directly want to analyze your scraped data, you might still be faced with challenges of data management and data cleaning. Sometimes this is because there was absolutely no way to pick up only the date but every date HTML element now also contains character strings that you want to get rid of. Or let’s assume that you need everything in lower case letters, or get rid of this one ad which your scraper picked up no matter what you tried. This is where regular expressions and the `stringr´ package (which is part of the tidyverse environment) come into play.\n\nB.5.1 regular expressions\nRegular expressions – in coding linguo referred to as regex (singular) or regexes (plural) – are a powerful tool for pattern matching and text manipulation, widely used across various programming languages, including R. Pattern matching in this case simply means that you tell R to look for a specific pattern in a character string (a variable that contains text in its rows) and then do something with it. Sometimes this might be simply one specific word, or even a specific sentence. But sometimes you might need to tell R to look up strings that are 4 digits, then a dot, two digits, another dot, followed by lastly two digits again. This would for example be a date (yyyy.mm.dd). It could happen that you have a lot of dates which were scraped together with other stuff and you only want to extract the dates. Regexes are the perfect tool for this.\nThey allow you to search, replace, split, or extract parts of strings based on specific patterns. Understanding regexes can significantly enhance your ability to work with textual data, making tasks that would be complex or cumbersome to achieve with standard string functions straightforward. Now, unfortunately regexes are not simple and require some learning. But once you have understood the basics, you will be able to do a lot of things with them. And quite frankly, Chat-GPT is a world champion of writing regexes once you know how to prompt it.\n\nB.5.2 stringr package\nIn R, regexes are used together with the stringr package. The stringr package provides a cohesive set of functions designed to make working with strings as easy as possible.\nThe following examples take loose inspiration from Felix Lennert’s CSS Toolbox Script. All of the functions of the stringr package start with str_. They all serve one specific purpose. Below, I explain the most frequently used functions of that package. You can put them into pipes (%&gt;%/|&gt;) and easily use mutate() to create new columns or filter() to filter out rows.\nThis will be our example character string:\n\nexample_string &lt;- \"I love this class and R is fun!\"\nexample_string\n\n[1] \"I love this class and R is fun!\"\n\n\n\n\nstr_detect() checks if a string contains a pattern.\n\n\nstr_detect(example_string, \"love\")\n\n[1] TRUE\n\n\nyou could also construct an object with patterns to detect. This object is what we call a dictionary.\n\ndic &lt;- c(\"love\", \"fun\")\n\nstr_detect(example_string, dic)\n\n[1] TRUE TRUE\n\n\n\n\nstr_count() counts the number of matches in a string.\n\n\nstr_count(example_string, \"is\")\n\n[1] 2\n\n\nbut be careful, this indicates that it is picked up twice because “is” is also included in the word “this”. If you only wanted to pick up the word “is”, you would have to use the regex \\\\bis\\\\b which would only pick up the word “is” if it is a word on its own.\n\nstr_count(example_string, \"\\\\bis\\\\b\")\n\n[1] 1\n\n\n\n\nstr_subset() returns the matching elements of a character vector.\n\n\nstr_subset(example_string, \"is\")\n\n[1] \"I love this class and R is fun!\"\n\n\n\n\nstr_replace() replaces the first occurrence of a pattern in a string with something you indicate. The function is used in a way so that you first feed it your string(s), then the word that ought to be replaced and lastly by what it should be replaced:\n\n\nstr_replace(example_string, \"is\", \"was\")\n\n[1] \"I love thwas class and R is fun!\"\n\n\n\n\nstr_replace_all() replaces all occurrences of a pattern in a string with something you indicate.\n\n\nstr_replace_all(example_string, \"is\", \"was\")\n\n[1] \"I love thwas class and R was fun!\"\n\n\n\n\nstr_split() splits a string into pieces at a given pattern point. Here I specify that the string should be split into different pieces at every space bar by using the two quotation marks with a space in between \" \":\n\n\nstr_split(example_string, \" \")\n\n[[1]]\n[1] \"I\"     \"love\"  \"this\"  \"class\" \"and\"   \"R\"     \"is\"    \"fun!\" \n\n\n\n\nstr_to_lower() converts a string to lower case.\n\n\nstr_to_lower(example_string)\n\n[1] \"i love this class and r is fun!\"\n\n\n\n\nstr_to_upper() converts a string to upper case.\n\n\nstr_to_upper(example_string)\n\n[1] \"I LOVE THIS CLASS AND R IS FUN!\"\n\n\n\n\nstr_trim() removes leading and trailing whitespace from a string. Whitespace are long blank spaces between your characters that might stem from the HTML code.\n\n\nstr_trim(\"   I love this      class and R is fun!   \")\n\n[1] \"I love this      class and R is fun!\"\n\n\n\n\nstr_sub() extracts and/or replaces substrings from a character vector. Here I tell R to extract the first 5 characters of the string.\n\n\nstr_sub(example_string, start = 1, end = 5)\n\n[1] \"I lov\"\n\n\nand as already used in my code somewhere above, you can also index the operation from the end:\n\n# extract strings from fourth-to-last to last character\nstr_sub(example_string, start = -4, end = -1)\n\n[1] \"fun!\"\n\n\n\n\nstr_length() returns the number of characters in a string.\n\n\nstr_length(example_string)\n\n[1] 31\n\n\n\n\nstr_c() concatenates strings.\n\n\nstr_c(\"I\", \"love\", \"this\", \"class\", \"and\", \"R\", \"is\", \"fun!\")\n\n[1] \"IlovethisclassandRisfun!\"\n\n\n\nB.6 Selenium\nWork in Progress!\n\nB.6.1 Dynamic Websites\nWork in Progress!\n\nB.7 Internal Website APIs\nWork in Progress!\n\nB.8 Minet (Plique et al. 2024)\n\nThis section is a quick reference to the minet Python package. It is developed by the people working at the Médialab SciencesPo. They are great people who also have a monthly seminar called the METAT which you can attend if you need help with coding projects.\nI would like to emphasize that this is in no way my work but all the work of the people who developed the minet package (Plique et al. 2024). This only serves to put your attention to their work. I would recommend you to read the documentation of the package on GitHub if you want to use it for your own projects. Further, if ever you use it, please do not forget to reference them!\nThe minet package is a Python package that allows you to scrape data from the web. It is a great tool to use if you want to scrape data from social media platforms such as Twitter, Facebook, Instagram, & Co. It works within your Terminal and you can relatively easily scrape a lot of data from Social Media. Go check it out!\n\n\n\n\n\n\nPlease be careful when using minet. For Twitter or Instagram, you will have to be logged in to an account of these to social media to scrape them. I strongly recommend that you use burner accounts. Especially at the beginning, it happens quite easily and quickly that you get banned and will potentially lose the accounts.\n\n\n\nIf you have any questions, feel free to contact me via email.\n\nB.9 Ethics of Webscraping\nWebscraping is a fun and extremely useful tool of Computational Social Sciences. However, it is important to remember the ethics and issues, as well as the legal aspects that come with it. I recommend that you read this section attentively and take my suggestions seriously. This is not to scare you in any way, but to make you aware of the responsabilities that we have.\nGenerally speaking, webscraping is not immediately illegal. However, it is important to remember that you are scraping data from a website that is not yours. This means that you are using someone else’s data. It is important to respect the data and the website. I am no lawyer and I cannot give you any legal advice. However, I can give you some general advice on how to behave when scraping data from the web.\nPurpose: Always have a clear purpose for your webscraping project. What do you want to achieve? What is the goal of your project? What do you want to do with the data? These are all questions that you should ask yourself before you start scraping data. And then we only scrape the data that we need and we know our purpose for!\nAPIs: Always check for APIs that might be offered by the website. APIs make our life easier and we can play by the rules that the owners of the website dictate. This way we get what we want without disrespecting their rules. Now, the problem is that oftentimes there is no API or the API is not great. In that case, you might have to scrape the data yourself.\nRespect the website: Always respect the website. This means that you should not scrape the website in a way that it crashes. This is not only annoying for the website owner, but this quickly also becomes illegal. How do you crash a website? The easiest way is by sending so many requests in such a short amount of time that the server just gives in and you get a 404 error when going to valid URLs. This we want to avoid at all costs!. So how do we respect websites?\n\n(Try to) Respect robots.txt: The robots.txt file is a file that is located on the server of a website. Simply type in the root of the URL https//:www.sciencespo.fr and then add robots.txt. You will get to a text file in black with white text on it where some rules for crawlers and scrapers are specified. It tells you which parts of the website you are allowed to scrape and which parts you are not allowed to scrape. These are the rules set out by the website. But playing along the most strict rules, prohibits us from both scraping everything that we want and also from having fun. So you might have to break the rules a bit sometimes… ;)\nCheck out the Terms of Service: Some websites explicitly prohibit scraping in their terms of service (ToS). Review these terms to ensure that your scraping activities are not in violation.\nTimeouts: If possible, make requests at a reasonable rate to avoid overwhelming the site’s server. Implement delays between requests. This could for example be one request per second. If it happens to you that a website keeps blocking you for suspicious (scraping) activities, you might also want to set a timeout; preferably you set the time out at a random interval in a specific time frame. The more randomness you introduce to your scraping, the less likely you are to get detected.\n\nBe respectful and thoughtful of the data that you are scraping and where you store it! Depending on the data that you are scraping, it might be more or less sensitive data. Or your data might be subject to copyright law. In itself, the collection of it is not illegal. But what you do with it (and subsequently thus also where you store it) becomes important relatively quickly. If your purpose of scraping is research, you are already on a safer side. Do not use your scraped data for any commercial purposes. My scripts are only destined for people that use scraping for research. Second, always be aware of the GDPR; it is the European regulation of data protection. And there are very good reasons the GDPR exists.\nIf you data is subject to copyright law, then be especially careful. Let’s, hypothetically, assume for a moment that you wanted to scrape newspapers and construct a large corpus of articles. This is a great idea and a great project. However, you would have to be very careful with the data. Only store it locally, do not share it with whomever asks you for it, and watch out for what you use it later on.\nWhen I say store data locally, this implies your local computer, an external hard drive or a USB stick. A cloud service, especially Google Drive, is not a local and secure storing service. Why? Because Google Drive runs on Google’s servers. And we do not want to give indirect access to our copyrighted data to Google. 4 The same goes for Dropbox, OneDrive, and all the other cloud services. Also sharing the data with collaborators should be done locally through hard drives and not services like WeTransfer!\nThe same rules apply for data that you scrape about individuals. This is sensitive data and you should be very careful with it. Anything that would make it possible to attribute anonymous data to a person is sensitive data.\n\nB.10 References\n\n\n\n\nPlique, Guillaume, Pauline Breteau, Jules Farjas, Héloïse Théro, Descamps, Amélie Pellé, Laura Miguel, and César Pichon. 2024. “Minet, a Webmining CLI Tool & Library for Python.” Zenodo. https://doi.org/10.5281/ZENODO.4564399.\n\n\n\n\nTo be completely frank, RSelenium is a pain in the butt and was one of the reasons why I started to learn Python at some point. And for now – it does seem as if things are changing for the rvest package – I would recommend that you do too. Contact me for questions on this or wait until I update this script and include Python code.↩︎\nNote that you can also use it as a search bar. If you are unsure about the path to your element of interest, you can type it in and it will highlight all the elements that belong to it in yellow.↩︎\nPlease note that I am writing this script in February 2024. The number 111 will not be up to date in a couple of days as the party keeps releasing press releases. Your code might have to be adapted slightly but that is not an issue usually.↩︎\nYou might amend that Google probably already has the newspaper data that we might scrape. And you are probably more than right. But we do not want to get into trouble and we should care about these things on our end. What they do is not our business.↩︎\n\n      \n         A  Quarto & The Markdown Language",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#extracting-content-from-websites-rvest",
    "href": "appendix/webscraping.html#extracting-content-from-websites-rvest",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.3 Extracting content from websites (rvest)",
    "text": "B.3 Extracting content from websites (rvest)\nFor my first example, we will stick withg the CEE’s website where they display the doctoral students. We will use the rvest package to extract the content from the website. The first thing which we have to do, is to copy paste the URL of the website we want to scrape into the read_html function of the package. This will give us the entire HTML code of that belongs to the website whose URL (remember the online address) we fed to the function. When you copy paste, don’t forget the “https://” as well as the quotation marks around the URL text string. If you simply go into your browser, click on the URL once, it will highlight the “https://” automatically (even though you might not see it).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nread_html(\"https://www.sciencespo.fr/centre-etudes-europeennes/fr/doctorants.html\")\n\n{html_document}\n&lt;html lang=\"fr\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" ...\n[2] &lt;body id=\"top\" class=\"not-front not-logged-in page-doctorants sidebar-rig ...\n\n\nThe output suggests that we have retrieved (“scraped” so to say) the html source code of the website. Feel free to go back to the original website, right click and inspect it to see that these are the same things. We could of course also store the HTML code in a variable. This is often useful if you want to scrape multiple websites and store the HTML code of each website in a separate variable.\n\ncee &lt;- read_html(\"https://www.sciencespo.fr/centre-etudes-europeennes/fr/doctorants.html\")\nglimpse(cee)\n\nList of 2\n $ node:&lt;externalptr&gt; \n $ doc :&lt;externalptr&gt; \n - attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n\n\nNow, we want to extract some content from the website. We want to know the names of the doctoral students. We can do this by using the html_nodes function of the rvest package. This function allows us to extract the content of the website based on the HTML node. Remember that we have used the Selector Gadget above to find the node corresponding to the names of the doctoral students. We can now use this information to extract the names of the doctoral students. For the sake of simplicity and to keep the code clean, I will store the URL in a variable called… url. This will be part of the workflow later on when we start scraping multiple websites.\n\nurl &lt;- \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/doctorants.html\"\n\nread_html(url) |&gt; \n  html_nodes(\".views-field-title a\") \n\n{xml_nodeset (35)}\n [1] &lt;a href=\"chercheur/lennard-alke.html\"&gt;Lennard Alke&lt;/a&gt;\n [2] &lt;a href=\"chercheur/marcela-alonso-ferreira.html\"&gt;Marcela Alonso Ferreira ...\n [3] &lt;a href=\"chercheur/simon-audebert.html\"&gt;Simon Audebert&lt;/a&gt;\n [4] &lt;a href=\"chercheur/meryem-bezzaz.html\"&gt;Meryem Bezzaz&lt;/a&gt;\n [5] &lt;a href=\"chercheur/marius-bickhardt.html\"&gt;Marius Bickhardt&lt;/a&gt;\n [6] &lt;a href=\"chercheur/jan-boguslawski.html\"&gt;Jan Boguslawski&lt;/a&gt;\n [7] &lt;a href=\"chercheur/jean-baptiste-bonnet.html\"&gt;Jean-Baptiste Bonnet&lt;/a&gt;\n [8] &lt;a href=\"chercheur/eva-bossuyt.html\"&gt;Eva Bossuyt&lt;/a&gt;\n [9] &lt;a href=\"chercheur/charlotte-boucher.html\"&gt;Charlotte Boucher&lt;/a&gt;\n[10] &lt;a href=\"chercheur/jens-carstens.html\"&gt;Jens Carstens&lt;/a&gt;\n[11] &lt;a href=\"chercheur/jean-baptiste-chambon.html\"&gt;Jean-Baptiste Chambon&lt;/a&gt;\n[12] &lt;a href=\"chercheur/thalia-creach.html\"&gt;Thalia Creac'h&lt;/a&gt;\n[13] &lt;a href=\"chercheur/pablo-cussac.html\"&gt;Pablo Cussac&lt;/a&gt;\n[14] &lt;a href=\"chercheur/soazig-dollet.html\"&gt;Soazig Dollet&lt;/a&gt;\n[15] &lt;a href=\"chercheur/lea-dornacher.html\"&gt;Lea Dornacher&lt;/a&gt;\n[16] &lt;a href=\"chercheur/maxence-dutilleul.html\"&gt;Maxence Dutilleul&lt;/a&gt;\n[17] &lt;a href=\"chercheur/zoe-evrard.html\"&gt;Zoé Evrard&lt;/a&gt;\n[18] &lt;a href=\"chercheur/maxime-gaborit.html\"&gt;Maxime Gaborit&lt;/a&gt;\n[19] &lt;a href=\"chercheur/leo-grillet.html\"&gt;Léo Grillet&lt;/a&gt;\n[20] &lt;a href=\"chercheur/marie-ines-harte.html\"&gt;Marie Inès Harté&lt;/a&gt;\n...\n\n\nHere we can see that there are 35 nodes that all have a similar structure &lt;a href=\"chercheur/lennard-alke.html\"&gt;Lennard Alke&lt;/a&gt;. Since I only want the text corresponding to the names, we will use the html_text function to extract the names of the doctoral students. The text is stored in the a tag.\n\nread_html(url) |&gt; \n  html_nodes(\".views-field-title a\") |&gt; \n  html_text()\n\n [1] \"Lennard Alke\"            \"Marcela Alonso Ferreira\"\n [3] \"Simon Audebert\"          \"Meryem Bezzaz\"          \n [5] \"Marius Bickhardt\"        \"Jan Boguslawski\"        \n [7] \"Jean-Baptiste Bonnet\"    \"Eva Bossuyt\"            \n [9] \"Charlotte Boucher\"       \"Jens Carstens\"          \n[11] \"Jean-Baptiste Chambon\"   \"Thalia Creac'h\"         \n[13] \"Pablo Cussac\"            \"Soazig Dollet\"          \n[15] \"Lea Dornacher\"           \"Maxence Dutilleul\"      \n[17] \"Zoé Evrard\"              \"Maxime Gaborit\"         \n[19] \"Léo Grillet\"             \"Marie Inès Harté\"       \n[21] \"Emilien Houard-Vial\"     \"Malo Jan\"               \n[23] \"Angeliki Konstantinidou\" \"Thomas Laffitte\"        \n[25] \"Claire Morgane Lejeune\"  \"Chiao Li\"               \n[27] \"Arno Lizet\"              \"Mattia Lupi\"            \n[29] \"Francesco Nardone\"       \"Selma Sarenkapa\"        \n[31] \"Luis Sattelmayer\"        \"Viviane Spitzhofer\"     \n[33] \"Théodore Tallent\"        \"Lucien Thabourey\"       \n[35] \"Marta Tramezzani\"       \n\n\nNow, we have extracted all the names on the website. Feel free to check out who these people are. You should know at least two by now.\nI can tell you though that there are other things stored in our initial HTML node .views-field-title a. We have extracted the text but there is also something called href. This is a hyperlink reference and key for webscraping. If you go back to your browser and on the website we are currently scraping, you will realize that by hovering over our names with your cursor, you can click on them and it will take you to a new website. The information of where your Browser should take you when you click on a name has to be stored somewhere. And well it is stored in the HTML source code as well, within the node we have already identified and to be more precise, the corresponding link (URL) is stored in the href attribute of the a tag. We can extract this information as well as such:\n\nread_html(url) |&gt; \n  html_nodes(\".views-field-title a\") |&gt; \n  html_attr(\"href\") \n\n [1] \"chercheur/lennard-alke.html\"           \n [2] \"chercheur/marcela-alonso-ferreira.html\"\n [3] \"chercheur/simon-audebert.html\"         \n [4] \"chercheur/meryem-bezzaz.html\"          \n [5] \"chercheur/marius-bickhardt.html\"       \n [6] \"chercheur/jan-boguslawski.html\"        \n [7] \"chercheur/jean-baptiste-bonnet.html\"   \n [8] \"chercheur/eva-bossuyt.html\"            \n [9] \"chercheur/charlotte-boucher.html\"      \n[10] \"chercheur/jens-carstens.html\"          \n[11] \"chercheur/jean-baptiste-chambon.html\"  \n[12] \"chercheur/thalia-creach.html\"          \n[13] \"chercheur/pablo-cussac.html\"           \n[14] \"chercheur/soazig-dollet.html\"          \n[15] \"chercheur/lea-dornacher.html\"          \n[16] \"chercheur/maxence-dutilleul.html\"      \n[17] \"chercheur/zoe-evrard.html\"             \n[18] \"chercheur/maxime-gaborit.html\"         \n[19] \"chercheur/leo-grillet.html\"            \n[20] \"chercheur/marie-ines-harte.html\"       \n[21] \"chercheur/emilien-houard-vial.html\"    \n[22] \"chercheur/malo-jan.html\"               \n[23] \"chercheur/angeliki-konstantinidou.html\"\n[24] \"chercheur/thomas-laffitte.html\"        \n[25] \"chercheur/claire-morgane-lejeune.html\" \n[26] \"chercheur/chiao-li.html\"               \n[27] \"chercheur/arno-lizet.html\"             \n[28] \"chercheur/mattia-lupi.html\"            \n[29] \"chercheur/francesco-nardone.html\"      \n[30] \"chercheur/selma-sarenkapa.html\"        \n[31] \"chercheur/luis-sattelmayer.html\"       \n[32] \"chercheur/viviane-spitzhofer.html\"     \n[33] \"chercheur/theodore-tallent.html\"       \n[34] \"chercheur/lucien-thabourey.html\"       \n[35] \"chercheur/marta-tramezzani.html\"       \n\n\nNote how this time we are not using the html_text function but the html_attr function. This function allows us to extract the content of a specific attribute of the HTML node. href is such an attribute.\nNow if you look at the list of elements we have extracted, you will see that they are not complete URLs. They are relative URLs. This means that they are not complete URLs but rather URLs that are relative to the current website. Our extracted URLs are of the form chercheur/malo-jan.html. Websites always need “https://www.” in front of the URL to be complete. In our case, we can look at the website of Malo on here to see what is missing. His actual URL is\n\nhttps://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/malo-jan.html\n\nThis means that we have to add https://www.sciencespo.fr/centre-etudes-europeennes/fr/ to our relative URLs. We can do this by using the str_c function from the stringr package that comes with the tidyverse. More on that package further below. Here it simply adds (concatenates which is where this function gets its c from) the two strings together.\n\ncee_phds &lt;- read_html(url) |&gt; \n  html_nodes(\".views-field-title a\") |&gt; \n  html_attr(\"href\") %&gt;%  # if you are wondering why the %&gt;%, check the note below\n  str_c(\"https://www.sciencespo.fr/centre-etudes-europeennes/fr/\", .)\n\nhead(cee_phds, 5)\n\n[1] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/lennard-alke.html\"           \n[2] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/marcela-alonso-ferreira.html\"\n[3] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/simon-audebert.html\"         \n[4] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/meryem-bezzaz.html\"          \n[5] \"https://www.sciencespo.fr/centre-etudes-europeennes/fr/chercheur/marius-bickhardt.html\"       \n\n\n\n\n\n\n\n\nIn some rare cases, you will have to specify something in long pipes (%&gt;% or |&gt;) that is called a placeholder. Usually in pipes, you do not have to specify the object on which you are doing something anymore. Sometimes you do, however. In our case that is to indicate whether the string of the URL that was missing should be added at the beginning of our relative URLs or at the end. The dot . is a placeholder that tells the function to use the object that was passed to the pipe before the placeholder. And for whatever reason, the Base R placeholder does not work in this case and you have to work around it by using the old magrittr %&gt;% pipe and the corresponding placeholder which is a dot.\n\n\n\nWe now have officially scraped the URLs which lead to the individualy profiles of the CEE’s PhD students.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#what-is-the-webscraping-workflow",
    "href": "appendix/webscraping.html#what-is-the-webscraping-workflow",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.4 What is the webscraping workflow?",
    "text": "B.4 What is the webscraping workflow?\nNow, you might be wondering why I emphasized the HTML attribute href so much in the section above. Or why we scraped URLs although I was speaking of content before. The reason is that webscraping is not only about extracting content from websites. It is also about extracting the structure of the website. This is important because it allows us to scrape multiple websites in a structured way. What I showed you on the CEE’s website could technically be done manually. It would take longer by hand, if you know how to code, but still… it could have been done manually. But let’s suppose, I would like to have 10 000 press releases of a party, or scrape all parliamentary speeches of the French Assemblée Nationale. This would be doable manually but it would be terribly chiant (excuse my French).\nIn very very broad terms, webscraping is a two step process. You first collect all the URLs behind which your content of interest is stored, and then you scrape the content behind these URLs. This is the workflow we will follow in the next section. To put it differently, we first need to collect a list of URLs (for which we will build one scraper) which serves us then for our next stept during which we scrape the content with a second scraper.\nThis is where we will have more coding fun and where you might need a refresher of how to write a function and the purrr package (Session 2) of this class. But I will try to talk you to my steps as much as possible. Feel free to go back, however, and look at the code from the previous sessions.\n\nB.4.1 Automated collection of URLS\nIn an ideal world, where web designers are nice people, all websites would have a similar structure. This would mean that we could write one scraper and use it for all websites. But we do not live in an ideal world. Websites are different and we usually have to write a new scraper for each website.\nThe typical example for an introduction to webscraping would be to scrape IMDB. But this is a social science oriented class and we will collect political texts. We will start by harvesting some press releases by the German Social Democratic Party, the SPD. There is no reason as to why this party other than their website is well coded and can easily be scraped. The first thing we need to do is to identify their press release archives. Ideally they have something like this, fortunately they do. If you click on here you can check them out yourselves. And no worries, you do not need to be able to speak German for this task. As a matter of fact, the HTML language and coding in general are both universal enough to bridge language barriers – and in some moments deepl does the trick (but I know that this is no news to any of us).\nThe URL of the press release archive of the SPD is https://www.spd.de/service/pressemitteilungen. First we need to understand the website’s structure to write code that will alternate over each page of their archive and retrieve only those URLs that we are interested in, i.e. the URLs of the press releases. If you click on the link and scroll all the way down, you will see that there is a red circle with a one, another with a two, three dots and then a 111 in another red circle, like in the picture below. 3\n\nThis is a typical pagination structure. It means that the press releases are not all on one page but on multiple pages. I know that all of you have come across this before and that we have all already clicked on these things in our digital lives.\nAnother thing that you might realize, while you have scrolled down, is that the page has remained more or less the same throughout and that nothing new appeared while going down. This is a very good and solid indicator that the website is static (yay).\nNow click on the red two in the circle. This will redirect you to the next page. You will see that the URL has changed to https://www.spd.de/service/pressemitteilungen/page/2. This is a very good sign. If you now scroll down, click on the red arrow next to the 111, you will be redirected to the next page. The URL will be https://www.spd.de/service/pressemitteilungen/page/3. This means that whenever we click on to the next page, the URL changes in a predictable way and we can – very – easily reproduce this in R by creating a list of URLs that we can then have our code use to extract the URLs that are stored on each of “page/3” to the last page. This is an ideal scenario. Sometimes you will have to look a bit more closely for the subtle changes in initial URLs that we need to find to automate our process.\nNow, a quick excursion in some sorting and filtering. We want to make sure that the list of initial URLs, over which we will then scrape in a second step, contains all the URLs. Since we know how the URLs are structured and behave, we can also simply go into our search bar and manually change the number from, let’s say, 3 to 100. If you do this, you will approximately land in 2016 and we will see the display of press releases. To speed up the process, you never want to go one by one, meaning to try out first “page/3”, “page/4”, “page/5” and so on. You want to find the last page as quickly as possible. For that, you randomly type in a large number and see what happens. The worst thing that can happen is that you stumble upon a 404 error; which is just the website telling us that the URL we are trying to navigate to looks like it is on their website but does not exist in reality. Then we know that we will have to try a smaller number to approximate the last page on which the press releases are stored. I suggest you do this in half steps meaning that you always take double or half the number and then see what happens. This is a much faster way to sort or filter things in computer science than increasing/decreasing one step at a time. In our case, I put in 100 and did not get a 404 error. If I now put in the double “https://www.spd.de/service/pressemitteilungen/page/200”, you surprisingly do not get a 404 error. But if you scroll down, we can see that we have reached 111. And that this seems to be the last page of available press releases in the SPD’s archives. Oftentimes, you would have gotten an error message somehow on the website. But from this paragraph, I just want you to take away that it is faster to sort and filter in double steps for the final URL than to go one by one.\n\n\n\n\n\n\nThere are plenty of errors that you can get on a website or on the Web. They are called HTTP response status codes. 404 is one of the most frequent ones but you might encounter others as you scrape more. You do not have to know them. If you see one that is not 404, simply google it and then troubleshoot from there. Here is an overview.\n\n\n\nAlright, let’s finally get our hands on some code. We know that the URLs are structured in a way that we can easily predict the next URL. We also know that they alternate by simply changing the last digit of the URL and that there are 111 URLs in total. We can now create an object that contains all the initial URLs. The code below stores a character string with the root of the URL in an object called intial_url. We then use the str_c function from the stringr package to concatenate the root URL with the numbers 1 to 111. The sep argument is set to \"\" to make sure that there is no space between our initial url root and the numbers we want to add.\n\nintial_url &lt;- \"https://www.spd.de/service/pressemitteilungen/page/\" \ninitial_urls_spd &lt;- str_c(intial_url, 1:111)\n\ninitial_urls_spd |&gt; head()\n\n[1] \"https://www.spd.de/service/pressemitteilungen/page/1\"\n[2] \"https://www.spd.de/service/pressemitteilungen/page/2\"\n[3] \"https://www.spd.de/service/pressemitteilungen/page/3\"\n[4] \"https://www.spd.de/service/pressemitteilungen/page/4\"\n[5] \"https://www.spd.de/service/pressemitteilungen/page/5\"\n[6] \"https://www.spd.de/service/pressemitteilungen/page/6\"\n\n\nAlright, now we have a list of URLs that we can use to scrape the URLs of the individual press releases. For that, we will have to find the URLs behind which the individual press release is stored.\n If you want to navigate to a single press release you have to click on the black button that says “MEHR” (more in German). Right-click on it -&gt; go to inspect -&gt; and the HTML source code will already show us in what node the URL is stored.\n\n\nThe HTML source code corresponding to the MEHR button.\n\nAs for the CEE’s website, it is within an “a” tag and the URL is stored in the “href” attribute. You could of course also use the GadgetSelector extension which will yield the same result. Now let’s apply the same logic as above. For the first example, I will only use the first entry of our initial_urls_spd list:\n\nlibrary(rvest)\ninitial_urls_spd[1] |&gt; \n  read_html() |&gt; \n  html_nodes(\"a\") |&gt; \n  html_attr(\"href\") |&gt; \n  head(40)\n\n [1] \"https://www.spd.de/site/datenschutz/#c38250\"                                                                                                           \n [2] \"https://www.spd.de/site/datenschutz/\"                                                                                                                  \n [3] \"?acceptCookiePolicy=1\"                                                                                                                                 \n [4] \"#main\"                                                                                                                                                 \n [5] \"#footer\"                                                                                                                                               \n [6] \"https://meine.spd.de/\"                                                                                                                                 \n [7] \"https://www.spd.de/suche\"                                                                                                                              \n [8] \"/\"                                                                                                                                                     \n [9] \"/programm\"                                                                                                                                             \n[10] \"/programm/europaprogramm\"                                                                                                                              \n[11] \"/programm/stark-gegen-rechts\"                                                                                                                          \n[12] \"/programm/respekt\"                                                                                                                                     \n[13] \"/programm/gesundheit-und-pflege\"                                                                                                                       \n[14] \"/programm/wohnen\"                                                                                                                                      \n[15] \"/programm/familien\"                                                                                                                                    \n[16] \"/programm/klimaschutz\"                                                                                                                                 \n[17] \"/programm/digitalisierung\"                                                                                                                             \n[18] \"https://www.spd.de/stabile-rente\"                                                                                                                      \n[19] \"/programm/beschluesse\"                                                                                                                                 \n[20] \"/programm/grundsatzprogramm\"                                                                                                                           \n[21] \"/programm/zukunftsprogramm\"                                                                                                                            \n[22] \"/partei\"                                                                                                                                               \n[23] \"/partei/geschichte\"                                                                                                                                    \n[24] \"https://www.spd.de/partei#c75377\"                                                                                                                      \n[25] \"https://www.spd.de/partei/#c75398\"                                                                                                                     \n[26] \"https://www.spd.de/partei/#c75461\"                                                                                                                     \n[27] \"/partei/preise\"                                                                                                                                        \n[28] \"/service\"                                                                                                                                              \n[29] \"/site/kontakt\"                                                                                                                                         \n[30] \"https://www.spd.de/service/#c75578\"                                                                                                                    \n[31] \"https://www.spd.de/service/#m75572\"                                                                                                                    \n[32] \"https://www.spd.de/service/#c75584\"                                                                                                                    \n[33] \"/service/finanzen-und-transparenz\"                                                                                                                     \n[34] \"https://www.spd.de/unterstuetzen\"                                                                                                                      \n[35] \"https://www.spd.de/suche\"                                                                                                                              \n[36] \"/\"                                                                                                                                                     \n[37] \"/service\"                                                                                                                                              \n[38] \"/service/pressemitteilungen/detail/news/nord-sued-neu-denken-veranstaltung-des-geschichtsforums-der-spd-mit-dem-vorsitzenden-lars-klingbeil/12/03/2024\"\n[39] \"/service/pressemitteilungen/detail/news/einladung-klausur-des-spd-parteivorstandes/11/03/2024\"                                                         \n[40] \"/service/pressemitteilungen/detail/news/spd-frauen-frueher-hat-verwaehlen-25-pfennige-gekostet-heute-kann-es-frauenrechte-kosten/07/03/2024\"           \n\n\nIf we look at this, we can see that it picked up plenty of more things that are stored in the same way with an a node and that have the href attribute. However, by clicking on the “MEHR” button, i.e. navigating to an individual press release, I can look at what the press release URL individually looks like. They all have /service/pressemitteilungen/detail/news/ in their URL whereas the other, unnecessary, stuff that we picked up as well does not have this. We can use this to filter out the URLs that we do not need.\n\nspd_urls &lt;- initial_urls_spd[1] |&gt; \n  read_html() |&gt; \n  html_nodes(\"a\") |&gt; \n  html_attr(\"href\") |&gt; \n  # this here transforms the output into a tibble on which we can then do\n  # the usual data management operations\n  tibble(url = _) |&gt; \n  # and I filter the column \"url\" for the string that we need\n  filter(str_detect(url, \"/service/pressemitteilungen/detail/news/\"))\n\nspd_urls |&gt; head(10)\n\n# A tibble: 10 × 1\n   url                                                                          \n   &lt;chr&gt;                                                                        \n 1 /service/pressemitteilungen/detail/news/nord-sued-neu-denken-veranstaltung-d…\n 2 /service/pressemitteilungen/detail/news/einladung-klausur-des-spd-parteivors…\n 3 /service/pressemitteilungen/detail/news/spd-frauen-frueher-hat-verwaehlen-25…\n 4 /service/pressemitteilungen/detail/news/katarina-barley-in-erlangen/07/03/20…\n 5 /service/pressemitteilungen/detail/news/saskia-esken-in-nordrhein-westfalen/…\n 6 /service/pressemitteilungen/detail/news/einladung-zur-pressekonferenz/06/03/…\n 7 /service/pressemitteilungen/detail/news/termine-lars-klingbeil-und-katarina-…\n 8 /service/pressemitteilungen/detail/news/spe-kongress-am-01-und-02-maerz-2024…\n 9 /service/pressemitteilungen/detail/news/spe-kongress-am-01-und-02-maerz-in-r…\n10 /service/pressemitteilungen/detail/news/arbeitsgemeinschaft-sozialdemokratis…\n\n\n\n\n\n\n\n\nNow before we automate this, one really really important thing! Always scrape as much information as you later need. This applies both for content extraction as well as simply recovering URLs. In our case, the URLs contain a string that indicates the date. That is awesome, but not the norm. I really suggest you always extract information which let’s you arrange things in a temporal order. This is a really important step to avoid later headaches or having to scrape all over again. What we are looking at, is a relatively easy task of scraping and it would not take too much of our time to do this again with another element. But if we are talking about scrapes that take a day or potentially weeks, you want to make sure beforehand that you have all the necessary elements.\n\n\n\nI suggest we also make use of the date element that comes within each URL and store it in a separate column called date. This will make it easier for us to sort the press releases by date later on, if ever we have to. And I want to get you used to good practices within scraping as early as possible. For that, I will use str_sub() of the stringr package (for a more thorough review of that powerful package see the section on it below). I mutate(), create a column called date and then specify that -10, i.e. the last 10 characters of the URL counting from the back of the character string, should be stored in that column. In R, if you want to specify that something should be counted/displayed/extracted or whatever from the end of something, you do so by putting a minus sign in front of it. The number simply counts the characters of that string.\n\nspd_urls &lt;- initial_urls_spd[1] |&gt; \n  read_html() |&gt; \n  html_nodes(\"a\") |&gt; \n  html_attr(\"href\") |&gt; \n  # this here transforms the output into a tibble\n  tibble(url = _) |&gt; \n  # and I filter the column \"url\" for the string that we need\n  filter(str_detect(url, \"/service/pressemitteilungen/detail/news/\")) |&gt; \n  # now I extract the date from the URL\n  mutate(date = str_sub(spd_urls$url, start = -10),\n  # here I add the root of the URL so that it can be read as an URL by\n  # RStudio later on\n         url = str_c(\"https://www.spd.de\", url))\n\nspd_urls\n\n# A tibble: 10 × 2\n   url                                                                     date \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 https://www.spd.de/service/pressemitteilungen/detail/news/nord-sued-ne… 12/0…\n 2 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-kl… 11/0…\n 3 https://www.spd.de/service/pressemitteilungen/detail/news/spd-frauen-f… 07/0…\n 4 https://www.spd.de/service/pressemitteilungen/detail/news/katarina-bar… 07/0…\n 5 https://www.spd.de/service/pressemitteilungen/detail/news/saskia-esken… 06/0…\n 6 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-zu… 06/0…\n 7 https://www.spd.de/service/pressemitteilungen/detail/news/termine-lars… 06/0…\n 8 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n 9 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n10 https://www.spd.de/service/pressemitteilungen/detail/news/arbeitsgemei… 01/0…\n\n\nThis is all fun and games but we have to automate this process. We could do this by using a for loop but this is not Python, I do not like for loops and the purrr package is (one of) my favorite packages in R. If we feed it a function, we can make it iterate over a list of URLs and apply the function to each element of the list. This is done with the map() function. We can also use map_df() which will return a data frame. However, since I work with tibbles we will write our function in a way that will return a tibble instead.\n\nlibrary(purrr)\n\nscraping_spd_urls &lt;- function(url) {\n  url |&gt; \n    read_html() |&gt; \n    html_nodes(\"a\") |&gt;\n    html_attr(\"href\") |&gt;\n    tibble(url = _) |&gt;\n    filter(str_detect(url, \"/service/pressemitteilungen/detail/news/\")) |&gt;\n    mutate(date = str_sub(url, start = -10),\n           url = str_c(\"https://www.spd.de\", url))\n}\n\nIf you run this on your end, you should now have a function in your environment under the section “Functions” that is called scraping_spd_urls. Now we can use map_df() to apply this function to our list of URLs. What you see me do here is that I only use the first 5 URLs of the list. This is because I want to make sure that the function works as intended. If it does, I can then apply it to the entire list. Then I specify the function that purrr should map over our list. The last element, .progress = TRUE will give us a loading bar that inidicates the progress of the scraping. This is particularly useful for longer scraping processes.\n\nspd_press_releases &lt;- map_df(initial_urls_spd[1:5], scraping_spd_urls,\n         .progress = TRUE)\n\nspd_press_releases\n\n# A tibble: 50 × 2\n   url                                                                     date \n   &lt;chr&gt;                                                                   &lt;chr&gt;\n 1 https://www.spd.de/service/pressemitteilungen/detail/news/nord-sued-ne… 12/0…\n 2 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-kl… 11/0…\n 3 https://www.spd.de/service/pressemitteilungen/detail/news/spd-frauen-f… 07/0…\n 4 https://www.spd.de/service/pressemitteilungen/detail/news/katarina-bar… 07/0…\n 5 https://www.spd.de/service/pressemitteilungen/detail/news/saskia-esken… 06/0…\n 6 https://www.spd.de/service/pressemitteilungen/detail/news/einladung-zu… 06/0…\n 7 https://www.spd.de/service/pressemitteilungen/detail/news/termine-lars… 06/0…\n 8 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n 9 https://www.spd.de/service/pressemitteilungen/detail/news/spe-kongress… 01/0…\n10 https://www.spd.de/service/pressemitteilungen/detail/news/arbeitsgemei… 01/0…\n# ℹ 40 more rows\n\n\nIf you are happy with the result, you could now apply the function to the entire list. For reasons of time, I will not do this. Congratulations, you have built your first scraper.\n\nB.4.2 Scraping content\nThis was the first step of the scraping workflow. Now we are going to inspect the structure of the website on which the respective press releases are stored. We will then write a function that will scrape the content of the press releases, put this in a map() and retrieve our information.\nAs already laid out above, you should really put some thoughts into the information you want to scrape. There is nothing worse than either having to scrape all over again or having to wrangle with your data afterwards because you have not tested your code sufficiently enough beforehand.\nIf you look at the press release’s individual website: https://www.spd.de/service/pressemitteilungen/detail/news/einladung-zur-pressekonferenz/02/02/2024, we can see that it has a title, the date, the content. Some other things you might encounter in these settings are sub-titles, sub-headers, other indices and so on. I suggest you always scrape everything. It is not the different elements that take time when scraping, it is navigating to the website, i.e. marginally more elements will not slow down your code.\nYou will have to find the different CSS selectors/XPath elements for the corresponding elements. And you want to make sure that they are unique and stay the same for each URL. You can never be sure of the later unless you do some proper validation before and after. We do not want to check this manually for each URL because that is not what automation is about. But you would want to check this for a sample of URLs. And be smart about it. If your code breaks after a certain amount of URLs or after a while it only returns NAs, you probably have a switch in the websites HTML structure. On well coded and new websites, this is rather rare because they are consistent. But as I have said before, the Web is full of badly coded website – the majority of them are.\nThe logic is the same as for Jan’s profile or the CEE’s website. You want to identify the HTML code blocks that correspond to the information of the title, the date, and the content. Here, I really recommend that you use the Selector Gadget I’ve shown you. This will allow you to click on the parts which you want and also eliminate other unwanted html elements. For the headline for example, I select the SelectorGadget, click on the headline and it gives me .news__headline as the CSS selector.\n\nFor the date:\n\nAnd now for the content:\n\nAnd this, we can now put into a function all together:\n\nscraping_press_spd &lt;- function(url) {\n  page_content &lt;- read_html(url)\n  date &lt;- str_sub(url, c(-10))\n  content &lt;-\n    html_elements(page_content, \".text__body\") |&gt;\n    html_text()\n  head_title &lt;- html_node(page_content,\n                          \"#main &gt; div &gt; section &gt; div.news &gt; div.news__header &gt; div &gt; h1\") |&gt;\n    html_text()\n  spd_pr &lt;- tibble(date, content, head_title)\n}\n\n\nspd_pr &lt;- map_df(spd_press_releases$url[1:5], scraping_press_spd, .progress = TRUE)\n\nspd_pr |&gt; head()\n\n# A tibble: 6 × 3\n  date       content                                                  head_title\n  &lt;chr&gt;      &lt;chr&gt;                                                    &lt;chr&gt;     \n1 12/03/2024 \"\"                                                       \" „Nord-S…\n2 12/03/2024 \"Wenige Tage nach seiner Reise nach Namibia, Südafrika … \" „Nord-S…\n3 11/03/2024 \"\"                                                       \" Einladu…\n4 11/03/2024 \"Zu Beratungen im Rahmen einer Klausur kommt der SPD-Pa… \" Einladu…\n5 07/03/2024 \"\"                                                       \" SPD FRA…\n6 07/03/2024 \"Anlässlich des Internationalen Frauentages 2024 finden… \" SPD FRA…\n\n\nIt seems as if the content column is filled twice; once with an empty string and once with the actual content. This is because the CSS selector I used is not specific enough. For the sake of the example, we will simply filter for an empty string. But you should always make sure that your CSS selectors are specific enough.\n\nspd_pr &lt;- spd_pr |&gt;\n  filter(content != \"\")\n\nspd_pr |&gt; head()\n\n# A tibble: 5 × 3\n  date       content                                                  head_title\n  &lt;chr&gt;      &lt;chr&gt;                                                    &lt;chr&gt;     \n1 12/03/2024 Wenige Tage nach seiner Reise nach Namibia, Südafrika u… \" „Nord-S…\n2 11/03/2024 Zu Beratungen im Rahmen einer Klausur kommt der SPD-Par… \" Einladu…\n3 07/03/2024 Anlässlich des Internationalen Frauentages 2024 finden … \" SPD FRA…\n4 07/03/2024 Die SPD-Spitzenkandidatin für die Europawahl Katarina B… \" Katarin…\n5 06/03/2024 Die SPD-Vorsitzende Saskia Esken kommt nach Nordrhein-W… \" Saskia …\n\n\n\nB.4.3 Speeding up the process with future and furrr",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#process-text-in-r",
    "href": "appendix/webscraping.html#process-text-in-r",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.5 Process text in R",
    "text": "B.5 Process text in R\nThe whole purpose of our scraping was to get textual data which we can then use to analyze the text in automated fashion using methods from Computational Social Sciences. However, handling text in R comes with some additional methods that you need to know.\nEven if you do not directly want to analyze your scraped data, you might still be faced with challenges of data management and data cleaning. Sometimes this is because there was absolutely no way to pick up only the date but every date HTML element now also contains character strings that you want to get rid of. Or let’s assume that you need everything in lower case letters, or get rid of this one ad which your scraper picked up no matter what you tried. This is where regular expressions and the `stringr´ package (which is part of the tidyverse environment) come into play.\n\nB.5.1 regular expressions\nRegular expressions – in coding linguo referred to as regex (singular) or regexes (plural) – are a powerful tool for pattern matching and text manipulation, widely used across various programming languages, including R. Pattern matching in this case simply means that you tell R to look for a specific pattern in a character string (a variable that contains text in its rows) and then do something with it. Sometimes this might be simply one specific word, or even a specific sentence. But sometimes you might need to tell R to look up strings that are 4 digits, then a dot, two digits, another dot, followed by lastly two digits again. This would for example be a date (yyyy.mm.dd). It could happen that you have a lot of dates which were scraped together with other stuff and you only want to extract the dates. Regexes are the perfect tool for this.\nThey allow you to search, replace, split, or extract parts of strings based on specific patterns. Understanding regexes can significantly enhance your ability to work with textual data, making tasks that would be complex or cumbersome to achieve with standard string functions straightforward. Now, unfortunately regexes are not simple and require some learning. But once you have understood the basics, you will be able to do a lot of things with them. And quite frankly, Chat-GPT is a world champion of writing regexes once you know how to prompt it.\n\nB.5.2 stringr package\nIn R, regexes are used together with the stringr package. The stringr package provides a cohesive set of functions designed to make working with strings as easy as possible.\nThe following examples take loose inspiration from Felix Lennert’s CSS Toolbox Script. All of the functions of the stringr package start with str_. They all serve one specific purpose. Below, I explain the most frequently used functions of that package. You can put them into pipes (%&gt;%/|&gt;) and easily use mutate() to create new columns or filter() to filter out rows.\nThis will be our example character string:\n\nexample_string &lt;- \"I love this class and R is fun!\"\nexample_string\n\n[1] \"I love this class and R is fun!\"\n\n\n\n\nstr_detect() checks if a string contains a pattern.\n\n\nstr_detect(example_string, \"love\")\n\n[1] TRUE\n\n\nyou could also construct an object with patterns to detect. This object is what we call a dictionary.\n\ndic &lt;- c(\"love\", \"fun\")\n\nstr_detect(example_string, dic)\n\n[1] TRUE TRUE\n\n\n\n\nstr_count() counts the number of matches in a string.\n\n\nstr_count(example_string, \"is\")\n\n[1] 2\n\n\nbut be careful, this indicates that it is picked up twice because “is” is also included in the word “this”. If you only wanted to pick up the word “is”, you would have to use the regex \\\\bis\\\\b which would only pick up the word “is” if it is a word on its own.\n\nstr_count(example_string, \"\\\\bis\\\\b\")\n\n[1] 1\n\n\n\n\nstr_subset() returns the matching elements of a character vector.\n\n\nstr_subset(example_string, \"is\")\n\n[1] \"I love this class and R is fun!\"\n\n\n\n\nstr_replace() replaces the first occurrence of a pattern in a string with something you indicate. The function is used in a way so that you first feed it your string(s), then the word that ought to be replaced and lastly by what it should be replaced:\n\n\nstr_replace(example_string, \"is\", \"was\")\n\n[1] \"I love thwas class and R is fun!\"\n\n\n\n\nstr_replace_all() replaces all occurrences of a pattern in a string with something you indicate.\n\n\nstr_replace_all(example_string, \"is\", \"was\")\n\n[1] \"I love thwas class and R was fun!\"\n\n\n\n\nstr_split() splits a string into pieces at a given pattern point. Here I specify that the string should be split into different pieces at every space bar by using the two quotation marks with a space in between \" \":\n\n\nstr_split(example_string, \" \")\n\n[[1]]\n[1] \"I\"     \"love\"  \"this\"  \"class\" \"and\"   \"R\"     \"is\"    \"fun!\" \n\n\n\n\nstr_to_lower() converts a string to lower case.\n\n\nstr_to_lower(example_string)\n\n[1] \"i love this class and r is fun!\"\n\n\n\n\nstr_to_upper() converts a string to upper case.\n\n\nstr_to_upper(example_string)\n\n[1] \"I LOVE THIS CLASS AND R IS FUN!\"\n\n\n\n\nstr_trim() removes leading and trailing whitespace from a string. Whitespace are long blank spaces between your characters that might stem from the HTML code.\n\n\nstr_trim(\"   I love this      class and R is fun!   \")\n\n[1] \"I love this      class and R is fun!\"\n\n\n\n\nstr_sub() extracts and/or replaces substrings from a character vector. Here I tell R to extract the first 5 characters of the string.\n\n\nstr_sub(example_string, start = 1, end = 5)\n\n[1] \"I lov\"\n\n\nand as already used in my code somewhere above, you can also index the operation from the end:\n\n# extract strings from fourth-to-last to last character\nstr_sub(example_string, start = -4, end = -1)\n\n[1] \"fun!\"\n\n\n\n\nstr_length() returns the number of characters in a string.\n\n\nstr_length(example_string)\n\n[1] 31\n\n\n\n\nstr_c() concatenates strings.\n\n\nstr_c(\"I\", \"love\", \"this\", \"class\", \"and\", \"R\", \"is\", \"fun!\")\n\n[1] \"IlovethisclassandRisfun!\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#selenium",
    "href": "appendix/webscraping.html#selenium",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.6 Selenium",
    "text": "B.6 Selenium\nWork in Progress!\n\nB.6.1 Dynamic Websites\nWork in Progress!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#internal-website-apis",
    "href": "appendix/webscraping.html#internal-website-apis",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.7 Internal Website APIs",
    "text": "B.7 Internal Website APIs\nWork in Progress!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#minet-plique_minet_2024",
    "href": "appendix/webscraping.html#minet-plique_minet_2024",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.8 Minet (Plique et al. 2024)\n",
    "text": "B.8 Minet (Plique et al. 2024)\n\nThis section is a quick reference to the minet Python package. It is developed by the people working at the Médialab SciencesPo. They are great people who also have a monthly seminar called the METAT which you can attend if you need help with coding projects.\nI would like to emphasize that this is in no way my work but all the work of the people who developed the minet package (Plique et al. 2024). This only serves to put your attention to their work. I would recommend you to read the documentation of the package on GitHub if you want to use it for your own projects. Further, if ever you use it, please do not forget to reference them!\nThe minet package is a Python package that allows you to scrape data from the web. It is a great tool to use if you want to scrape data from social media platforms such as Twitter, Facebook, Instagram, & Co. It works within your Terminal and you can relatively easily scrape a lot of data from Social Media. Go check it out!\n\n\n\n\n\n\nPlease be careful when using minet. For Twitter or Instagram, you will have to be logged in to an account of these to social media to scrape them. I strongly recommend that you use burner accounts. Especially at the beginning, it happens quite easily and quickly that you get banned and will potentially lose the accounts.\n\n\n\nIf you have any questions, feel free to contact me via email.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#ethics-of-webscraping",
    "href": "appendix/webscraping.html#ethics-of-webscraping",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.9 Ethics of Webscraping",
    "text": "B.9 Ethics of Webscraping\nWebscraping is a fun and extremely useful tool of Computational Social Sciences. However, it is important to remember the ethics and issues, as well as the legal aspects that come with it. I recommend that you read this section attentively and take my suggestions seriously. This is not to scare you in any way, but to make you aware of the responsabilities that we have.\nGenerally speaking, webscraping is not immediately illegal. However, it is important to remember that you are scraping data from a website that is not yours. This means that you are using someone else’s data. It is important to respect the data and the website. I am no lawyer and I cannot give you any legal advice. However, I can give you some general advice on how to behave when scraping data from the web.\nPurpose: Always have a clear purpose for your webscraping project. What do you want to achieve? What is the goal of your project? What do you want to do with the data? These are all questions that you should ask yourself before you start scraping data. And then we only scrape the data that we need and we know our purpose for!\nAPIs: Always check for APIs that might be offered by the website. APIs make our life easier and we can play by the rules that the owners of the website dictate. This way we get what we want without disrespecting their rules. Now, the problem is that oftentimes there is no API or the API is not great. In that case, you might have to scrape the data yourself.\nRespect the website: Always respect the website. This means that you should not scrape the website in a way that it crashes. This is not only annoying for the website owner, but this quickly also becomes illegal. How do you crash a website? The easiest way is by sending so many requests in such a short amount of time that the server just gives in and you get a 404 error when going to valid URLs. This we want to avoid at all costs!. So how do we respect websites?\n\n(Try to) Respect robots.txt: The robots.txt file is a file that is located on the server of a website. Simply type in the root of the URL https//:www.sciencespo.fr and then add robots.txt. You will get to a text file in black with white text on it where some rules for crawlers and scrapers are specified. It tells you which parts of the website you are allowed to scrape and which parts you are not allowed to scrape. These are the rules set out by the website. But playing along the most strict rules, prohibits us from both scraping everything that we want and also from having fun. So you might have to break the rules a bit sometimes… ;)\nCheck out the Terms of Service: Some websites explicitly prohibit scraping in their terms of service (ToS). Review these terms to ensure that your scraping activities are not in violation.\nTimeouts: If possible, make requests at a reasonable rate to avoid overwhelming the site’s server. Implement delays between requests. This could for example be one request per second. If it happens to you that a website keeps blocking you for suspicious (scraping) activities, you might also want to set a timeout; preferably you set the time out at a random interval in a specific time frame. The more randomness you introduce to your scraping, the less likely you are to get detected.\n\nBe respectful and thoughtful of the data that you are scraping and where you store it! Depending on the data that you are scraping, it might be more or less sensitive data. Or your data might be subject to copyright law. In itself, the collection of it is not illegal. But what you do with it (and subsequently thus also where you store it) becomes important relatively quickly. If your purpose of scraping is research, you are already on a safer side. Do not use your scraped data for any commercial purposes. My scripts are only destined for people that use scraping for research. Second, always be aware of the GDPR; it is the European regulation of data protection. And there are very good reasons the GDPR exists.\nIf you data is subject to copyright law, then be especially careful. Let’s, hypothetically, assume for a moment that you wanted to scrape newspapers and construct a large corpus of articles. This is a great idea and a great project. However, you would have to be very careful with the data. Only store it locally, do not share it with whomever asks you for it, and watch out for what you use it later on.\nWhen I say store data locally, this implies your local computer, an external hard drive or a USB stick. A cloud service, especially Google Drive, is not a local and secure storing service. Why? Because Google Drive runs on Google’s servers. And we do not want to give indirect access to our copyrighted data to Google. 4 The same goes for Dropbox, OneDrive, and all the other cloud services. Also sharing the data with collaborators should be done locally through hard drives and not services like WeTransfer!\nThe same rules apply for data that you scrape about individuals. This is sensitive data and you should be very careful with it. Anything that would make it possible to attribute anonymous data to a person is sensitive data.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#references",
    "href": "appendix/webscraping.html#references",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "\nB.10 References",
    "text": "B.10 References\n\n\n\n\nPlique, Guillaume, Pauline Breteau, Jules Farjas, Héloïse Théro, Descamps, Amélie Pellé, Laura Miguel, and César Pichon. 2024. “Minet, a Webmining CLI Tool & Library for Python.” Zenodo. https://doi.org/10.5281/ZENODO.4564399.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "appendix/webscraping.html#footnotes",
    "href": "appendix/webscraping.html#footnotes",
    "title": "Appendix B — Introduction to Webscraping",
    "section": "",
    "text": "To be completely frank, RSelenium is a pain in the butt and was one of the reasons why I started to learn Python at some point. And for now – it does seem as if things are changing for the rvest package – I would recommend that you do too. Contact me for questions on this or wait until I update this script and include Python code.↩︎\nNote that you can also use it as a search bar. If you are unsure about the path to your element of interest, you can type it in and it will highlight all the elements that belong to it in yellow.↩︎\nPlease note that I am writing this script in February 2024. The number 111 will not be up to date in a couple of days as the party keeps releasing press releases. Your code might have to be adapted slightly but that is not an issue usually.↩︎\nYou might amend that Google probably already has the newspaper data that we might scrape. And you are probably more than right. But we do not want to get into trouble and we should care about these things on our end. What they do is not our business.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Introduction to Webscraping</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#introduction",
    "href": "session6/session6.html#introduction",
    "title": "\n4  Time Series\n",
    "section": "",
    "text": "I would also like to emphasize that this week’s code is inspired by a publication by Abou-Chadi (2016). I have not had the time to come up with a witty coding idea myself and the paper by Abou-Chadi came with a replication dataset. The TSCS model with which we are going to play around with does not follow the exact same method but is loosely based on his paper and generates somewhat close results. The purpose of this script is to expose you to the logic of ARIMA models, TSCS, especially to fixed and random effects models.\n\n4.1.1 Some Thoughts on Time Series in general…\nNow, you might be wondering if you are ever going to use these seemingly incomprehensible formulas, equations and terms like Error Correction Model, lagged dependent variable or structural equation approach. The boring answer is that this is up to you. It was up to me too at one point. Remember that I took this class too, had my mind made up about a multinomal model I wanted to do and was pretty sure that I would never dig deeper into time series. Little did I know that your research interests sometimes force you to learn methods you thought you would never need. This happened to me a couple months later, when at the beginning of my third semester I wanted to work on parties and the evolution of party positions in response to other party’s shifts. Jan suggested that I would have to look at time series at some point. I did and I have come to hate and to like them. I did not say love them, I like them. They are still quite awful in terms of math and push me to the boundaries of what I can grasp…\nTime series are one of the best statistical methods to understand social and many other change(s) over time. They allow you to understand causal relationships between many kinds of variables over time and most importantly the drivers of that relationship or change across time. There is a huge abundance of questions that relate to changes over time that we are interested in explaining. It does not have to be limited to my boring party competition stuff: what about crime and approval ratings, and then trying to see whether there is a statistical relationship between increases in crime and decreases in approval ratings.\n\n4.1.2 … and more specifically on Time Series in R\nThis is now the 5th session and you might still believe me that R can be fun, and that it can be easy in some cases. The logic and math behind time series analysis can be tricky, I am aware of that. But you will see that many things are taken care of by packages like the plm or tseries packages for panel data analysis (Whatever that is for now…) or general time series analyses. Some things we still need to do ourselves, especially the specification of our ARIMA models which you have seen in Jan’s class. I will try to further give you some help on that as well as some other tricks!",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#what-is-time-series-cross-section-tscs",
    "href": "session6/session6.html#what-is-time-series-cross-section-tscs",
    "title": "\n4  Time Series\n",
    "section": "\n4.2 What is Time Series Cross Section (TSCS)?",
    "text": "4.2 What is Time Series Cross Section (TSCS)?\nI suppose you have seen this with Jan, so I am only going to briefly recapitulate the idea of TSCS. Generally speaking, we have two things going on with which we can have a lot of data fun. First, we have observations for more than one unit (e.g., countries, parties) and for more than one point in time. If you wish to work with TSCS models, you first need to find a dataset that has enough observations over time and in more than one unit. As a rough guesstimation, I would say 20 observations at least as the bare minimum, ideally many more. Unfortunately, this is where data availability can be an issue 2 In the next paragraphs, I will try to show you what we need to understand to conduct TSCS analyses in R. For all the tricky math stuff, please refer to Jan’s lecture which gives a much better and thorough overview on everything that is going on underneath the hood of TSCS and its assumptions. If you have any questions on this, he is the better reference than me but I can try my best in answering your questions.\n\n4.2.1 Data Structure\nAs we will work with the CMP, I know that we have enough observations to have time series. However, we must make sure that the structure is according to time series across units. The data structure (and you might have seen this table in Jan’s document) should look somewhat like this:\nHere we have a structure that allows us to do TSCS models. The data is structured in a way that it follows the order of unit per year.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#the-comparative-manifesto-project",
    "href": "session6/session6.html#the-comparative-manifesto-project",
    "title": "\n4  Time Series\n",
    "section": "\n4.3 The Comparative Manifesto Project",
    "text": "4.3 The Comparative Manifesto Project\nLet’s import the CMP dataset. As you can see country 41 corresponds to Germany. I have imported some variables of the CMP as well but for now, we will simply look at the variable rile. It’s the CMP’s special measure of an aggregated left-right position of parties based on the ideas of the saliency theory by Budge and Farlie (1983).\n\nlibrary(tidyverse)\n\n\n# for the download of the dataset, please click on the link for \n# of this script the CMP in this script\ncmp &lt;- read_csv(\"data/cmp.csv\") |&gt; \n  # I only import the variables I am interested in; check the codebook first\n  # ALWAYS\n  select(country, partyname, date, rile, per602_2, per601_2, per607, per608) |&gt;\n  # this is an old code I have used; 41 = Germany (check out the codebook)\n  filter(country == 41) |&gt; \n  # here I align the names as there are some inconsistencies over the years\n  mutate(party = case_when(\n    partyname == \"Alliance‘90/Greens\" ~ \"Grüne\",\n    partyname == \"Greens/Alliance‘90\" ~ \"Grüne\",\n    partyname == \"Alternative for Germany\" ~ \"AFD\",\n    partyname == \"Christian Democratic Union/Christian Social Union\" ~ \"CDU/CSU\",\n    partyname == \"Free Democratic Party\" ~ \"FDP\",\n    partyname == \"Party of Democratic Socialism\" ~ \"Linke\",\n    partyname == \"Social Democratic Party of Germany\" ~ \"SPD\",\n    partyname == \"The Left\" ~ \"Linke\",\n    TRUE ~ NA_character_\n  ),\n  year = str_sub(date, 1, 4))\n\nRows: 4778 Columns: 174\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (7): countryname, edate, partyname, partyabbrev, corpusversion, datase...\ndbl (167): country, oecdmember, eumember, date, party, parfam, coderid, manu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n4.3.1 Quick excursion: the stringr package\nIn the last line of the code, I create a new variable called year. Since the CMP is based on party manifestos, the initial date variable is estimated for the respective election day. But we are only interested in the year that the election took place. The function I use, str_sub() is from the stringr package which is one of the most useful ones you can find in R. We will talk about it again in the next session when we will have to transform textual data in R. What this function does is that it will take the first the fourth character of the variable and substract/delete all the others. I do this because the intital date variable follows the pattern of “yyyy/mm/dd”. And as I only want to have the “yyyy” part, I take the first four characters (or in our case numbers). If I wanted create a variable yearmonth for example, I would have specified str_sub(date, 1, 6). By the way, if you ever want to indicate in R that you want something until the value of something, you can do so by specifying -1. In our case, let’s say I only wanted to get the month and the day of the date variable (which would be the 5th to 8th character), I could specify it as follows: str_sub(date, 5, -1).\nEnough theoretical talk about functions. Let’s look at the left-right estimations of the CMP per party:\n\nggplot(cmp, aes(x = year, y = rile, color = party)) +\n  geom_point() +\n  stat_smooth(aes(group = party), method = \"loess\", se = FALSE) +\n  scale_color_manual(values = c(\"#20aefa\", \"#000000\", \"#FFFF00\", \"#00e81b\",\n                                \"#f26dd5\", \"#fa2024\")) +\n  # without this line, the lables of the x-axis would overlap\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +\n  theme_minimal()\n\n\n\n\n\n\n\nThis gives us the smoothed means for every party over the span of 1949 until 2021. We should start with a . This means that we will look at the evolution of one series over time; in our case, this specifically means that we will look at the evolution of the left-right position of over time. Let’s start with the Christian-Democrats, the CDU, and the Social-Democrats, the SPD. I will quickly create an object, only containing values of the CDU and another one for only the SPD.\n\ncdu &lt;- cmp |&gt; \n  select(party, rile, year) |&gt; \n  filter(party == \"CDU/CSU\")\n\nspd &lt;- cmp |&gt; \n  select(party, rile, year) |&gt; \n  filter(party == \"SPD\")\n\nWe are, however, not interested in the mean position over time but the inidividual values over time. The first thing you should always do, when working with time series data, is to plot your variable of interest. It’s like looking at a data set when you first import it. Sometimes you can already detect things like seasonality that we will test for either way:\n\nggplot(cdu, aes(year, rile)) + \n  geom_line(aes(group = party)) + \n  geom_point(alpha = 0.5, shape=20, size=2) +\n   labs(title = \"RILE CDU/CSU over time\", x = \"Year\", y = \"Right to Left\") +\n  theme_minimal()",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#footnotes",
    "href": "session6/session6.html#footnotes",
    "title": "\n4  Time Series\n",
    "section": "",
    "text": "He is absolutely right in reminding me to work on my theory :(↩︎\nUntil we see many more ways to get to data in the next session, when we speak about quantitative text analysis.↩︎\nIf you are wondering why we are using multiculturalism and not immigration, you’re raising a good question. It is not the authors fault but problems of the CMP measures. But as I said, I will not go into too much detail on my criticism of the CMP’s measures.↩︎",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Time Series</span>"
    ]
  }
]