[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced RStudio Labsessions",
    "section": "",
    "text": "Course Overview\nThis repository contains all the course material for the RStudio Labsessions for the Spring semester 2024 at the School of Research at SciencesPo Paris. The class follows Brenda van Coppenolle’s and Jan Rovny’s lecture on Quantitative Methods II. Furthermore, the RStudio part of the course is a direct continuation of Malo Jan’s RStudio introduction course. If you feel the need to go back to some basics of general R use, data management or visualization, feel free to check out his course’s website. Rest assured, however, that 1) we will recap plenty of things, 2) make slow but steady progress, 3) and come back to the essentials of data wrangling again during the semester while building statistical models.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Structure",
    "text": "Course Structure\nIn total we will see each other 6 times. The lessons will be structured in such a way that I will first present something to you and explain my script. Ideally, you will then start coding in groups of 2 and work on exercises related to the topic. You can find more information about the exercises in the subsection “course validation”. I will of course be there to help you. The rest you solve at home and send me your final script. At the beginning of each next meeting we will go through the solutions together. Also, I upload my own script before each session, so you can use it as a template when solving the tasks and also later, when the course is over, as a template for further coding (if you like of course…).\n\n\n\n\n\n\n\n\nSession\nDescription\nDates\n\n\n\n\nSession 1\nRStudio Recap & OLS\n01/02 & 08/02\n\n\nSession 2\nLogistic Regressions\n15/02 & 29/02\n\n\nSession 3\nMultinomial Regression\n07/03 & 14/03\n\n\nSession 4\nCausal Inference I\n21/03 & 28/03\n\n\nSession 5\nCausal Inference II\n04/04 & 11/04\n\n\nSession 6\nTime Series\n18/04 & 25/04",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-validation",
    "href": "index.html#course-validation",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Validation",
    "text": "Course Validation\nIn the two weeks between each lecture, you will be given exercises to upload to the designated link for each session. The document where you write the solutions must be written in Markdown format.\nI will grade your solutions to my exercises on a 0 to 5 scale. I would like to see that you have done something and hopefully finished the exercise. If you are unable to finish the exercise, it is no problem and I do understand that not everybody feels as comfortable with R as some other people might do. Handing something in is key to getting points! This class can be finished by everyone and I do not want you to worry about your grade too much. But I would like that you all at least try to solve the exercises! Work in groups of two and try to hand in something after each session. The precise deadline will be communicated in class, the course’s GitHub page and on the Moodle page.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#optional-course-parts",
    "href": "index.html#optional-course-parts",
    "title": "Advanced RStudio Labsessions",
    "section": "Optional Course Parts",
    "text": "Optional Course Parts\nWhen I taught the course last year, some students approached me and asked for several levels of difficulty. I will try to implement this in the homework and in class. I have also decided to add an optional part to each session. In the optional parts, I will introduce new packages, advanced methods, and I will also upload a few scripts in the appendix on things like text-as-data, webscraping or similar, if I have the time. Also – again, if the times allows it – I will go through these optional parts in class. But please be assured that if you decide not to follow the optional parts, that is okay. But if you do, I can promise you will make better and faster progress. Lastly, if you are interested in certain things, want to learn about specific methods or how to implement things or workflows in RStudio, please do not hesitate to contact me and I will see if I can squeeze it in somewhere.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Advanced RStudio Labsessions",
    "section": "Requirements",
    "text": "Requirements\nYou must have downloaded R and RStudio by the beginning of the course (you need to install both!) before our sessions. Please let me know if you encounter any problems during the installation. Here is a quick guide on how to do that: https://rstudio-education.github.io/hopr/starting.html\nR and RStudio are both free and open source. You need both of them installed in order to operate with the R coding language.\nFor R, go on the CRAN website and download the file for your respective operating system: https://cran.r-project.org/ For RStudio, you need to do the same thing by clicking on this link: https://posit.co/products/open-source/rstudio/ RStudio has received a new name recently (“posit”) but you will still find all the necessary steps behind this link under the name of RStudio.\nOtherwise, there are few prerequisites except that you must bring your computer to the sessions with the required programs installed. I will provide you with datasets in each case and I will explain everything else in the course.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#help-and-office-hours",
    "href": "index.html#help-and-office-hours",
    "title": "Advanced RStudio Labsessions",
    "section": "Help and Office Hours",
    "text": "Help and Office Hours\nThere are unfortunately no regular office hours. But please do not hesitate to reach out, if you have any concerns, questions or feedback for me! My inbox is always open. I tend to reply quickly but in the case that I have not replied in under 48h, simply send the email again. I will not be offended!\nLearning how to code and working with RStudio can be a struggle and a tough task. I have started out once like you and I will try to keep that in mind. Feel free to always ask questions in class or if you see me on campus. The most important thing, however, is that you try!",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Advanced RStudio Labsessions",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis is only a quick section to give credit, where credit is due! Malo Jan is one of my daily inspirations for anything that has to do with research and RStudio. I have taught this class already last year and had most of my scripts written in PDFs but Rohan Alexander’s book Telling Stories with Data served as a new inspiration to write this course in its Quarto book format. Also shout outs to Felix Lennert and some of his ideas for the homework.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "1.1 Introduction\nThis is a short recap of things you have seen last year and will need this year as well. It will refresh your understanding of the linear regression method called ordinary least squares (OLS). This script is supposed to serve as a cheat sheet for you to which you can always come back to.\nThese are the main points of today’s session and script:\nThese are the packages we will be using in this session:\nneeds(\n  tidyverse,\n  rio, \n  stargazer,\n  broom,\n)",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#introduction",
    "href": "session1/session1.html#introduction",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "A refresher of Ordinary Least Squares (OLS)\nWhat is Base R and what are packages?\nA recap of basic coding in R\nBuilding & Interpreting a simple linear model\nVisualizing Residuals\nThe broom package (OPTIONAL!)",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#ordinary-least-squares-ols",
    "href": "session1/session1.html#ordinary-least-squares-ols",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.2 Ordinary Least Squares (OLS)",
    "text": "1.2 Ordinary Least Squares (OLS)\nOLS regressions are the powerhouse of statistics. The world must have been a dark place without them. They are the most basic form of linear regression and are used to predict the value of a dependent variable (DV) based on the value of independent variables (IVs). It is important to note that the relationship between the DV and the IVs is assumed to be linear.\nAs a quick reminder, this is the formula for a basic linear model: \\(\\widehat{Y} = \\widehat{\\alpha} + \\widehat{\\beta} X\\).\nOLS is a certain kind of method of linear model in which we choose the line which has the least prediction errors. This means that it is the best way to fit a line through all the residuals with the least errors. It minimizes the sum of the squared prediction errors \\(\\text{SSE} = \\sum_{i=1}^{n} \\widehat{\\epsilon}_i^2\\)\nFive main assumptions have to be met to allow us to construct an OLS model:\n\nLinearity: Linear relationship between IVs and DVs\nNo endogeneity between \\(y\\) and \\(x\\)\n\nErrors are normally distributed\nHomoscedasticity (variance of errors is constant)\nNo multicolinearity (no linear relationship between the independent variables)\n\nFor this example, I will be working with some test scores of a midterm and a final exam which I once had to work through. We are trying to see if there is a relationship between the score in the midterm and the grade of the final exam. Theoretically speaking, we would expect most of the students who did well on the first exam to also get a decent grade on the second exam. If our model indicates a statistical significance between the independent and the dependent variable and a positive coefficient of the former on the latter, this theoretical idea then holds true.",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#coding-recap",
    "href": "session1/session1.html#coding-recap",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.3 Coding Recap",
    "text": "1.3 Coding Recap\nBefore we start, let’s refresh our coding basics again. RStudio works with packages and libraries. There is something called Base R, which is the basic infrastructure that R always comes with when you install it. The R coding language has a vibrant community of contributors who have written their own packages and libraries which you can install and use. As Malo, I am of the tidyverse school and mostly code with this package or in its style when I am wrangling with data, changing its format or values and so on. Here and there, I will, however, try to provide you with code that uses Base R or other packages. In coding, there are many ways to achieve the same goal – and I will probably be repeating this throughout the semester – and we always strive for the fastest or most automated way. I will not force the tidyverse environment on you but I do think that it is one of the most elegant and fastest way of doing statistics in R. It is sort of my RStudio dialect but you can obviously stick to yours if you have found it. Also, as long as you find a way that works for you, that is fine with me!\nTo load the packages, we are going to need:\n\nlibrary(tidyverse)\n\nNext we will import the dataset of grades.\n\ndata &lt;- read_csv(\"course_grades.csv\")\n\nRows: 200 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): midterm|final_exam|final_grade|var1|var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe path which I specify in the read_csv file is short as this quarto document has the same working directory to which the data set is also saved. If you, for example, have your dataset on your computer’s desktop, you can access it via some code like this one:\n\ndata &lt;- read_csv(\"~/Desktop/course_grades.csv\")\n\nOr if it is within a folder on your desktop:\n\ndata &lt;- read_csv(\"~/Desktop/folder/course_grades.csv\")\n\n\n\n\n\n\n\nI will be only working within .Rproj files and so should you. 1 This is the only way to ensure that your working directory is always the same and that you do not have to change the path to your data set every time you open a new RStudio session. Further, this is the only way to make sure that other collaborators can easily open your project and work with it as well. Simply zip the file folder in which you have your code and\n\n\n\nYou can also import a dataset directly from the internet. Several ways are possible that all lead to the same end result:\n\ndataset_from_internet_1 &lt;- read_csv(\"https://www.chesdata.eu/s/1999-2019_CHES_dataset_meansv3.csv\")\n  \n# this method uses the rio package\nlibrary(rio)\ndataset_from_internet_2 &lt;- import(\"https://jan-rovny.squarespace.com/s/ESS_FR.dta\")\n\nLet’s take a first look at the data which we just imported:\n\n# tidyverse\nglimpse(data)\n\nRows: 200\nColumns: 1\n$ `midterm|final_exam|final_grade|var1|var2` &lt;chr&gt; \"17.4990613754243|15.641013…\n\n# Base R\nstr(data)\n\nspc_tbl_ [200 × 1] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ midterm|final_exam|final_grade|var1|var2: chr [1:200] \"17.4990613754243|15.64101334897|17.63|NA|NA\" \"17.7446326301825|18.7744366510731|14.14|NA|NA\" \"13.9316618079058|14.9978584022336|18.2|NA|NA\" \"10.7068243984724|11.9479428399047|19.85|NA|NA\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `midterm|final_exam|final_grade|var1|var2` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nSomething does not look right, this happens quite frequently when saving a csv file. It stands for comma separated value. R is having trouble reading this file since I have saved all grades with commas instead of points. Thus, we need to use the read_delim() function. Sometimes the read_csv2() function also does the trick. You’d be surprised by how often you encounter this problem. This is simply to raise your awareness to it!\nThe read_delim() function is the overall function of the readr package to read any sort of data file, whereas read_csv() and read_csv2() are specific functions to read csv files. The read_delim() function has a delim argument which you can use to specify the delimiter of your data file. For the sake of the example, I had purposefully saved the csv file using the | delimiter.\n\ndata &lt;- read_delim(\"course_grades.csv\", delim = \"|\")\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"|\"\ndbl (3): midterm, final_exam, final_grade\nlgl (2): var1, var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data)\n\nRows: 200\nColumns: 5\n$ midterm     &lt;dbl&gt; 17.499061, 17.744633, 13.931662, 10.706824, 17.118799, 17.…\n$ final_exam  &lt;dbl&gt; 15.641013, 18.774437, 14.997858, 11.947943, 15.694728, 17.…\n$ final_grade &lt;dbl&gt; 17.63, 14.14, 18.20, 19.85, 14.67, 20.26, 16.90, 13.40, 12…\n$ var1        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ var2        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nThis time, it has been properly imported. But by looking closer at it, we can see that there are two columns in the data frame that are empty and do not even have a name. We need to get rid of these first. Here are several ways of doing this. In coding, many ways lead to the same goal. In R, some come with a specific package, some use Base R. It is up to you to develop your way of doing things.\n\n# This is how you could do it in Base R\ndata &lt;- data[, -c(4, 5)]\n\n# Using the select() function of the dplyr package you can drop the fourth\n# and fifth columns by their position using the - operator and the -c() to\n# remove multiple columns\ndata &lt;- data  |&gt;  select(-c(4, 5))\n\n# I have stored the mutated data set in the old object; \n# you can also just transform the object itself...\ndata |&gt; select(-c(4, 5))\n\n# ... or create a new one\ndata_2 &lt;- data |&gt; select(-c(4, 5))\n\nNow that we have set up our data frame, we can build our OLS model. For that, we can simply use the lm() function that comes with Base R, it is built into R so to speak. In this function, we specify the data and then construct the model by using the tilde (~) between the dependent variable and the independent variable(s). Store your model in an object which can later be subject to further treatment and analysis.\n\nmodel &lt;- lm(final_exam ~ midterm, data)\nsummary(model)\n\n\nCall:\nlm(formula = final_exam ~ midterm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6092 -0.8411 -0.0585  0.8712  3.3086 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.62482    0.73212   6.317 1.72e-09 ***\nmidterm      0.69027    0.04819  14.325  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.34 on 198 degrees of freedom\nMultiple R-squared:  0.5089,    Adjusted R-squared:  0.5064 \nF-statistic: 205.2 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nSince the summary() function only shows us something in our console and the output is not very pretty, I encourage you to use the broom package for a nicer regression table.\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.62     0.732       6.32 1.72e- 9\n2 midterm        0.690    0.0482     14.3  2.10e-32\n\n\nYou can also use the stargazer package in order to export your tables to text or LaTeX format which you can then copy to your documents.\n\nlibrary(stargazer)\nstargazer(model, type = \"text\", out = \"latex\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                            final_exam         \n-----------------------------------------------\nmidterm                      0.690***          \n                              (0.048)          \n                                               \nConstant                     4.625***          \n                              (0.732)          \n                                               \n-----------------------------------------------\nObservations                    200            \nR2                             0.509           \nAdjusted R2                    0.506           \nResidual Std. Error      1.340 (df = 198)      \nF Statistic          205.196*** (df = 1; 198)  \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#interpretation-of-ols-results",
    "href": "session1/session1.html#interpretation-of-ols-results",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.4 Interpretation of OLS Results",
    "text": "1.4 Interpretation of OLS Results\nHow do we interpret this?\n\n\nR2: Imagine you’re trying to draw a line that best fits a bunch of dots (data points) on a graph. The R-squared value is a way to measure how well that line fits the dots. It’s a number between 0 and 1, where 0 means the line doesn’t fit the dots at all and 1 means the line fits the dots perfectly. R-squared tells us how much of the variation in the dependent variable is explained by the variation in the predictor variables.\n\nAdjusted R2: Adjusted R-squared is the same thing as R-squared, but it adjusts for how many predictor variables you have. It’s like a better indicator of how well the line fits the dots compared to how many dots you’re trying to fit the line to. It always adjusts the R-squared value to be a bit lower so you always want your adjusted R-squared value to be as high as possible.\n\nResidual Std. Error: The residual standard error is a way to measure the average distance between the line you’ve drawn (your model’s predictions) and the actual data points. It’s like measuring how far off the line is from the actual dots on the graph. Another way to think about this is like a test where you want to get as many answers correct as possible and if you are off by a lot in your answers, the residual standard error would be high, but if you are only off by a little, the residual standard error would be low. So in summary, lower residual standard error is better, as it means that the model is making predictions that are closer to the true values in the data.\n\nF Statistics: The F-statistic is like a test score that tells you how well your model is doing compared to a really simple model. It’s a way to check if the model you’ve built is any better than just guessing. A large F-statistic means that your model is doing much better than just guessing.",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#visualizing-the-regression-line",
    "href": "session1/session1.html#visualizing-the-regression-line",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.5 Visualizing the Regression Line",
    "text": "1.5 Visualizing the Regression Line\nJust for fun and to refresh you ggplot knowledge, let’s visualize the regression line. Here, we specify\n\nggplot(data, aes(x = midterm, y = final_exam)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n    labs(\n    title = \"Relationship between Midterm and Final Exam Scores\",\n    x = \"Midterm Scores\",\n    y = \"Final Exam Scores\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nRemember how I told you above that OLS was about finding the smallest amount of squared errors! This is what we can visualize here. The red lines are the distance from the residuals to the fitted line. The OLS line is the line that minimizes the sum of the squared errors!\n\nggplot(data, aes(x = midterm, y = final_exam)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # highlight the OLS logic graphically\n  geom_segment(\n    aes(\n      x = midterm,\n      y = final_exam,\n      xend = midterm,\n      yend = predict(model),\n      color = \"red\",\n      alpha = 0.5,\n    )) +\n  labs(title = \"Relationship between Midterm and Final Exam Scores\",\n       x = \"Midterm Scores\",\n       y = \"Final Exam Scores\") +\n  guides(color = FALSE, alpha = FALSE) +\n  theme_minimal()\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#the-broom-package-optional",
    "href": "session1/session1.html#the-broom-package-optional",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.6 The broom package (OPTIONAL!)",
    "text": "1.6 The broom package (OPTIONAL!)\nThe broom package in R is designed to bridge the gap between R’s statistical output and tidy data. 2 It takes the output of various R statistical functions and turns them into tidy data frames. This is particularly useful because many of R’s modeling functions return outputs that are not immediately suitable for further data analysis or visualization within the tidyverse framework.\n\n\n\n\n\n1.6.1 Nesting with nest()\n\nNesting in the context of the broom package usually refers to the idea of creating a list-column in a data frame (or even better a tibble) where each element of this list-column is itself a data frame (again, even better a tibble) or a model object. In a complex analysis, you might fit separate models to different subsets of data. Nesting allows you to store each of these models (or their broom-tidied summaries that we will see in the next three sub-sections) within a single, larger data frame (for the third time, the best is to do this with a tibble) for easier manipulation and analysis. For questions on what tibbles are see the below section on tibbles.\nLet’s go back to the midterm data which I have used before and randomly assign students into two classes (Class A or Class B) pretending that two different classes of students had taken the exams. I will then later on nest the data by class so that you can understand the logic of nesting.\n\ndata &lt;- data |&gt; \n  mutate(class = sample(c(\"A\", \"B\"), size = nrow(data), replace = TRUE))\n\nNow I will group my observations by class using group_by() and then nest them within these groups. The output will only contain two rows, class A and class B, and each row will contain a tibble with the observations of the respective class.\n\ndata |&gt; \n  group_by(class) |&gt;\n  nest()\n\n# A tibble: 2 × 2\n# Groups:   class [2]\n  class data              \n  &lt;chr&gt; &lt;list&gt;            \n1 A     &lt;tibble [91 × 3]&gt; \n2 B     &lt;tibble [109 × 3]&gt;\n\n\nThat is a very simple application of a nesting process. You can group_by() and nest() by many different variables. You might want to nest your observations per year or within countries. You can also nest by multiple variables at the same time. We will see this idea again in the next session when we will talk about the purrr package and how to automatically run regressions for several countries at the same time\n\n1.6.2 Model estimates with tidy()\n\nThe tidy() function takes statistical output ofa model and turns it into a tidy tibble. This means each row is an observation (e.g., a coefficient in a regression output) and each column is a variable (e.g., estimate, standard error, statistic). For instance, after fitting a linear model, you can use tidy() to create a data frame where each row represents a coefficient, with columns for estimates, standard errors, t-values, and p-values.\n\n1.6.3 Key metrics with glance()\n\nglance() provides a one-row summary of a model’s information. It captures key metrics that describe the overall quality or performance of a model, like \\(R^2\\), AIC, BIC in the context of linear models. This is useful for getting a quick overview of a model’s performance metrics.\n\n1.6.4 Residuals with augment()\n\nThe augment() function adds information about individual observations to the original data, such as fitted values or residuals in a regression model. You want to do this when you are evaluating a model fit at the observation level, checking for outliers, or understanding the influence of individual data points. We will talk more about model diagnostics in the next session!\nLet’s take a look at how this would work out in practice. For simplicity sake, we will set the nest() logic aside for a second and only look at the tidy(), glance(), and augment() functions.\nHere I only build the same model that we have already seen above:\n\nmodel &lt;- lm(final_exam ~ midterm, data = data)\n\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.62     0.732       6.32 1.72e- 9\n2 midterm        0.690    0.0482     14.3  2.10e-32\n\n\nNow let’s glance the hell out of the model:\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.509         0.506  1.34      205. 2.10e-32     1  -341.  689.  699.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nDon’t worry about what these things might mean for now, AIC and BIC will for example come up again next session.\n\n1.6.5 What is a tibble?",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#footnotes",
    "href": "session1/session1.html#footnotes",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "Malo’s explanation and way of introducing you to RStudio projects can be found here.↩︎\nAs a quick reminder these three principles are guiding when we speak of tidy data: 1) Every column is a variable, 2) Every row is an observation, 3) Every cell is a single value.↩︎",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session2/session2.html",
    "href": "session2/session2.html",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "2.1 Introduction\nYou have seen the logic of Logistic Regressions with Professor Rovny in the lecture. In this lab session, we will understand how to apply this logic to R and how to build a model, interpret and visualize its results and how to run some diagnostics on your models. If the time allows it, I will also show you how automatize the construction of your model and run several logistic regressions for many countries at once.\nThese are the main points of today’s session and script:",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#introduction",
    "href": "session2/session2.html#introduction",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "Getting used to the European Social Survey\nCleaning data: dropping rows, columns, creating and mutating variables\nBuilding a generalized linear model (glm()); special focus on logit/probit\nExtracting and interpreting the coefficients\nVisualization of results\nIntroduction to the purrr package and automation in RStudio (OPTIONAL!)",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#data-management-data-cleaning",
    "href": "session2/session2.html#data-management-data-cleaning",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.2 Data Management & Data Cleaning",
    "text": "2.2 Data Management & Data Cleaning\nAs I have mentioned last session, I will try to gradually increase the data cleaning part. It is integral to R and operationalizing our quantitative questions in models. A properly cleaned data set is worth a lot. This time we will work on how to drop values of variables (and thus rows of our dataset) which we are either not interested in or, most importantly, because they skew our estimations.\n\n# these are the packages, I will need for this session \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#importing-the-data",
    "href": "session2/session2.html#importing-the-data",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.3 Importing the data",
    "text": "2.3 Importing the data\nWe have seen how to import a dataset. Create an .Rproj of your choice and create a folder in which the dataset of this lecture resides. You can download this dataset from our Moodle page. I have pre-cleaned it a bit. If you were to download this wave of the European Social Survey from the Internet, it would be a much bigger data set. I encourage you to do this and try to figure out ways to manipulate your data but for now, we’ll stick to the slightly cleaner version.\n\n# importing the data; if you are unfamiliar with this operator |&gt; , ask me or\n# go to my document \"Recap of RStudio\" which you can find on Moodle\ness &lt;- read_csv(\"ESS_10_fr.csv\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs you can see from the dataset’s name, we are going to work with the European Social Survey. It is the biggest, most comprehensive and perhaps also most important survey on social and political life in the European Union. It comes in waves of two years and all the European states which want to pay for it produce their own data. In fact, the French surveys (of which we are going to use the most recent, 10th wave) are produced at SciencesPo, at the Centre de Données Socio-Politiques (CDSP)!\nThe ESS is extremely versatile if you need a broad and comprehensive data set for both national politics in Europe or to compare European countries. Learning how to use it, how to manage and clean the ESS waves will give you all the instruments to work with almost any data set that is “out there”. Also, some of you might want to use the ESS waves for your theses or research papers. There is a lot that can be done with it, not only cross-sectionally but also over time. So give it a try :)\nEnough advertisement for the ESS, let’s get back to wrangling with our data! As always, the first step is to inspect (“glimpse”) at our data and the data frame’s structure. We do this to see if obvious issues arise at a first glance.\n\nglimpse(ess)\n\nRows: 33,351\nColumns: 25\n$ name     &lt;chr&gt; \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS1…\n$ essround &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ edition  &lt;dbl&gt; 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2…\n$ proddate &lt;chr&gt; \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.1…\n$ idno     &lt;dbl&gt; 10002, 10006, 10009, 10024, 10027, 10048, 10053, 10055, 10059…\n$ cntry    &lt;chr&gt; \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"…\n$ dweight  &lt;dbl&gt; 1.9393836, 1.6515952, 0.3150246, 0.6730366, 0.3949991, 0.8889…\n$ pspwght  &lt;dbl&gt; 1.2907065, 1.4308782, 0.1131722, 1.4363747, 0.5848892, 0.6274…\n$ pweight  &lt;dbl&gt; 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177…\n$ anweight &lt;dbl&gt; 0.28100810, 0.31152576, 0.02463945, 0.31272244, 0.12734002, 0…\n$ prob     &lt;dbl&gt; 0.0003137546, 0.0003684259, 0.0019315645, 0.0009040971, 0.001…\n$ stratum  &lt;dbl&gt; 185, 186, 175, 148, 138, 182, 157, 168, 156, 135, 162, 168, 1…\n$ psu      &lt;dbl&gt; 2429, 2387, 2256, 2105, 2065, 2377, 2169, 2219, 2155, 2053, 2…\n$ polintr  &lt;dbl&gt; 4, 1, 3, 4, 1, 1, 3, 3, 3, 3, 1, 4, 2, 2, 3, 3, 2, 2, 4, 2, 3…\n$ trstplt  &lt;dbl&gt; 3, 6, 3, 0, 0, 0, 5, 1, 2, 0, 5, 4, 7, 5, 2, 2, 2, 2, 0, 3, 0…\n$ trstprt  &lt;dbl&gt; 3, 7, 2, 0, 0, 0, 3, 1, 2, 0, 7, 4, 2, 6, 2, 1, 3, 1, 0, 3, 3…\n$ vote     &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2…\n$ prtvtefr &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ clsprty  &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2…\n$ gndr     &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1…\n$ yrbrn    &lt;dbl&gt; 1945, 1978, 1971, 1970, 1951, 1990, 1981, 1973, 1950, 1950, 1…\n$ eduyrs   &lt;dbl&gt; 12, 16, 16, 11, 17, 12, 12, 12, 11, 3, 12, 12, 15, 15, 19, 11…\n$ emplrel  &lt;dbl&gt; 1, 3, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1…\n$ uemp12m  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n$ uemp5yr  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n\n\nAs we can see, there are many many variables (25 columns) with many many observations (33351). Some are quite straight-forward and the name is clear (“essround”, “age”) and some much less. Sometimes we can guess the meaning of a variable’s name. But most of the time – either because guessing is too annoying or because the abbreviation is not making any sense – we need to turn to the documentation of the data set. You can find the documentation of this specific version of the data set in an html-file on Moodle (session 2).\nEvery (good and serious) data set has some sort of documentation somewhere. If not, it is not a good data set and I am even tempted to say that we should be careful in using it! The documentation for data sets is called a code book. Code books are sometimes well crafted documents and sometimes just terrible to read. In this class, you will be exposed to both kinds of code books in order to familiarize you with both.\nIn fact, this dataframe still contains many variables which we either won’t need later on or that are simply without any information. Let’s get rid of these first. This is a step which you can also do later on but I believe that it is smart to this right at the beginning in order to have a neat and tidy data set from the very beginning.\nYou can select variables (select()) right at the beginning when importing the csv file.\n\ness &lt;- read_csv(\"ESS_10_fr.csv\")  |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  )\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHowever, I am realizing that when looking at the number of rows that my file is a bit too large for only one wave and only one country. By inspecting the ess$cntry variable, I can see that I made a mistake while downloading the dataset because it contains all countries of wave 10 instead of just one. We can fix this really easily when importing the dataset:\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  ) |&gt;\n  filter(cntry != \"FR\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis only leaves us with the values for France!\n\n2.3.0.1 Cleaning our DV\nAt this point, you should all check out the codebook of this data set and take a look at what the values mean. If we take the variable of ess$vote for example, we can see that there are many numeric values of which we can make hardly any sense (without guessing and we don’t do this over here) of what they might stand for.\n\nsummary(ess$vote) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.415   2.000   9.000 \n\n# remember that you can summary() both dataframes and individual variables\n\nOr in a table containing the amount of times that a value was given:\n\ntable(ess$vote)\n\n\n    1     2     3     7     8     9 \n22504  6542  1939   179   165    45 \n\n\nHere we can see that the variable vote contains the numeric values of 1 to 3 and then 7, 8, and 9. If we take a look at the code book, we can see what they stand for:\n\n1 = yes (meaning that the respondent voted)\n2 = no (meaning that the respondent did not vote)\n3 = not eligible to vote\n7 = Refusal\n8 = Don’t know\n9 = No answer\n\nThe meaning behind the values of 7, 8, and 9 are quite common and you will find them in almost all data sets which were made out of surveys. Respondents might not want to give an answer, the answer was not able to be read, or they indicated that they did not remember.\nSpoiler alert: We will work on voter turnout this session: Therefore, we will try to see what makes people vote and what decreases the likelihood that they vote at election day. The question of our dependent variable will thus be: Has the respondent voted or not?. Mathematically, this question cannot be answered with a linear regression which uses the OLS method as the dependent variable is binary meaning that 0 = has not voted/1 = has voted. There is only two possible outcomes and the variable is not continuous (one of the assumptions of an OLS).\nBut if we were to use the variable on voting turnout as it is right now, we would neither have a binary variable nor have a reliable variable as it contains values in which we are both not interested in and that will skew our estimations strongly. In fact, we cannot do a logistic regression (logit) on variables other than binary.\nThus, we first need to transform our dependent variable. We need to get rid of unwanted values and transform the 1s and 2s in 0s and 1s.\nFirst we will get rid of the unwanted values in which we are not interested.\n\n# dplyr\ness &lt;- ess |&gt;\n  filter(!vote %in% c(3, 7, 8))\n\nHere are two other ways to do it:\n\n# this one would be in base R\ness[!vote %in% c(3, 7, 8, 9)]\n\n# using the subset() function, this returns a logical vector which elements of\n# vote are not in the set of values 3, 7, 8, or 9\ness &lt;- subset(ess, vote %in% c(3,7,8,9) == F) \n\n# Alternatively you can use the %in% function with ! operator as well like this:\ness &lt;- subset(ess, !vote %in% c(3,7,8,9))\n\nQuick check to see if we got rid of all the values:\n\ntable(ess$vote)\n\n\n    1     2     9 \n22504  6542    45 \n\n\nPerfect, now we just need to transform the ones and twos into zeros and ones. This is both out of convention and also to torture you with some more data management. Since we are interested in people who do not vote, we will code those people as 0 and those who did vote as 1.\n\ness &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0))\n\nThe mutate() function is not perfectly intuitive at first sight. Here, I use the ifelse() function within mutate() to check if vote is equal to 1, if it is then it will remain 1 and if not it will be replaced by 0.\nIn Base R, you could do it like this but I believe that the mutate() function is probably the most elegant way of doing things… It’s up to you though:\n\n# only use the ifelse() function\ness$vote &lt;- ifelse(ess$vote == 1, 1, 0)\n\n# This will leave the value of 1 at 1 and change 2 to 0 for the column vote.\ness$vote[ess$vote == 1] &lt;- 1\ness$vote[ess$vote == 2] &lt;- 0\n\nWe are this close to having finished our data management part and to being able to finally work on our model. But we still have many many variables which are as “untidy” as our initial dependent variable was. Lastly, and I promise that this is the last data wrangling part for today and that we will get to our model in a moment, we need to check in the code book if specific values that we cannot simply replace over the whole data frame are still void of interest.\nOur independent variables of interest:\n\npolitical interest (polintr): c(7:9) needs to be dropped\ntrust in politicians (trstplt): no recoding necessary (unwanted values already NAs)\ntrust in political parties (trstprt): already done\nfeeling close to a party (clsprty): transform 1/2 into 0/1, drop c(7:8)\n\ngender (gndr): transform into 0/1 and drop the no answers\nyear born (yrbrn): already done\nyears of full-time education completed (eduyrs): already done\n\nWe will do every single step at once now using the pipes %&gt;% (tidyverse) or |&gt; (Base R) to have a tidy and elegant way of doing everything at once.\nBelow you can see that I first filter() out the unwanted values of our dependent variable. Then open up a muatate() in which I manipulate the independent variables which we are going to need for our model. Note how you can separate the different manipulations with a comma within the same function. This renders a very clean and easy to read code. There are several different ways you could do this. The recode(), if_else() and case_when() functions are all very useful for this kind of data wrangling. For clarity, I will use case_when() here. There is a newer version of this function called case_match() but it is only a couple of months old and for now the documentation of case_when() will probably be more helpful.\ncase_when() evaluates a series of conditions and applies the corresponding transformation to each element of a vector or column in a dataset. It’s like a more flexible and vectorized version of the if…else statement, allowing for multiple if…else conditions to be checked in a single function call. This is how it reads:\n\ncase_when(\n  condition1 ~ value_if_true1,\n  condition2 ~ value_if_true2,\n  TRUE ~ default_value\n)\n\nWith case_when(), you list your rules one by one. For each rule, you start with a condition (like “score is 80 or above”), followed by a ~, and then what you want to happen if that condition is true (like “they get an ‘A’”). After listing all your specific rules, you end with a special catch-all rule: TRUE ~ x. This is like saying, “For anyone else, or any situation I didn’t mention, do this.” This TRUE ~ x part is crucial. It ensures that if none of the specific conditions match (maybe because of some unexpected situation), you have a default action ready. It’s your “in case of anything else” rule.\nNow, let’s talk about NA_integer_. In R, NA means “not available” or missing information. But R is very particular about the kind of data it deals with. If your variable is numeric, R wants to know that even your missing pieces should be numbers. Using NA_integer_is your way of telling R, “This is a missing piece of information, but if it were here, it would be a number.” It helps keep everything organized and avoids confusion when R is looking through your data. There are also other instances (when recoding character strings for example), where you would use NA_character_ instead. Pay attention that if you have numeric values with decimal points, you will have to use NA_real_ instead of NA_integer_.\n\ness_final &lt;- ess |&gt; \n  # Filtering the dependent variable to get rid of any unnecessary rows\n  filter(!vote %in% c(3, 7, 8, 9)) |&gt; \nmutate(\n    # Recode 'polintr' to drop values 7 to 9\n    polintr = case_when(\n      polintr %in% 7:9 ~ NA_integer_,\n      TRUE ~ polintr\n    ),\n    # Recode 'clsprty' 1/2 into 0/1 and drop 7, 8\n    clsprty = case_when(\n      clsprty == 1 ~ 0,\n      clsprty == 2 ~ 1,\n      clsprty %in% c(7, 8) ~ NA_integer_,\n      TRUE ~ clsprty\n    ),\n    # Recode 'gndr' into 0/1 and handle other cases as NAs\n    gndr = case_when(\n      gndr == 1 ~ 0,\n      gndr == 2 ~ 1,\n      TRUE ~ NA_integer_\n    ),\n    # Recode 'vote' as binary 1 (voted) & 0 (abstention)\n    vote = case_when(\n      vote == 1 ~ 1,\n      TRUE ~ 0\n    )\n  )\n\n\n2.3.0.2 Optional quick/fancy data cleaning\nEdit: I have added a more sophisticated way of cleaning all your variables in one go. I have realized during the first session of session 2 that this might be too overwhelming but I would still like to keep this way of doing things to show you how you can do it in the future. If you wish to stick to the slightly longer but maybe also clearer way above, that is absolutely fine with me! Chose the way that you understand and feel comfortable with. As always, there are a lot of different paths to the same goal in R!\n\n# specify a string of numbers we are absolutely certain we won't need\nunwanted_numbers &lt;- c(66, 77, 88, 99, 7777, 8888, 9999)\n\n# make sure to create a new object/data frame; if you don't and re-run your code\n# a second time, it will transform some mutated values again!\ness_final &lt;- ess |&gt; \n  # filtering the dependent variable to get rid of any unnecessary rows\n  filter(!vote %in% c(3, 7, 8, 9)) |&gt; \n  # mutate allows us to transform values within variables into other \n  # values or NAs\n  # vote as binary 1 (voted) & 0 (abstention)\n  mutate(across(c(polintr, clsprty), ~replace(., . %in% c(7:9), NA)),\n         across(everything(), ~replace(., . %in% unwanted_numbers, NA)),\n         vote = ifelse(vote == 1, 1, 0),\n         # recode the variable to 0 and 1\n         clsprty = recode(clsprty, `1` = 1, `2` = 0),\n         # same for gender\n         gndr = recode(gndr, `1` = 0, `2` = 1))\n\n\n2.3.1 Constructing the logit-model\nIf you have made it this far and still bear with me, you have made it to the fun part! Specifying the model and running it, literally only takes one line of code (or two depending on the amount of independent variables). And as you can see, it is really straightforward. The glm() function stands for generalized linear model and comes with Base R.\nIn Professor Rovny’s lecture, we have seen that for a Maximum Likelihood Estimation (MLE) you need to know or have an assumption about the distribution of your dependent variable. And according to this distribution, you need to find the right linear model. If you have a binary outcome, your distribution is binomial. Within the function, we thus specify the family of the distribution as such. Note that you could also specify other families such as Gaussian, poisson, gamma or many more. We are not going to touch further on that but the glm() function is quite powerful. We can specify, within the family = argument, that we are doing a logistic regression. This can be done by adding link = logit to the argument. If ever you wanted to be precise and call a probit or cauchy link, it is here that you can specify this. The standard, however, is set to logit, so we would technically not be forced to specify it in this case.\nIn terms of the model we are building right now, it follows the idea that voting behavior (voted/not-voted) is a function of political interest, trust in politicians, trust in parties, feeling close to a specific party, as well as usual control variables such as gender, age, and education:\nBy no means is this regression just extensive enough to be published. It is just one example in which I suspect that political interest, trust in politics and politicians, and party affiliation are explanatory factors.\n\nlogit &lt;- glm(\n  vote ~ polintr + trstplt + trstprt + clsprty + gndr +\n    yrbrn + eduyrs,\n  data = ess_final,\n  family = binomial(link = logit)\n)\n\nThe object called logit contains our model with its coefficients, confidence intervals and many more things that we will play with! But as you can see, the actual construction of the model is more than simple…\n\nlibrary(broom)\ntidy(logit)\n\n# A tibble: 8 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  3.71      0.0835       44.4   0        \n2 polintr     -0.694     0.0193      -35.9   3.37e-282\n3 trstplt      0.00546   0.00253       2.16  3.08e-  2\n4 trstprt     -0.00521   0.00220      -2.37  1.78e-  2\n5 clsprty     -0.906     0.0347      -26.1   3.80e-150\n6 gndr         0.119     0.0307        3.88  1.07e-  4\n7 yrbrn       -0.0000156 0.0000262    -0.595 5.52e-  1\n8 eduyrs       0.00786   0.00162       4.85  1.27e-  6\n\n\nYou have seen both the broom package as well as stargazer in the last session.\n\nstargazer::stargazer(\n  logit,\n  type = \"text\",\n  dep.var.labels = \"Voting Behavior\",\n  dep.var.caption = c(\"Voting turnout; 0 = abstention | 1 = voted\"),\n  covariate.labels = c(\n    \"Political Interest\",\n    \"Trust in Politicians\",\n    \"Trust in Parties\",\n    \"Feeling Close to a Party\",\n    \"Gender\",\n    \"Year of Birth\",\n    \"Education\"\n  )\n)\n\n\n===================================================================\n                         Voting turnout; 0 = abstention | 1 = voted\n                         ------------------------------------------\n                                      Voting Behavior              \n-------------------------------------------------------------------\nPolitical Interest                       -0.694***                 \n                                          (0.019)                  \n                                                                   \nTrust in Politicians                      0.005**                  \n                                          (0.003)                  \n                                                                   \nTrust in Parties                          -0.005**                 \n                                          (0.002)                  \n                                                                   \nFeeling Close to a Party                 -0.906***                 \n                                          (0.035)                  \n                                                                   \nGender                                    0.119***                 \n                                          (0.031)                  \n                                                                   \nYear of Birth                             -0.00002                 \n                                         (0.00003)                 \n                                                                   \nEducation                                 0.008***                 \n                                          (0.002)                  \n                                                                   \nConstant                                  3.709***                 \n                                          (0.084)                  \n                                                                   \n-------------------------------------------------------------------\nObservations                               28,347                  \nLog Likelihood                          -13,535.140                \nAkaike Inf. Crit.                        27,086.280                \n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n2.3.2 Interpretation of a logistic regression\nInterpreting the results of a logistic regression can be a bit tricky because the predictions are in the form of probabilities, rather than actual outcomes. This sounds quite abstract and you are right, it is abstract. However, with a proper understanding of the coefficients and odds ratios, you can gain insights into the relationship between your independent variables and the binary outcome variable even without transforming your coefficients into more easily intelligible values.\nFirst the really boring and technical definition: The coefficients of a logistic regression model represent the change in the log-odds of the outcome for a one-unit change in the predictor variable (holding all other predictors constant). The sign of the coefficient indicates the direction of the association: positive coefficients indicate that as the predictor variable increases, the odds of the outcome also increase, while negative coefficients indicate that as the predictor variable increases, the odds of the outcome decrease.\nThe odds ratio, which can be calculated from the coefficients and we will see how that works in a second (exponentation is the key word), represents the ratio of the odds of the outcome for a particular value of the predictor variable compared to the odds of the outcome for a reference value of the predictor variable. An odds ratio greater than 1 indicates that the predictor variable is positively associated with the outcome, while an odds ratio less than 1 indicates that the predictor variable is negatively associated with the outcome.\nIt’s also important to keep in mind that a logistic regression model makes assumptions about the linearity, independence and homoscedasticity of the data, if these assumptions are not met it can affect the model’s performance and interpretation. We will see the diagnostics of logistic regression models again next session.\nIs this really dense and did I lose you? It is dense but I hope you bear with me because we will see that it becomes much clearer once we apply this theory to our model but also once we exponentiate the coefficients (reversing the logarithm so to speak) and interpret them as odds-ratios.\nBut from a first glimpse at our model summary we can see that political interest, trust in politicians, closeness to a party, age and education are all statistically significant, meaning that their p-value is &lt;.05! I will not regard any other variable that is not statistically significant as you do not usually interpret non-significant variables.\nNext, we can already say that the association of interest, closeness to party and age with voting behavior is negative. This is quite logical and makes sense in our case. If we look at the scales on which these variables are coded (code book!), we can see that the higher the value of the variable, the less interested, close or aged the respondents were. Thus, it decreases their likelihood to vote on voting day. Trust in politicians is coded the other way around. If I had been a little more thorough, it would have been good to put each independent variable on the same scale… But it means that trust in politicians (in fact meaning that they trust them less) raises the likelihood of not voting somehow (positive association).\n\n2.3.2.1 Odds-ratio\nIf you exponentiate the coefficients of your model, you can interpret them as odds-ratios. Odds ratios (ORs) are often used in logistic regression to describe the relationship between a predictor variable and the outcome. ORs are easier to interpret than the coefficients of a logistic regression because they provide a measure of the change in the odds of the outcome for a unit change in the predictor variable.\nAn OR greater than 1 indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome, and an OR less than 1 indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome.\nThe OR can also be used to compare the odds of the outcome for different levels of the predictor variable. For example, an OR of 2 for a predictor variable means that the odds of the outcome are twice as high for one level of the predictor variable compared to another level. Therefore, odds ratios are often preferred to coefficients for interpreting the results of a logistic regression, especially in applied settings.\nI will try to rephrase this and make it more accessible so that odds-ratios maybe become more intelligible (they are really nasty statistical stuff):\nImagine you’re playing a game where you have to guess whether a coin will land on heads or tails. If the odds of the coin landing on heads is the same as the odds of it landing on tails, then the odds-ratio would be 1. This means that the chances of getting heads or tails are the same. But if the odds of getting heads is higher than the odds of getting tails, then the odds-ratio would be greater than 1. This means that the chances of getting heads is higher than the chances of getting tails. On the other hand, if the odds of getting tails is higher than the odds of getting heads, then the odds-ratio would be less than 1. This means that the chances of getting tails is higher than the chances of getting heads. In logistic regression, odds-ratio is used to understand the relationship between a predictor variable (let’s say “X”) and an outcome variable (let’s say “Y”). Odds ratio tells you how much the odds of Y happening change when X changes.\nSo, for example, if the odds ratio of X is 2, that means that if X happens, the odds of Y happening are twice as high as when X doesn’t happen. And if the odds ratio of X is 0.5, that means that if X happens, the odds of Y happening are half as high as when X doesn’t happen.\n\n# simply use exp() on the coefficients of the logit\nexp(coef(logit))\n\n(Intercept)     polintr     trstplt     trstprt     clsprty        gndr \n 40.8120705   0.4997134   1.0054777   0.9948009   0.4040527   1.1261108 \n      yrbrn      eduyrs \n  0.9999844   1.0078943 \n\n# here would be a second way of doing it\nexp(logit$coefficients)\n\n(Intercept)     polintr     trstplt     trstprt     clsprty        gndr \n 40.8120705   0.4997134   1.0054777   0.9948009   0.4040527   1.1261108 \n      yrbrn      eduyrs \n  0.9999844   1.0078943 \n\n\nWe can also, and should, add the 95% confidence intervals (CI). As a quick reminder, the CI is a range of values that is likely to contain the true value of a parameter (the coefficients of our predictor variables in our case). This comes at a certain level of confidence. The most commonly used levels (attention, this is only a statistical convention!) of confidence are 95% and sometimes 99%.\nA 95% CI for a parameter, for example, means that if the logistic regression model were fitted to many different samples of data, the true value of the parameter would fall within the calculated CI for 95% of those samples.\n\n# most of the times the extra step in the next lines is not necessary and this \n# line of code is enough\nexp(cbind(OR = coef(logit), confint(logit)))\n\nWaiting for profiling to be done...\n\n\n                    OR      2.5 %     97.5 %\n(Intercept) 40.8120705 34.6333002 48.0596000\npolintr      0.4997134  0.4810885  0.5189496\ntrstplt      1.0054777  1.0005481  1.0105282\ntrstprt      0.9948009  0.9905477  0.9991390\nclsprty      0.4040527  0.3773609  0.4323782\ngndr         1.1261108  1.0604502  1.1958345\nyrbrn        0.9999844  0.9999340  1.0000369\neduyrs       1.0078943  1.0047438  1.0111607\n\n# here, however, we must combine both the exponentiate coefficients with the 95% confidence intervals\n# the format() function, helps me to show the numbers without the exponentiated \n# \"e\" and without scientific notation; the round() function within this function gives me values which are rounded on the 5th decimal place.\nformat(round(exp(cbind(\n  OR = coef(logit), confint(logit)\n)), 5),\nscientific = FALSE, digits = 4)\n\nWaiting for profiling to be done...\n\n\n            OR        2.5 %     97.5 %   \n(Intercept) \"40.8121\" \"34.6333\" \"48.0596\"\npolintr     \" 0.4997\" \" 0.4811\" \" 0.5190\"\ntrstplt     \" 1.0055\" \" 1.0006\" \" 1.0105\"\ntrstprt     \" 0.9948\" \" 0.9906\" \" 0.9991\"\nclsprty     \" 0.4041\" \" 0.3774\" \" 0.4324\"\ngndr        \" 1.1261\" \" 1.0604\" \" 1.1958\"\nyrbrn       \" 1.0000\" \" 0.9999\" \" 1.0000\"\neduyrs      \" 1.0079\" \" 1.0047\" \" 1.0112\"\n\n\nThis exponentiated value, the odds ratio (OR), now allows us to say that for a one unit increase in political interest, for example, the odds of voting (versus not voting) decrease. The same goes for the other variables.\nThe last thing about odds-ratio and I hope that this is the easiest to interpret, is when you try to make percentages out of it:\n\n# the [-1] drops the value of the intercept as it is statistically meaningless\n# we put another minus one to get rid of 1 as a threshold for interpreting the\n# odds-ratio\n# we multiply by 100 to have percentages\n100*(exp(logit$coefficients[-1])-1)\n\n      polintr       trstplt       trstprt       clsprty          gndr \n-50.028658955   0.547769108  -0.519908142 -59.594726196  12.611084187 \n        yrbrn        eduyrs \n -0.001560893   0.789429192 \n\n\nThis allows us to say that being politically uninterested decreases the odds of voting by 35%. Much more straightforward right?\n\n2.3.2.2 Predicted Probabilities\nPredicted probabilities also allow us to understand our logistic regression. In logistic regressions, the predicted probabilities and ORs are two different ways of describing the relationship between the predictor variables and the outcome. Predicted probabilities refer to the probability that a specific outcome will occur, given a set of predictor variables. They are calculated using the logistic function, which maps the linear combination of predictor variables (also known as the log-odds) to a value between 0 and 1.\nThe importance here is that we chose the predictor variables and at which values of those we are trying to predict the outcome. This is what we call “holding independent variables constant” while we calculate the predicted probability for a specific independent variable of interest.\nI will repeat this to make sure that everybody can follow along. With the predicted probabilities, we are trying to make out the effect of one specific variable of interest on our dependent variable, while we hold every other variable at their mean, median in some cases or, in the case of a dummy variable, at one of the two possible values. By holding them constant, we can be sure to see the singular effect of our independent variable of interest.\nIn our case, let “feeling close to a party” (1 = yes; 0 = no) be our independent variable of interest. We take our old ess_final dataframe and create a new one. In the newdata dataframe, we hold all values at their respective means or put our binary/dummy variables to 1. It is an arbitrary choice to put it to one here. We could also put it to 0. The only variable that we allow to alternate freely to find the predicted probabilities is our variable of interest clsprty.\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = mean(polintr),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = c(0, 1),\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n    ))\n\nIf that all worked out, we can predict the values for this specific independent variable by using the Base R predict() function:\n\nnewdata$preds &lt;- predict(logit, newdata = newdata, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata, aes(x = clsprty, y = preds)) +\n  geom_line() +\n  ylab(\"Likelihood of Voting\") + xlab(\"Feeling Close to a Party\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nWe can also do the same thing to see the predicted probability of political interest on voting behavior. This is a bit more interesting as the variable is not binary like ess$clsprty:\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata_1 &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = c(1:4),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = 1,\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n  )\n)\n\n\nnewdata_1$preds &lt;- predict(logit, newdata = newdata_1, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata_1, aes(x = polintr, y = preds)) +\n  geom_line() +\n  ylab(\"predicted probability\") + xlab(\"political interest\")\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n#combines value data frame created above with predicted probabilities evaluated \n# at the data values\nnewdata_1 &lt;-\n  cbind(newdata_1,\n        predict(\n          logit,\n          newdata = newdata_1,\n          type = \"link\",\n          se = TRUE\n        )) \n\n\nnewdata_1 &lt;- within(newdata_1, {\n  pp &lt;- plogis(fit)                   # predicted probability\n  lb &lt;- plogis(fit - (1.96 * se.fit)) # builds lower bound of CI\n  ub &lt;- plogis(fit + (1.96 * se.fit)) # builds upper bound of CI\n})\n\n\nggplot(newdata_1, aes(x = polintr, y = pp)) +\n  geom_line(aes(x = polintr, y = pp, color = as.factor(gndr))) +\n  geom_ribbon(aes(ymin = lb, ymax = ub), alpha = 0.3) +\n  theme(legend.position = \"none\") +\n  ylab(\"predicted probability to abstain from voting\") +\n  xlab(\"political interest\")\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\n\n2.3.3 Making life easiest\nYou are going to hate me if I tell you that all these steps which we just computed by hand… can be done by using a package. This is only 0.01% of me trying to be mean but mostly because it is extremely helpful and necessary to understand what is going on under the hood of predicted probabilities. The interpretation of logistic regressions is tricky and if you do not know what you are computing, it is even more complicated.\nWorking with packages is great, and I am aware that I always encourage you to use packages that make your life easier. But and this is an important “but” we do not always understand what is going on under the hood of a package. It is like putting your logistic regression into a black box, shaking it really well, and then taking a look at the output and putting it on shaky interpretational terms.\nBut enough of personal defense, as to why I made you suffer through all this. Here is my code to do most of the steps at once:\n\n# this package contains everything we need craft predicted probabilities and\n# visualize them as well\nlibrary(ggeffects)\n\n# like the predcit() function of Base R, we use ggpredict() and specify\n# our variable of interest\ndf &lt;- ggpredict(logit, terms = \"polintr\")\n\n# this is the simplest way of plotting this\nggplot(df, aes(x = x, y = predicted)) +\n  # our graph is more or less a line, so geom_line() applies\n  geom_line() +\n  # geom_ribbon() with the values that ggpredict() provided for the confidence\n  # intervals then gives\n  # us a shade around the geom_()line as CIs\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1)\n\n\n\n\n\n\n\nAnd voilà, your output it less than 10 lines of code.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#automating-things-in-r-optional",
    "href": "session2/session2.html#automating-things-in-r-optional",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.4 Automating things in R (OPTIONAL!)",
    "text": "2.4 Automating things in R (OPTIONAL!)\nWhen programming, it usually takes time to understand and apply things. The next step should often be to think about how to automate something in order to make processes faster and more elegant. Once we have understood a process relatively well, we can apply it to other areas in an automated way relatively easily.\nFor example, let’s think about our logistical regressions today. We worked with the ESS and looked at the 10th round of the data set for France. Our model aimed to investigate which variables can predict abstention. But I can also ask myself this question over a certain period of time, i.e. over several waves of the ESS, or for several countries. If I now tell you that you should do the logit for all countries of the ESS, the first step would be to run my code for n = countries of the ESS. However, this would result in a very long script, would not be very elegant and the longer the code, the higher the probability of errors.\nIf you are programming and realize that you have to do the same steps several times and it is actually the same step, only that a few parameters (such as the names of variables) change, then you can be sure that this could also be automated. At a certain point in programming, the goal should always be to create a pipeline for the work steps, which has the goal of making our code run as automatically as possible.\nIn RStudio we have several ways to automate this. For example, you can write so-called for-loops, which perform certain operations one after the other for certain list entries. Or you can write your own functions. At some point, someone decided to write the function glimpse() or read_csv2(), for example. As private users of R, we can do the same. In this way, we can, for example, accommodate several operations within a function that we write ourselves, which can then be applied simultaneously to several objects or a data set with different countries.\nI am aware that this is only the second session and that may sound like a lot. Everything from here on is optional, but I think it’s important that you see this as early as possible. Some of you may already feel comfortable enough to try something like this out for yourselves. If you don’t, that’s okay too. You will get there, trust me! I just want to show you what is possible and what you can do with R.\n\n\n\n\n\n2.4.1 Writing your function\nFunctions in R are incredibly powerful and essential for efficient and automated programming. A function in R is defined using the function keyword. The basic structure includes the name of the function, a set of parameters, and the body where the actual computations are performed.\nThe basic syntax is as follows:\nThere are some minor conventions in RStudio when writing functions. Some of them also apply to other parts than just functions.\n\nYou should give them some “breathing” space. When you write the accolades, put a space bar in between.\n\n\n# this is bad\nfunction(x){\n  x + 1\n}\n\n\n# this is good\nfunction(x) {\n  x + 1\n}\n\n\nYou should always put a space between the equal sign and the value you assign to a variable. Place spaces around all infix operators (=, +, -, &lt;-, etc.)\n\n\n# this is bad\ndata&lt;-read.csv(\"data.csv\")\n\n\ndata &lt;- read.csv(\"data.csv\")\n\n\n“stretch” your code when possible ctrl + shift + a can be of help for that\n\n\n# this is bad but in a longer example\ncountry_model &lt;- function(df) {glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, family = binomial(link = \"logit\"), data = df)\n}\n\n\n# this is better\ncountry_model &lt;- function(df) {\n  glm(\n    vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs,\n    family = binomial(link = \"logit\"),\n    data = df\n  )\n}\n\n\nWe usually assign verbs to functions. This means that the name of the function should be a verb that describes what the function does. If we want to create a function that reads the ESS files and does several operations at once, we should call it something like read_ess().\n\n2.4.2 The purrr package\nThe purrr package in R is a powerful tool that helps in handling repetitive tasks more efficiently. It’s part of the Tidyverse, a collection of R packages designed to make data science faster, easier, and more fun!\nIn simple terms, purrr improves the way you work with lists and vectors in R. It provides functions that allow you to perform operations, i.e. pre-existing functions or functions you will write yourselves, on each element of a list or vector without writing explicit loops. This concept is known as functional programming.\n\n\n\n\n\n\nWhy use purrr instead of for-loops?\n\n\n\n\n\n\nSimplifies Code: purrr makes your code cleaner and more readable. Instead of writing several lines of loop code, you can achieve the same with a single line using purrr.\nConsistency and Safety: purrr functions are consistent in their behavior, which reduces the chances of errors that are common in for loops, like mistakenly altering variables outside the loop.\nHandles Complexity Well: When working with complex or nested lists, purrr offers tools that make these tasks much simpler compared to traditional loops.\nIntegration with tidyverse: Since purrr is part of the tidyverse, it integrates smoothly with other Tidyverse packages, making your entire data analysis workflow more efficient.\n\n\n\n\nPut simply, we can use the functions of the purrr package to apply a function to each element of a list or vector. Depending on the specific purrr-function, our output can be different but we can also specify the output in our manually written function which we feed into purrr.\n\n2.4.3 The map() function\nThe map() function, part of the purrr package in R, is built around a simple yet powerful concept: applying a function to each element of a list or vector and returning a new list with the results. This concept is known as “mapping,” hence the name map().\n\n\nThe logic is simple. You take map(a list of your choice + your function) which then creates an output that behaves as if you had applied that function to each list entry individually.\n\nHere’s a breakdown of the logic behind map():\n\nInput: The primary input to map() is a list or a vector. This could be a list of numbers, characters, other vectors, or even more complex objects like data frames.\nFunction Application: You specify a function that you want to apply to each element of the list. This function could be a predefined function in R, or a custom function you’ve written. The key is that this function will be applied individually to each element of your list/vector.\nIteration: map() internally iterates over each element of your input list/vector. You don’t need to write a loop for this; map() handles it for you. For each element, map() calls the function you specified.\nOutput: For each element of the list/vector, the function’s result is stored. map() then returns a new list where each element is the result of applying the function to the corresponding element of the input.\nFlexibility in Output Type: The purrr package provides variations of map() to handle different types of output. For example, map_dbl() if your function returns doubles, map_chr() for character output, and so on. This helps in ensuring that the output is in the format you expect.\n\n2.4.4 Automatic regressions for several countries\nThis is absolutely only optional. I do not ask you to reproduce anything of this at any point in this class. I simply wanted to show you what you can do in R and what I mean when I say that automating stuff makes life easier.\nI present you here with a code that does the logistic regression we have been doing but on all countries of the ESS at the same time and then plots us the odds-ratio of our variable of interest “political interest”, as well as comparing McFadden pseudo R2 (we’ll see this term next session again).\nI will combine things from the optional section of Session 1 and the purrr package to do this. The idea is to build one model per country of the ESS, nest() it in a new tibble (What are tibbles again?) 1 where each row contains the information necessary for one country-model and to then use map() to apply the function tidy() from the broom package to each of these models.\nBelow, you can find a simple function that takes a dataframe as input and returns a logistic regression model. Within, I only specify the glm() function which I have shown you above. Within the parentheses of function() I specify the name of the input object. This name is arbitrary and can be anything you want. I chose df for dataframe. It has to appear somewhere within your function later on; usually there where your operation on the input object is supposed to be performed. In my case this is data = df.\n\ncountry_model &lt;- function(df) {\n  glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, \n      family = binomial(link = \"logit\"), data = df)\n}\n\n\nI will first talk you through the different steps and then provide you one long pipeline that does all this in one step.\n\nFirst, I import the ESS data and select the variables I want to use. I also clean the data a bit and get rid of unwanted observations or values.\n\n# Recoding the 'vote' variable to binary (1 or 0)\n# and filtering the dataset based on specified criteria\nprepared_ess &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt; \n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  )\n\nThen we will nest() the data as described here where I explain the logic of nesting.\n\n# library for McFadden pseudo R2\nlibrary(pscl)\nlibrary(broom)\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  select(cntry,\n         vote,\n         polintr,\n         trstplt,\n         trstprt,\n         clsprty,\n         gndr,\n         yrbrn,\n         eduyrs)\n\ness_model &lt;- ess |&gt; \n  as_tibble() |&gt;  # Convert the data frame to a tibble\n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt;\n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  ) |&gt;\n  group_by(cntry) |&gt;\n  nest() |&gt;\n  mutate(\n    model = map(data, country_model),\n    tidied = map(model, ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE)),\n    glanced = map(model, glance),\n    augmented = map(model, augment),\n    mcfadden = map(model, ~ pR2(.x)[4])\n  )\n\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\n\n\n\n# Comparing AICs\npR2(logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1.353514e+04 -1.520203e+04  3.333773e+03  1.096490e-01  1.109536e-01 \n         r2CU \n 1.686556e-01 \n\ness_model |&gt;\n  unnest(mcfadden) |&gt;\n  ggplot(aes(fct_reorder(cntry, mcfadden), mcfadden)) +\n  geom_col() + coord_flip() +\n  scale_x_discrete(\"Country\")\n\n\n\n\n\n\n\n\n# Comparing coefficients\ness_model |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"polintr\") |&gt;\n  ggplot(aes(\n    reorder(cntry, estimate),\n    y = exp(estimate),\n    color = cntry,\n    ymin = exp(conf.low),\n    ymax = exp(conf.high)\n  )) +\n  geom_errorbar() +\n  geom_point() +\n  scale_x_discrete(\"Country\") +\n  ylab(\"Odds-Ratio of political interest\") +\n  xlab(\"Country\")",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#footnotes",
    "href": "session2/session2.html#footnotes",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "The quick answer is that tibbles are a modern take on data frames in R, offering improved printing (showing only the first 10 rows and fitting columns to the screen), consistent subsetting behavior (always returning tibbles), tolerance for various column types, support for non-standard column names, and no reliance on row names. They represent a more adaptable, user-friendly approach for handling data in R, especially suited for large, complex datasets.↩︎",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session3/session3.html",
    "href": "session3/session3.html",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "3.1 Introduction\nIn this script, I will show you how to construct a multinomial logistic regression in R. For this, we will work on the European Social Survey (ESS) again. These are the main points that are covered in this script:",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#introduction",
    "href": "session3/session3.html#introduction",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "The logic of multinomial (logistic) regressions\nAdvanced Data Management\nInterpretation of a multinomial Model\nModel Diagnostics\nGoodness of Fit\nAPIs and Data Visualization (OPTIONAL!)",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#the-logic-of-multinomial-logistic-regressions",
    "href": "session3/session3.html#the-logic-of-multinomial-logistic-regressions",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.2 The Logic of multinomial (logistic) Regressions",
    "text": "3.2 The Logic of multinomial (logistic) Regressions\nI have chosen four countries out of which you will be able to choose one later one when I ask you to work on some exercises. For now, I will mainly work on Germany. One of the classic applications of multinomial models in political science is the question of voting behavior, more precisely vote choice. Last week, we have seen models of a logistic regression (logit). It is used in cases when our dependent variable (DV) is binary (0 or 1; true or false; yes or no) which means that we are not allowed to use OLS. The idea of logit can be extended to unordered categorical or nominal variables with more than two categories, e.g.: Vote choice, Religion, Brands…\nInstead of one equation modelling the log-odds of \\(P(X=1)\\), we do the same thing but for the amount of categories that we have. In fact, this means that a multinomial model runs several single logistic regressions on something we call a baseline. R will choose this baseline to which the categorical values of our DV will then relate. But we can also change it (this is called releveling). This allows us to make very interesting inferences with categorical (or ordinal) variables. If this sounds confusing, you should trust me when I tell you that this will become more straightforward in a second!\nHowever, this also makes the interpretation of these models a bit intricate and opaque at times. Nevertheless, you will see that once you have understood the basic idea of a multinomial regression and how to interpret the values in accordance to the baseline, it is not much different from logistic regressions on binary variables (and in my eyes even a bit simpler…). If the logic of logit is not 100% clear at this point, I recommend you go back to last session’s script on logit and work through my explanations. And if that does not help, try to follow this lecture attentively. As I said, the logic is the same, so I will repeat myself :) And if it is still unclear, you can always ask in class or come see me after the session!\nBut enough small talk, let’s first do some data wrangling which you all probably dread at this point…",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#data-management-for-multinomial-regression",
    "href": "session3/session3.html#data-management-for-multinomial-regression",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.3 Data Management for Multinomial Regression",
    "text": "3.3 Data Management for Multinomial Regression\nAs I have said, we will work on voting choice in four different countries. I selected Denmark and Germany. Germany I have chosen because I was working on this model a couple of months ago and Denmark is for fun.\nThe data which we will use for this session is the 9th round of the ESS published in 2018. The goal of this session is to understand predictors that tell us more about why people vote for Populist Radical-Right Parties, henceforth called PRRP (Mudde 2007). For this I have two main hypotheses in mind, as well as some predictors which I know are important based on the literature. Finally we also need some control variables which we need to control for in almost any regression analysis using survey data.\nMy two hypotheses (H1) and H2) are as follows:\n\nH1: Thinking that immigrants enrich a country’s culture decreases the likelihood of voting for PRRPs.\nH2: Having less trust in politicians increases the likelihood of voting for PRRPs than voting for other parties.\n\nNow you might notice two things. First, my hypotheses are relatively self-explanatory and you are absolutely right. They are more than that, they are perhaps even self-evident. But to this, I would just reply that this is supposed to be an easy exercise which is supposed to expose you to a multinomial regression and the logic of it. Second, you might see that my hypotheses are relatively broadly formulated. This is because I would like you, later in class, to choose one of the countries of the 9th wave of the ESS and build a model yourselves. By giving you broad hypotheses, you can do this ;)\n\n# read_csv from the tidyverse package\ness &lt;- read_csv(\"ESS9e03_1.csv\") |&gt; \n  # dplyr allows me to select only those variables I want to use later\n  select(cntry, \n         prtvtdfr, \n         prtvede1, \n         prtvtddk, \n         prtvtdpl, \n         imueclt, \n         yrbrn, \n         eduyrs, \n         hinctnta, \n         stflife, \n         trstplt, \n         blgetmg, \n         gndr) |&gt; \n  # based on the selected variables, I filter the dataframe so that I am only\n  # left with the data for Germany and Denmark\n  filter(cntry %in% c(\"DE\", \"DK\"))\n\nRows: 49519 Columns: 572\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): name, proddate, cntry, ctzshipd, cntbrthd, lnghom1, lnghom2, fbrn...\ndbl (562): essround, edition, idno, dweight, pspwght, pweight, anweight, pro...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAgain, every transformation and mutation of variables which you see below is done based on my knowledge of the dataset which I solely gained from looking at the code book. The code book can be found on the Moodle page (or the ESS’ website). It is highly important that you get used to reading a code book in general but especially to familiarize yourselves with the data which you will use by looking at the way that the variables are coded in the code book. There, for example, you will find information on the numeric values which are stored in the variables prtvtdfr, prtvede1, prtvtddk and prtvtdpl. They all stand for a category or, in our case, a party name which you can only identify if you open the code book. You will see that I only selected some parties in the mutate() function below. This is more or less to get rid of those parties that did not make it into the national parliament at the last national election of each country.\nYou have seen a similar chunk of code in the last script. See how, once you have a code that works for one dataset, you can use it again?\n\n# cleaning the dependent variables all over the dataframe\ness_clean &lt;- ess |&gt;\n    mutate(across(where(is.numeric), ~case_when(\n           . %in% c(66, 77, 88, 99, 7777, 8888, 9999) ~ NA_integer_,\n           TRUE ~ .)),\n      prtvtdfr = replace(prtvtdfr, prtvtdfr %in% c(1, 2, 10, 12:99), NA),\n           prtvede1 = replace(prtvede1, !prtvede1 %in% c(1:6), NA),\n           prtvtddk = replace(prtvtddk, !prtvtddk %in% c(1:10), NA),\n           prtvtdpl = replace(prtvtdpl, !prtvtdpl %in% c(1:8), NA),\n           # get rid of unwanted values indicating no response etc\n           blgetmg = replace(blgetmg, !blgetmg %in% c(1:2), NA),\n           # gender recoded to 1 = 0, 2 = 1 (my personal preference)\n           gndr = recode(gndr, `1` = 0, `2` = 1))\n\nIn fact, you could already build the model now and start the multinomial regression. However, I add an additional data management step by placing the numeric values of the election variable in a new variable called vote_de, where I convert the numeric values to character values and at the same time give them the names of the parties. This will automatically transform NAs in all the rows in which the country is not that in which the person has voted.\nBut more importantly, once I run the regression, it will display the parties’ names instead of the numbers. This means that I won’t have to go back to the code book every time to check what the 1s or 2s correspond to.\n\n# this is simple base R creating a new column/variable with character\n# values corresponding to the parties' names behind the numeric values\ness_clean$vote_de[ess_clean$prtvede1==1]&lt;-\"CDU/CSU\"\ness_clean$vote_de[ess_clean$prtvede1==2]&lt;-\"SPD\"\ness_clean$vote_de[ess_clean$prtvede1==3]&lt;-\"Die Linke\"\ness_clean$vote_de[ess_clean$prtvede1==4]&lt;-\"Grüne\"\ness_clean$vote_de[ess_clean$prtvede1==5]&lt;-\"FDP\"\ness_clean$vote_de[ess_clean$prtvede1==6]&lt;-\"AFD\"\n\nHere is a way to mutate all the variables at once. However, this somehow creates conflicts with a package used further below.\n\ness_clean &lt;- ess_clean |&gt; \n  mutate(\n    vote_dk = case_when(prtvtddk == 1 ~ \"Socialdemokratiet\",\n                        prtvtddk == 2 ~ \"Det Radikale Venstre\",\n                        prtvtddk == 3 ~ \"Det Konservative Folkeparti\",\n                        prtvtddk == 4 ~ \"SF Socialistisk Folkeparti\",\n                        prtvtddk == 5 ~ \"Dansk Folkeparti\",\n                        prtvtddk == 6 ~ \"Kristendemokraterne\",\n                        prtvtddk == 7 ~ \"Venstre\",\n                        prtvtddk == 8 ~ \"Liberal Alliance\",\n                        prtvtddk == 9 ~ \"Enhedslisten\",\n                        prtvtddk == 10 ~ \"Alternativet\",\n                        TRUE ~ NA_character_),\n    vote_de = case_when(prtvede1 == 1 ~ \"CDU/CSU\",\n                        prtvede1 == 2 ~ \"SPD\",\n                        prtvede1 == 3 ~ \"Die Linke\",\n                        prtvede1 == 4 ~ \"Grüne\",\n                        prtvede1 == 5 ~ \"FDP\",\n                        prtvede1 == 6 ~ \"AFD\"))",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#constructing-the-model",
    "href": "session3/session3.html#constructing-the-model",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.4 Constructing the Model",
    "text": "3.4 Constructing the Model\nNow that the data management process is finally over, we can specify our model. For this, you need to install the nnet package and load it to your library. Once this is done, we will take the exact same steps as you would do for an OLS or logit model. You specify your DV followed by a ~ and then you only need to add all your IVs. Lastly, you need to specify the data source. Hess = TRUE will provide us with a Hessian matrix that we need for a package later. If you don’t know what that is… that is absolutely fine!\n\nlibrary(nnet)\nmodel_de &lt;- multinom(vote_de ~ imueclt  + stflife + trstplt + blgetmg + \n                    gndr + yrbrn + eduyrs + hinctnta,\n                     data = ess_clean,\n                     Hess = TRUE)\n\n# weights:  60 (45 variable)\ninitial  value 2512.046776 \niter  10 value 2043.014063\niter  20 value 2023.586615\niter  30 value 1980.080494\niter  40 value 1938.289705\niter  50 value 1927.868641\niter  60 value 1926.043042\niter  70 value 1925.949503\niter  80 value 1925.873772\nfinal  value 1925.814902 \nconverged\n\nmodel_dk &lt;- multinom(vote_dk ~ imueclt  + stflife + trstplt + blgetmg + gndr +\n                     yrbrn + eduyrs + hinctnta,\n                     data = ess_clean,\n                     Hess = TRUE)\n\n# weights:  100 (81 variable)\ninitial  value 2525.935847 \niter  10 value 2265.701974\niter  20 value 2136.112054\niter  30 value 2056.238932\niter  40 value 1999.847654\niter  50 value 1950.379411\niter  60 value 1938.124756\niter  70 value 1934.172909\niter  80 value 1915.231553\niter  90 value 1908.197162\niter 100 value 1906.641552\nfinal  value 1906.641552 \nstopped after 100 iterations\n\n\n\n3.4.1 Re-leveling your DV\nIn my case, the German PRRP is called Alternative für Deutschland meaning it starts with an “A”. R tends to take the alphabetical order as a criterion for the baseline meaning that the baseline for your multinomial model is chosen based on the party which comes first in alphabetical order. Depending on what you want to show, you might want to change the baseline which we can do with the relevel() function. Let’s say we are not interested in vote choice regarding the PRRP but conservative parties and thus want to put the German Christian conservative party, the CDU/CSU, as a baseline. Here is how we could do this in R:\n\n# don't run this code chunk\n#| eval: false\n# you need to specify your DV as a factor for this; further, the ref must \n# contain the exact character label of the party\ness_clean$vote_de &lt;- relevel(as.factor(ess_clean$vote_de), ref = \"CDU/CSU\")",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#interpreting-a-multinomial-model",
    "href": "session3/session3.html#interpreting-a-multinomial-model",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.5 Interpreting a Multinomial Model",
    "text": "3.5 Interpreting a Multinomial Model\nYou already know that I like the stargazer package for displaying a regression table. This time I paid attention to what level of statistical significance leads to a star (*). I changed it so that, like in the summary() function, p-values below 0.05 will be used as the minimum level of statistical significance instead of 0.1. dep.var.caption = allows be to specify a caption for our DV and we can use our own labels for the IVs instead of the variables’ names by using the covariate.labels = argument.\nI have specified in the first chunk of code which arguments concern the generated output in LaTeX. I still recommend you start learning how to write papers in LaTeX. This is just to say that some arguments are not useful at all when type = \"text. But LaTeX generates more beautiful tables ;)\n\n# specifying the object in which the model is stored\nstargazer::stargazer(\n  model_de,\n  # adding a title to the table\n  title = \"Multinomial Regression Results Germany\",\n  # change this to the desired output format; either\n  # LaTeX, html, or text (depending on your document)\n  # editor\n  type = \"html\",\n  # some LaTeX information\n  float = TRUE,\n  # font size of the LaTeX table\n  font.size = \"small\",\n  # column width in final LaTeX table\n  column.sep.width = \"-10pt\",\n  # specifying the p-values which lead to stars in our\n  # table\n  star.cutoffs = c(.05, .01, .001),\n  # caption for the DV\n  dep.var.caption = c(\"Vote Choice\"),\n  # labels for our IVs; must be in the same order as our\n  # IVs in the initial model\n  covariate.labels = c(\n    \"Positivity Immigration\",\n    \"Satisfaction w/ Life\",\n    \"Trust in Politicians\",\n    \"Ethnic Minority\",\n    \"Gender\",\n    \"Age\",\n    \"Education\",\n    \"Income\"\n  )\n)\n\n\nMultinomial Regression Results Germany\n\n\n\n\n\n\n\n\n\nVote Choice\n\n\n\n\n\n\n\n\n\n\n\n\nCDU/CSU\n\n\nDie Linke\n\n\nFDP\n\n\nGrüne\n\n\nSPD\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n\n\n\n\n\n\nPositivity Immigration\n\n\n0.350***\n\n\n0.617***\n\n\n0.326***\n\n\n0.784***\n\n\n0.493***\n\n\n\n\n\n\n(0.063)\n\n\n(0.080)\n\n\n(0.076)\n\n\n(0.075)\n\n\n(0.065)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatisfaction w/ Life\n\n\n0.029\n\n\n-0.171*\n\n\n0.095\n\n\n-0.094\n\n\n-0.109\n\n\n\n\n\n\n(0.067)\n\n\n(0.083)\n\n\n(0.093)\n\n\n(0.079)\n\n\n(0.067)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust in Politicians\n\n\n0.513***\n\n\n0.281**\n\n\n0.424***\n\n\n0.352***\n\n\n0.398***\n\n\n\n\n\n\n(0.077)\n\n\n(0.092)\n\n\n(0.090)\n\n\n(0.085)\n\n\n(0.078)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthnic Minority\n\n\n-0.054***\n\n\n-0.443***\n\n\n-1.086***\n\n\n-0.382***\n\n\n-0.385***\n\n\n\n\n\n\n(0.002)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.002)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n0.859***\n\n\n0.240*\n\n\n0.573***\n\n\n0.928***\n\n\n0.395***\n\n\n\n\n\n\n(0.103)\n\n\n(0.096)\n\n\n(0.095)\n\n\n(0.137)\n\n\n(0.112)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n-0.013***\n\n\n0.007***\n\n\n-0.010***\n\n\n-0.002***\n\n\n-0.016***\n\n\n\n\n\n\n(0.0004)\n\n\n(0.0005)\n\n\n(0.001)\n\n\n(0.0005)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation\n\n\n0.014\n\n\n0.081\n\n\n0.036\n\n\n0.091\n\n\n0.040\n\n\n\n\n\n\n(0.051)\n\n\n(0.059)\n\n\n(0.060)\n\n\n(0.055)\n\n\n(0.052)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n0.116*\n\n\n-0.032\n\n\n0.114\n\n\n0.136*\n\n\n0.047\n\n\n\n\n\n\n(0.055)\n\n\n(0.066)\n\n\n(0.067)\n\n\n(0.062)\n\n\n(0.056)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n23.264***\n\n\n-16.715***\n\n\n17.583***\n\n\n-1.578***\n\n\n30.969***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.00005)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\n\n# the annotations of the above model would be the same for this model\nstargazer::stargazer(\n  model_dk,\n  title = \"Multinomial Regression Results Denmark\",\n  type = \"html\",\n  float = TRUE,\n  font.size = \"tiny\",\n  star.cutoffs = c(.05, .01, .001),\n  dep.var.labels = c(\"Germany\"),\n  dep.var.caption = c(\"Vote Choice\"),\n  covariate.labels = c(\n    \"Positivity Immigration\",\n    \"Satisfaction w/ Life\",\n    \"Trust in Politicians\",\n    \"Ethnic Minority\",\n    \"Gender\",\n    \"Age\",\n    \"Education\",\n    \"Income\"\n  )\n)\n\n\nMultinomial Regression Results Denmark\n\n\n\n\n\n\n\n\n\nVote Choice\n\n\n\n\n\n\n\n\n\n\n\n\nGermany\n\n\nDet Konservative Folkeparti\n\n\nDet Radikale Venstre\n\n\nEnhedslisten\n\n\nKristendemokraterne\n\n\nLiberal Alliance\n\n\nSF Socialistisk Folkeparti\n\n\nSocialdemokratiet\n\n\nVenstre\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n(7)\n\n\n(8)\n\n\n(9)\n\n\n\n\n\n\n\n\nPositivity Immigration\n\n\n-0.667***\n\n\n-0.440***\n\n\n-0.018\n\n\n0.162*\n\n\n-0.169\n\n\n-0.403***\n\n\n0.028\n\n\n-0.238***\n\n\n-0.400***\n\n\n\n\n\n\n(0.066)\n\n\n(0.083)\n\n\n(0.084)\n\n\n(0.081)\n\n\n(0.133)\n\n\n(0.087)\n\n\n(0.084)\n\n\n(0.059)\n\n\n(0.060)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatisfaction w/ Life\n\n\n0.153*\n\n\n0.238\n\n\n0.132\n\n\n-0.043\n\n\n0.174\n\n\n0.164\n\n\n0.071\n\n\n0.081\n\n\n0.249***\n\n\n\n\n\n\n(0.070)\n\n\n(0.131)\n\n\n(0.109)\n\n\n(0.082)\n\n\n(0.176)\n\n\n(0.125)\n\n\n(0.098)\n\n\n(0.059)\n\n\n(0.069)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust in Politicians\n\n\n0.180**\n\n\n0.349***\n\n\n0.300***\n\n\n0.003\n\n\n0.334*\n\n\n0.298**\n\n\n0.103\n\n\n0.264***\n\n\n0.451***\n\n\n\n\n\n\n(0.068)\n\n\n(0.093)\n\n\n(0.086)\n\n\n(0.076)\n\n\n(0.144)\n\n\n(0.095)\n\n\n(0.082)\n\n\n(0.061)\n\n\n(0.064)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthnic Minority\n\n\n0.557***\n\n\n23.666***\n\n\n-0.386***\n\n\n-0.389***\n\n\n11.010***\n\n\n0.596***\n\n\n0.179***\n\n\n-0.430***\n\n\n0.494***\n\n\n\n\n\n\n(0.001)\n\n\n(0.0003)\n\n\n(0.002)\n\n\n(0.006)\n\n\n(0.0004)\n\n\n(0.001)\n\n\n(0.002)\n\n\n(0.004)\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n-0.617***\n\n\n-0.128**\n\n\n-0.378***\n\n\n-0.030\n\n\n-0.706***\n\n\n-0.507***\n\n\n0.204\n\n\n-0.164\n\n\n-0.182\n\n\n\n\n\n\n(0.185)\n\n\n(0.047)\n\n\n(0.106)\n\n\n(0.204)\n\n\n(0.007)\n\n\n(0.029)\n\n\n(0.119)\n\n\n(0.130)\n\n\n(0.138)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n-0.023***\n\n\n-0.024***\n\n\n-0.001\n\n\n-0.008***\n\n\n-0.005***\n\n\n0.054***\n\n\n-0.010***\n\n\n-0.024***\n\n\n-0.025***\n\n\n\n\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.0005)\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation\n\n\n-0.144***\n\n\n-0.035\n\n\n-0.017\n\n\n-0.052\n\n\n-0.083\n\n\n-0.036\n\n\n-0.042\n\n\n-0.103***\n\n\n-0.105***\n\n\n\n\n\n\n(0.033)\n\n\n(0.038)\n\n\n(0.035)\n\n\n(0.034)\n\n\n(0.060)\n\n\n(0.045)\n\n\n(0.036)\n\n\n(0.029)\n\n\n(0.030)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n-0.023\n\n\n0.216**\n\n\n0.118\n\n\n-0.067\n\n\n-0.165\n\n\n0.162*\n\n\n-0.050\n\n\n-0.008\n\n\n0.070\n\n\n\n\n\n\n(0.064)\n\n\n(0.081)\n\n\n(0.071)\n\n\n(0.065)\n\n\n(0.115)\n\n\n(0.080)\n\n\n(0.069)\n\n\n(0.057)\n\n\n(0.059)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n48.626***\n\n\n-1.783***\n\n\n0.004***\n\n\n17.170***\n\n\n-12.969***\n\n\n-107.485***\n\n\n18.189***\n\n\n51.982***\n\n\n50.057***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\nThe format of the regression table on our Danish model is not ideal since the names of the parties are quite long and overlap. Blame this on my lack of knowledge of abbreviations of Danish parties…",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#interpreting-a-multinomial-regression-table",
    "href": "session3/session3.html#interpreting-a-multinomial-regression-table",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.6 Interpreting a Multinomial Regression Table",
    "text": "3.6 Interpreting a Multinomial Regression Table\nWe can see that many many things are going on in this regression table. Let us try to analyze our results step by step.\nFirst of all, we can see that we have many variables that are statistically significant (lots of stars yay!). This is always a good sign. Note also that the baseline was the party AFD. You can see this based on the fact that the category AFD which our DV can take on is not given in our table. This means that whenever we see the results where the DV is one of the parties, R has calculated the coefficients based on the logic that the respondent would have voter for either the party in the dependent variable or the party of the baseline, which in our case is that of the AFD. In more mathematical terms these are several single logistic regressions always with regards to the baseline AFD which are then aggregated to a multinomial regression. And to be slightly more mathematical, this means our DV is technically: \\(1 = DV\\) and then \\(0 = AFD\\).\nTherefore, we can interpret the results exactly like we would for a logistic regression. Last week it was about the likelihood of voting abstention, this week it is the likelihood of voting for the CDU/CSU instead of the AFD, or voting for the SPD instead of the AFD, or voting for Die Linke instead of the AFD, and so on. You get the idea hopefully.\nRemember that these are the coefficients of logistic regressions. We cannot interpret them linearily like in OLS. For now, the regression table tells us something about the statistical significance of our predictors and the direction of association: whether or not a statistically significant predictor increases or decreases the likelihood of voting for either or.\n\n3.6.1 The Hypotheses\nAs a reminder, these were my initial (frankly also bad) hypotheses:\n\nH1: Thinking that immigrants enrich a country’s culture decreases the likelihood of voting for PRRPs.\nH2: Having less trust in politicians increases the likelihood of voting for PRRPs than voting for other parties.\n\nI am now interested to see the effect of positivity toward migration and trust in politicians on the vote choice for each party instead of the AFD. What we can see is that a one-unit increase in positive attitudes toward migration (thinking that immigrants culturally enrich the respondents’ country) raises the likelihood for voting for all other parties instead of voting for the AFD. In the case of the first column, in which the vote was either for the CDU/CSU or the AFD, a one unit increase in stances on immigration results in a higher likelihood of voting of voting for the CDU/CSU than the AFD.\nIf we now turn to trust in politicians and this variable’s effect on vote choice for the different German parties, we can see that overall there is a statistically significant a positive association with having more trust in politicians and also voting for other parties than the AFD. In return, this also means that low trust in politicians raises the likelihood of voting for the AFD.\nYou could obviously exponentiate the values that we have here in order to get the odds-ratio. But I have tortured you enough with ORs and predicted probabilities are much more intuitively interpreted. Therefore, we will calculate them in the next section.",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#predicted-probabilities",
    "href": "session3/session3.html#predicted-probabilities",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.7 Predicted Probabilities",
    "text": "3.7 Predicted Probabilities\nYou all hopefully still remember the idea of predicted probabilities which we have already seen last time for a simply logistic regression. You hold all but one predictor variables (IVs) constant at their mean or another logical value. The one predictor which you do not hold constant you let alternate/vary to estimate the predicted probabilities of this specific variable of interest and the different values it can take on (on your dependent variable). The predicted probabilities can be tricky to code manually and we are not going to do this again but we will use a package that can do this for us.\nThe package is called MNLpred and allows us to specify the variable of interest. This packages makes draws from our posterior distribution (hello Bayesian statistics) and simulates our coefficients n-times (we tell it how many times to run the simulation) and then takes the mean value of all of our simulations. This way, we end up more or less with the same predicted probabilities that we have seen last week. These are much more easily interpreted than relative risk ratios (the odds-ratios of multinomial regressions) and can be plotted.\n\nlibrary(MNLpred)\npred1 &lt;- mnl_pred_ova(\n  model = model_de,\n  # specify data source\n  data = ess_clean,\n  # specify predictor of interest\n  x = \"imueclt\",\n  # the steps which should be used for the simulated prediction\n  by = 1,\n  # this would be for replicability, we do not care about it\n  # here\n  seed = \"random\",\n  # number of simulations\n  nsim = 100,\n  # confidence intervals\n  probs = c(0.025, 0.975)\n)\n\nMultiplying values with simulated estimates:\n================================================================================\nApplying link function:\n================================================================================\nDone!\n\n\nThe pred1 object now contains the simulated means for each party at each step of our predictor of interest meaning that there are 10 simulated mean values for each value that imueclt can take on for each party:\n\npred1$plotdata |&gt; head()\n\n  imueclt vote_de       mean      lower      upper\n1       0 CDU/CSU 0.24240032 0.17868819 0.33066569\n2       1 CDU/CSU 0.18811005 0.14598253 0.24493147\n3       2 CDU/CSU 0.14129106 0.11467209 0.17598534\n4       3 CDU/CSU 0.10262549 0.08367724 0.12491718\n5       4 CDU/CSU 0.07208947 0.05589069 0.08975706\n6       5 CDU/CSU 0.04901826 0.03538595 0.06446253\n\n\nLet’s simulate the exact same thing for our second hypothesis regarding the trust in politicians:\n\npred2 &lt;- mnl_pred_ova(\n  model = model_de,\n  data = ess_clean,\n  x = \"trstplt\",\n  by = 1,\n  seed = \"random\",\n  nsim = 100,\n  probs = c(0.025, 0.975)\n)\n\nMultiplying values with simulated estimates:\n================================================================================\nApplying link function:\n================================================================================\nDone!\n\n\nThe results, which we have both stored respectively in the objects pred1 and pred2 can be used for a visualization with ggplot().\n\nlibrary(ggplot2)\nggplot(data = pred2$plotdata, aes(\n  x = trstplt,\n  y = mean,\n  ymin = lower,\n  ymax = upper\n)) +\n  # this gives us the confidence intervals\n  geom_ribbon(alpha = 0.1) +\n  # taking the mean of the values\n  geom_line() +\n  # here we display the predicted probabilities for all parties in one plot\n  facet_wrap(. ~ vote_de, ncol = 2) +\n  # putting the values of the y-axis in percentages\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  # the x-axis follows the 0-10 scale of the predictor\n  scale_x_continuous(breaks = c(0:10)) +\n  # specifying the ggplot theme\n  theme_minimal() +\n  # lastly you only need to label your axes; Always label your axes ;)\n  labs(y = \"Predicted probabilities\",\n       x = \"Trust in Politicians\") \n\n\n\n\n\n\n\nHere we can see very well by how many percent the likelihood increases or decreases for each party given that our independent variable, our predictor, of trust in politicians increases (increasing values mean more trust in politicians).\nWe can also visualize our predicted probabilities in one single plot. I made the effort of coordinating the colors so that they would be displayed in the colors of the parties. If you want to have a color selector to get the HEX color codes, you can click on this link: (it will say Google Farbwähler, which is not a scam but German…). As by recently, R will also display the color you have selected.\n\nggplot(data = pred2$plotdata, aes(\n  x = trstplt,\n  y = mean,\n  color = as.factor(vote_de)\n)) +\n  geom_smooth(aes(ymin = lower, ymax = upper), stat = \"identity\") +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_x_continuous(breaks = c(0:10)) +\n  scale_color_manual(\n    values = c(\n      \"#03c2fc\",\n      \"#000000\",\n      \"#f26dd5\",\n      \"#FFFF00\",\n      \"#00e81b\",\n      \"#fa0000\"\n    ),\n    name = \"Vote\",\n    labels = c(\"AFD\", \"CDU\", \"DIE LINKE\", \"FDP\",\n               \"GRUENE\", \"SPD\")\n  ) +\n  ylab(\"Predicted Probability Vote\") +\n  xlab(\"Trust in Politicians\") +\n  theme_minimal()\n\n\n\n\n\n\n\nThis here is the plot for our first hypothesis for which we have stored the predicted probabilities in the object pred1:\n\nlibrary(ggplot2)\nggplot(data = pred1$plotdata, aes(\n  x = imueclt,\n  y = mean,\n  ymin = lower,\n  ymax = upper\n)) +\n  geom_ribbon(alpha = 0.1) + # Confidence intervals\n  geom_line() + # Mean\n  facet_wrap(. ~ vote_de, ncol = 2) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + # % labels\n  scale_x_continuous(breaks = c(0:10)) +\n  theme_minimal() +\n  labs(y = \"Predicted probabilities\",\n       x = \"Positivity towards Immigrants\") # Always label your axes ;)\n\n\n\n\n\n\n\nAnd here the code which puts all the predicted probabilities in one plot:\n\nggplot(data = pred1$plotdata, aes(\n  x = imueclt,\n  y = mean,\n  color = as.factor(vote_de)\n)) +\n  geom_smooth(aes(ymin = lower,\n                  ymax = upper),\n              stat = \"identity\") +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_x_continuous(breaks = c(0:10)) +\n  scale_color_manual(\n    values = c(\n      \"#03c2fc\",\n      \"#000000\",\n      \"#f26dd5\",\n      \"#FFFF00\",\n      \"#00e81b\",\n      \"#fa0000\"\n    ),\n    name = \"Vote\",\n    labels = c(\"AFD\", \"CDU\", \"DIE LINKE\", \"FDP\",\n               \"GRUENE\", \"SPD\")\n  ) +\n  ylab(\"Predicted Probability Vote\") +\n  xlab(\"Positivity towards Immigration\")",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#diagnostics-of-multinomial-models",
    "href": "session3/session3.html#diagnostics-of-multinomial-models",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.8 Diagnostics of Multinomial Models",
    "text": "3.8 Diagnostics of Multinomial Models\nI have talked about diagnostics of models before. This will be the first time that we really touch upon that in models that are not linear like OLS. Usually this is a step which you should take between the building and the final interpretation of your model.\nThe estimates of your model change depending on several influences. The number of predictors, the scaling of your predictors, the scaling of your dependent variable or the coding of your dependent variable. All these kind of things (and many more) will have an effect on your model’s results. We need to be sure that we have a good amount of variables to account for enough variance. But we also need to make sure that we do not overfit our model, meaning that we put in too many predictors for example. We also need to make sure that our model is not biased by the scaling of our variables. This is why we need to check for multicollinearity, heteroskedasticity and other things.\nWe are firstly concerned with the goodness of fit of our model. In a linear model using the OLS method, we have looked at the \\(R^2\\) and adjusted \\(R^2\\) of the models. This tells us something about how much variance of the DV is explained by our IVs. Unfortunately, this measure does not exist for logistic or multinomial models. But the good news is that we can calculate something that is called McFadden’s Pseudo \\(R^2\\). It is interpreted in a similar way as you would do it with a normal \\(R^2\\) meaning that anything ranging between 0.2 and 0.4 is a result that should make us happy.\nThis is how you do this in R:\n\n# you obviously need to install the package first\nlibrary(pscl)\n\nClasses and Methods for R developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University\nSimon Jackman\nhurdle and zeroinfl functions by Achim Zeileis\n\npR2(model_de)\n\nfitting null model for pseudo-r2\n# weights:  12 (5 variable)\ninitial  value 2512.046776 \niter  10 value 2158.240962\niter  10 value 2158.240953\niter  10 value 2158.240953\nfinal  value 2158.240953 \nconverged\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1925.8149021 -2158.2409526   464.8521011     0.1076924     0.2821995 \n         r2CU \n    0.2958110 \n\n\n\n3.8.1 Hetereoskedasticity and Multicollinearity\nThen there are issues of scary words like multicollinearity or heteroskedasticity (oftentimes also refered to as “heteroske-something”). These two things describe two phenomena that can skew our estimations and, in the worst case scenario, will lead to wrong inferences. Therefore, we must check for them in all different kinds of models, be it a simple model using the OLS method, or a logistic regression or a multinomial regression. There are ways to test for potential problems that might arise and also ways to work our way around them if ever we encounter them.\n\n3.8.2 Multicollinearity and how to eliminate it\nFor now, we will only look at the potential issue of multicollinearity. It occurs when your independent variables are correlated among each other. This means that they vary very similarly in their values and measure either similar things or measure things the same way. The higher the multicollinearity within your model, the less reliable are your statistical inferences.\nWe can detect the amount and measure of (multi)collinearity by calculating the Variance Inflation Factor (VIF). It measures the amount of correlation between our predictors. The VIF should be below 10. If it is below 0.2, this is a potential problem. Anything below 0.1 should have us really worried. To do this in R, we use the vif() function of the car package. However, it does not work on the object of a multinomial model. Thus, we cheat our way around it and build a GLM model (glm()) in which we set our DV as factors and pretend that they are binomially distributed. This way, R sort of manually calculates the individual logistic regressions according to a baseline and we can calculate the VIF for the IVs individually.\n\nmodel_vif &lt;-\n  glm(\n    as.factor(vote_de) ~ imueclt  + stflife + trstplt + blgetmg +\n      gndr + yrbrn + eduyrs + hinctnta,\n    data = ess_clean,\n    family = binomial()\n  )\n\ncar::vif(model_vif)\n\n imueclt  stflife  trstplt  blgetmg     gndr    yrbrn   eduyrs hinctnta \n1.350982 1.141501 1.274526 1.013258 1.037425 1.112390 1.260009 1.202217 \n\n\nBased on the results, we can see that our variance is not inflated since all values are below 10. That is great news! A VIF of 1 means that there is no correlation within our predictors, a VIF between 1 and 5 (which is quite normal) indicates slight correlation, and a VIF between 5 and 10 shows a strong correlation.\nIf, however, you should encounter issues of multicollinearity, you should test the VIFs for different versions of your model by starting to drop the IV with the highest VIF and see how that affects your VIFs overall. Or you check the variables which have high values, see if theoretically speaking they measure similar things, and combine them into a single measure.\nIt is important to do this step in order to test the validity and reliability of our models!",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#goodness-of-fits-and-its-other-measures",
    "href": "session3/session3.html#goodness-of-fits-and-its-other-measures",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.9 Goodness of Fits and its other measures",
    "text": "3.9 Goodness of Fits and its other measures\nYou have seen me use the term goodness of fit before and that this becomes very important in quantitative research when you try to model statistical relationships. Until now, we have always only modeled one model and then interpreted its coefficients and model values. We have seen the \\(R^2\\) and adjusted \\(R^2\\) and we have mostly seen bad OLS models which showed very low values in both these measures. However, this measure does not always exist for generalized linear models. Thus, statisticians have come up with other ways to compare models and their goodness of fit. As a rule of thumb, we should always favor models which explain as much as possible by not making too many (strong) assumptions and overfitting our predictors, e.g. adding too many in one regression etc. In one of your introduction to (political) science classes, you might have heard of Ockham’s razor; this is the same idea but for statistical models.\nGoodness of fit in our case refers to how well the model which we have constructed, fits the set of our made observations. Thus, goodness of fit somewhat measures the discrepancy between our observed and expected values given our model. If we do not have an adjusted \\(R^2\\), we need to use other information criteria to determine which model fits best our data. There is quite an abundance of criteria which come to mind. Some of them are related to specific kinds of statistical models, whereas some are more general. The two which I would like to mention here are the AIC and the BIC.\n\n3.9.1 AIC (Akaike Information Criterion)\nDon’t be like me and think for years AIC was a bad abbreviation of Akaike. It actually stands for Akaike Information Criterion. It is calculated based on the number of predictors of our model and how well it reproduces our data (the likelihood estimation). If you go back to our multinomial regressions above, you can see that the the last line of our table shows the AIC for this model. Individually, this information criterion is meaningless. It becomes important when we compare it to an AIC of a similar model and check which one indicates a better fit.\nWhat would a similar model look like? Well, if we dropped one of our IVs for example, we would alter the model a bit but keep its global structure. In that case, we would generate a second but different AIC. Comparing the AIC then tells us something about which model (meaning which composition of model) indicates a better fit.\nWhat is a better AIC? The lower AIC indicates that the model fits our data better than the model with the higher AIC. This is simply a mathematical measure. Stand-alone values of the AIC do not tell us much. They need to be considered in comparison to other values.\n\n3.9.2 BIC (Bayesian Information Criterion)\nBayesian Statistics 1 are super fascinating and I will include them wherever I can. Luckily, the BIC is very common and is to the AIC what the adjusted \\(R^2\\) is to the \\(R^2\\). This means that it is a “stricter” measure of goodness of fit than the AIC. It is quite similar to the AIC but differs in that it penalizes you for adding “less useful” variables to your model (potentially overfitting or overcomplexifing your model). Thus, similarly to the AIC, we should also favor the lower BIC!",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#apis-and-fun-data-visualization-optional",
    "href": "session3/session3.html#apis-and-fun-data-visualization-optional",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.10 APIs and Fun Data Visualization (OPTIONAL!)",
    "text": "3.10 APIs and Fun Data Visualization (OPTIONAL!)\nIn this session’s optional section, I will introduce you to the logic of APIs and how to use them in R. Further, we will use data from Spotify (which we will collect through their API) to make fun data visualizations!\n\n3.10.1 What is an API?\nAPI stands for Application Programming Interface. It is a way for two applications to communicate with each other. This is a very technical way of saying that an API allows us to carefully and gently collect data from a server, a website or a database. APIs are a very common way of collecting data from websites. Most websites use internal APIs to communicate with their own databases 2, whereas some websites also offer public APIs which allow you to collect data from their website.\nThink about APIs like this: You go to a restaurant and you want to eat. You could be very rude, ignore every other customer, ignore every social norm you have ever learned and go to the kitchen to tell the chef what you want. Now, if only one person did this, nothing would happen (even if I were surprised if the chef would take your order). But if now every customer did this, started yelling their order at the chef, he would probably just stop working and throw in the towel, or worse, throw you out of the restaurant. To avoid that, there are waiters and waitresses that usually come to your table, take your order, deliver that to the kitchen and once your dish is ready, you will get it to your table. Transposing this onto APIs, the waiter/waitress is our API. They make sure that we can communicate with the kitchen (the database) without disrupting the work of the chef (the server).\nUsually APIs are win-win situations for us and for them. They can control how much data we collect from them, they can dictate the rules and have traces of what we were doing when and where. And we can collect data without disrupting their work. However, sometimes APIs are not public and we need to find other ways to collect data from websites. This is called webscraping and is a bit more complicated and in the appendix.\nSome APIs are more useful than others. The New York Times’ one is unfortunately quite useless. Spotify’s API is more fun, as we will see in a bit. Twitter used to have one of the best developer’s accesses to their data through a wonderful API until he-who-must-not-be-named destroyed the platform. But also government websites have APIs which make it a bit more regulated to access their data.\n\n3.10.2 Spotify API\nFor the sake of the example, we will use the Spotify API to communicate with the Spotify servers and ask them for data. You usually have to sign up for an API on the respective website. If you want to reproduce my code, you will have to sign up for a Spotify API here. Once you have signed up, you will be able to create a new app. This will give you a client ID and a client secret. These are your credentials to access the API. You will need them to access the API.\n\nneeds('spotifyr',\n      'tidyverse',\n      'plotly',\n      'ggimage',\n      'httpuv',\n      'httr', \n      'usethis')\n\n\n3.10.3 System Enivronments and API keys\nWe will get a bit more technical and speak about good practices in R for a second. When you apply to an API or sign up for an access, you will usually get a sort of key, clientID or something to authenticate yourself whenever you make requests to the API. This way, they know that it is you who is making requests and that you are validated by them. This information is sensitive! You do not share it with anyone, and you never show this to anyone. This is why you need to store it somewhere safe. One of the options we have in R is to save them in our system environment. It is a file in which we can store these things. Honestly, the keys to APIs are really long and complicated and you do not want to type them in every time you want to use them. So, we will store them in our system environment. This is a bit more complicated than just typing them in, but it is a good practice. It is also not the safest way to do it but for simplicity’s sake we will do it like this for now.\n\n\n\n\n\n\nIf you want to follow my code and run it on your end, you will have to first sign up to the Spotify developer’s portal, generate the key and clientID and come back to my script then.\n\n\n\nThis line of code opens up a file called .Renviron in which we can store our keys. If you have never done this before, it will open up a new file. If you have done this before, it will open up the file in which you probably already have things stored.\n\nusethis::edit_r_environ()\n\nOnce you have opened the file, you can add the following lines to it. You will have to replace the XXXX with your own keys. Once you have done that, save the file and restart R! This is necessary for the changes to take effect.\n\nSPOTIFY_CLIENT_ID = \"XXXX\"\nSPOTIFY_CLIENT_SECRET = \"XXXX\"\n# this has to be the same localhost as in indicated in the spotify developer portal\nSPOTIFY_REDIRECT_URI = \"http://localhost:1410/\"\n\nNow that we have added the sensitive information to our R Environment, it will remain there until we decide to delete it. If we need to load it now, we can run these lines of code:\n\nclient_id &lt;- Sys.getenv(\"SPOTIFY_CLIENT_ID\")\nclient_secret &lt;- Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\nredirect_uri &lt;- Sys.getenv(\"SPOTIFY_REDIRECT_URI\")\n\n\n3.10.4 Getting the data from Spotify\nThis is where the technical part is over and we can finally turn to the more fun side of things and collect data from Spotify. We will use the spotifyr package to do so. This package is a wrapper for the Spotify API. It makes it easier for us to collect data from Spotify. It has been developed by Charlie Thompson and is a great example of how to use APIs in R.\nAgain, credit where credit is due: When I first discovered the Spotify API, I got a lot of inspiration from this chapter by Marie-Lou Sohnius and Johanna Mehltretter.\nStore your credentials in an object. This will only work if you have properly saved your Spotify access keys and credentials to your system environment.\n\naccess_token &lt;- get_spotify_access_token()\n\nWe will use the get_playlist_audio_features(). This function will collect all the information of a specific Spotify playlist. There is an insane amount of information that we can already extract with this. We will use the playlist of the top 50 songs in the France You can find the playlist here. The playlist has a unique identifier, called a URI (= Uniform Resource Identifier which is a sequence that identifies resources found on the internet). To find the URI, you simply open the Spotify web browser, log in, and go to any playlist of your choice. The URL for the Top 50 of France is https://open.spotify.com/playlist/37i9dQZEVXbIPWwFssbupI. The URI is the last part of the URL: 37i9dQZEVXbIPWwFssbupI. Below, we simply feed it into the function and can store all the information about the Top 50 songs in an object.\n\ntop50 &lt;- get_playlist_audio_features(playlist_uris = '37i9dQZEVXbIPWwFssbupI') \n\nWe can have a look at the data. It is a tibble with 50 rows and 61 columns. Each row is a song and each column is a variable. We have the name of the song, the artist, the album, the URI, the danceability, the energy, the key, the loudness, the mode, the speechiness, the acousticness, the instrumentalness, the liveness, the valence, the tempo, the duration, the time signature and the date of release. All in all, quite some data.\n\nglimpse(top50)\n\nRows: 50\nColumns: 61\n$ playlist_id                        &lt;chr&gt; \"37i9dQZEVXbIPWwFssbupI\", \"37i9dQZE…\n$ playlist_name                      &lt;chr&gt; \"Top 50 - France\", \"Top 50 - France…\n$ playlist_img                       &lt;chr&gt; \"https://charts-images.scdn.co/asse…\n$ playlist_owner_name                &lt;chr&gt; \"Spotify\", \"Spotify\", \"Spotify\", \"S…\n$ playlist_owner_id                  &lt;chr&gt; \"spotify\", \"spotify\", \"spotify\", \"s…\n$ danceability                       &lt;dbl&gt; 0.495, 0.791, 0.794, 0.934, 0.472, …\n$ energy                             &lt;dbl&gt; 0.871, 0.499, 0.578, 0.807, 0.471, …\n$ key                                &lt;int&gt; 10, 8, 1, 6, 10, 11, 5, 7, 1, 6, 9,…\n$ loudness                           &lt;dbl&gt; -2.333, -8.472, -5.932, -4.286, -5.…\n$ mode                               &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,…\n$ speechiness                        &lt;dbl&gt; 0.3420, 0.0509, 0.2500, 0.1730, 0.0…\n$ acousticness                       &lt;dbl&gt; 0.399000, 0.446000, 0.103000, 0.142…\n$ instrumentalness                   &lt;dbl&gt; 0.00e+00, 2.41e-05, 1.24e-05, 1.99e…\n$ liveness                           &lt;dbl&gt; 0.0674, 0.0899, 0.3630, 0.2990, 0.1…\n$ valence                            &lt;dbl&gt; 0.8770, 0.6690, 0.9670, 0.8780, 0.2…\n$ tempo                              &lt;dbl&gt; 103.639, 99.986, 126.032, 127.967, …\n$ track.id                           &lt;chr&gt; \"3Zw66L4FvI9YUUx17OReRD\", \"6XjDF6nd…\n$ analysis_url                       &lt;chr&gt; \"https://api.spotify.com/v1/audio-a…\n$ time_signature                     &lt;int&gt; 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 3, 4,…\n$ added_at                           &lt;chr&gt; \"2024-04-25T10:53:22Z\", \"2024-04-25…\n$ is_local                           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ primary_color                      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ added_by.href                      &lt;chr&gt; \"https://api.spotify.com/v1/users/\"…\n$ added_by.id                        &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ added_by.type                      &lt;chr&gt; \"user\", \"user\", \"user\", \"user\", \"us…\n$ added_by.uri                       &lt;chr&gt; \"spotify:user:\", \"spotify:user:\", \"…\n$ added_by.external_urls.spotify     &lt;chr&gt; \"https://open.spotify.com/user/\", \"…\n$ track.preview_url                  &lt;chr&gt; NA, \"https://p.scdn.co/mp3-preview/…\n$ track.available_markets            &lt;list&gt; &lt;\"AR\", \"AU\", \"AT\", \"BE\", \"BO\", \"BR…\n$ track.explicit                     &lt;lgl&gt; FALSE, TRUE, TRUE, TRUE, FALSE, FAL…\n$ track.type                         &lt;chr&gt; \"track\", \"track\", \"track\", \"track\",…\n$ track.episode                      &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ track.track                        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,…\n$ track.artists                      &lt;list&gt; [&lt;data.frame[1 x 6]&gt;], [&lt;data.fram…\n$ track.disc_number                  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ track.track_number                 &lt;int&gt; 4, 1, 1, 3, 4, 1, 11, 14, 1, 1, 2, …\n$ track.duration_ms                  &lt;int&gt; 141226, 222000, 217259, 153649, 180…\n$ track.href                         &lt;chr&gt; \"https://api.spotify.com/v1/tracks/…\n$ track.name                         &lt;chr&gt; \"Position\", \"Gata Only\", \"Petit gén…\n$ track.popularity                   &lt;int&gt; 79, 98, 81, 73, 88, 100, 80, 72, 79…\n$ track.uri                          &lt;chr&gt; \"spotify:track:3Zw66L4FvI9YUUx17ORe…\n$ track.is_local                     &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ track.album.available_markets      &lt;list&gt; &lt;\"AR\", \"AU\", \"AT\", \"BE\", \"BO\", \"BR…\n$ track.album.type                   &lt;chr&gt; \"album\", \"album\", \"album\", \"album\",…\n$ track.album.album_type             &lt;chr&gt; \"album\", \"single\", \"single\", \"album…\n$ track.album.href                   &lt;chr&gt; \"https://api.spotify.com/v1/albums/…\n$ track.album.id                     &lt;chr&gt; \"2dF2ZByoSuH2ZZrzoGpjzQ\", \"5tSQtQGk…\n$ track.album.images                 &lt;list&gt; [&lt;data.frame[3 x 3]&gt;], [&lt;data.fram…\n$ track.album.name                   &lt;chr&gt; \"Prime\", \"Gata Only\", \"Petit génie\"…\n$ track.album.release_date           &lt;chr&gt; \"2024-02-15\", \"2024-02-02\", \"2023-0…\n$ track.album.release_date_precision &lt;chr&gt; \"day\", \"day\", \"day\", \"day\", \"day\", …\n$ track.album.uri                    &lt;chr&gt; \"spotify:album:2dF2ZByoSuH2ZZrzoGpj…\n$ track.album.artists                &lt;list&gt; [&lt;data.frame[1 x 6]&gt;], [&lt;data.fram…\n$ track.album.total_tracks           &lt;int&gt; 12, 1, 1, 15, 15, 1, 12, 23, 1, 1, …\n$ track.album.external_urls.spotify  &lt;chr&gt; \"https://open.spotify.com/album/2dF…\n$ track.external_ids.isrc            &lt;chr&gt; \"FRD8V2400040\", \"QZL382406049\", \"QM…\n$ track.external_urls.spotify        &lt;chr&gt; \"https://open.spotify.com/track/3Zw…\n$ video_thumbnail.url                &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ key_name                           &lt;chr&gt; \"A#\", \"G#\", \"C#\", \"F#\", \"A#\", \"B\", …\n$ mode_name                          &lt;chr&gt; \"minor\", \"minor\", \"minor\", \"major\",…\n$ key_mode                           &lt;chr&gt; \"A# minor\", \"G# minor\", \"C# minor\",…\n\n\n\n3.10.5 Preparing the data\nFor any further operation, we will first have to create a rank, or identifier for each song. In the next step, we use a simple for-loop to extract the artist and the image of the album cover. We will use the ggimage package to plot the album covers.\n\ntop50$rank &lt;- seq.int(nrow(top50))\n\n\nfor (i in 1:50) {\n  top50$artist[i] &lt;- top50[[28]][[i]]$name\n  top50$image[i] &lt;- c(top50[[48]][[i]]$url[2], size=10, replace = TRUE)\n}\n\n\n3.10.6 Plotting the data\nUnfortunately, Spotify has no documentation on how they measure or decide which song is more lively, acoustic or danceable than others. But we can probably get the gist of it by listening to the songs. Below we can plot the songs by their danceability and happiness. The happier the song, the more to the right it is. The more danceable the song, the higher it is. We can see that there seems to be a linear relationship between danceability and happiness, which does not necessarily come as a surprise.\n\nggplot(data = top50, aes(x = valence, y = danceability, text = (\n  paste(\"Track:\",\n        track.name,\n        \"&lt;br&gt;\",\n        \"Artist:\",\n        artist)\n))) +\n  geom_image(aes(image = image), asp = 1.5) +\n  theme_minimal() +\n  ylab(\"Danceability\") +\n  xlab(\"Happiness\") \n\nBut we can also see that there are some outliers. There are some songs that are very happy but not very danceable. And there are some songs that are very danceable but not very happy. We can have a look at these songs.\nUsing the ggridges package we can study the distribution of some of our variables. The code below extracts data on all Taylor Swift albums and songs. We can then plot the distribution of the happiness of her songs:\n\nlibrary(ggridges)\n\ntaylor &lt;- get_artist_audio_features('taylor swift')\n\ntaylor_filtered &lt;- taylor |&gt;\n  filter(\n    album_name %in% c(\n      \"Speak Now (Taylor's Version)\",\n      \"evermore\",\n      \"Fearless (Taylor's Version)\",\n      \"folklore\",\n      \"Midnights (3am Edition)\",\n      \"Red (Taylor's Version)\",\n      \"Taylor Swift\",\n      \"reputation\",\n      \"1989 (Taylor's Version)\",\n      \"Lover\"\n    )\n  ) |&gt;\n  arrange(album_release_year)\n\nggplot(taylor_filtered,\n       aes(x = valence, y = album_name, fill = after_stat(x))) +\n  geom_density_ridges_gradient() +\n  scale_fill_viridis_c(name = \"Happiness\", option = \"C\") +\n  labs(title = \"Joyplot of Taylor Swift Albums (Based on Spotify's valence value)\",\n       subtitle = \" Built using the R-package spotifyr\") +\n  theme_minimal() +\n  ylab(\"Album Name\") +\n  xlab(\"Happiness score according to Spotify\")\n\nPicking joint bandwidth of 0.0866\n\n\n\n\n\n\n\n\nHere is the danceability of her albums:\n\nggplot(taylor_filtered,\n       aes(x = danceability, y = album_name, fill = after_stat(x))) +\n  geom_density_ridges_gradient() +\n  scale_fill_viridis_c(name = \"Danceability\", option = \"F\") +\n  labs(title = \"Danceplot of Taylor Swift Albums\",\n       subtitle = \" Built using the spotifyr package\") +\n  theme_minimal() +\n  ylab(\"Album Name\") +\n  xlab(\"Danceability score according to Spotify\")\n\nPicking joint bandwidth of 0.0449",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#footnotes",
    "href": "session3/session3.html#footnotes",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "The second school of doing statistics. What we are doing is called frequentist statistics. Bayesian statistics are a bit more complicated and require a different way of thinking about statistics but they are the more intuitive way of conducting statistics (without p-values but with something you could call certainty). They are also more computationally expensive and thus, have only gained traction over the recent decades. They are gaining more and more popularity. If you are interested in learning more about them, I recommend the book by Richard McElreath (2016) Statistical Rethinking: A Bayesian Course with Examples in R and Stan. He also has lectures on YouTube that follow the book. Definitely worth a try!↩︎\nThis is a fun exercise for webscraping and you can imitate the websites’ calls to their server API to collect data but more on that at a later point.↩︎",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session4/session4.html",
    "href": "session4/session4.html",
    "title": "\n4  Diff-in-Diff\n",
    "section": "",
    "text": "4.1 Introduction\nThis week we will start talking about Causal Inference in R. You will have already seen the theoretical and mathematical parts with Brenda in the lecture. This script is supposed to initiate you to the practical part of Causal Inference.\nThis session will show you how to :\nlibrary(haven)\nlibrary(knitr)\nlibrary(modelsummary)\n\nVersion 2.0.0 of `modelsummary`, to be released soon, will introduce a\n  breaking change: The default table-drawing package will be `tinytable`\n  instead of `kableExtra`. All currently supported table-drawing packages\n  will continue to be supported for the foreseeable future, including\n  `kableExtra`, `gt`, `huxtable`, `flextable, and `DT`.\n  \n  You can always call the `config_modelsummary()` function to change the\n  default table-drawing package in persistent fashion. To try `tinytable`\n  now:\n  \n  config_modelsummary(factory_default = 'tinytable')\n  \n  To set the default back to `kableExtra`:\n  \n  config_modelsummary(factory_default = 'kableExtra')\n\nlibrary(scales)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Diff-in-Diff",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diff-in-Diff</span>"
    ]
  },
  {
    "objectID": "session4/session4.html#introduction",
    "href": "session4/session4.html#introduction",
    "title": "\n4  Diff-in-Diff\n",
    "section": "",
    "text": "Prepare data for Causal Inference\nPerform a simple Diff-in-Diff analysis in R\nInterpret the results of a Diff-in-Diff analysis in R",
    "crumbs": [
      "Diff-in-Diff",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diff-in-Diff</span>"
    ]
  },
  {
    "objectID": "session4/session4.html#diff-in-diff",
    "href": "session4/session4.html#diff-in-diff",
    "title": "\n4  Diff-in-Diff\n",
    "section": "\n4.2 Diff-in-Diff",
    "text": "4.2 Diff-in-Diff\nFor the first example on Diff-in-Diff I am going to use the code written by the brilliant Rohan Alexander who has written one of the (if not the) best introduction to Data Analysis in R which you can find here. Furthermore, the example he uses is a paper written by Charles Angelucci and Julia Cagé who you probably know already.\nThe paper is called “Newspapers in Times of Low Advertising Revenues” (Angelucci and Cagé 2019) and follows a difference-in-difference analysis and it also comes with replication material which means that we can try to emulate their results.\nIn 1967, the French government introduced advertisements on French TV programs. This led to a decrease in advertising revenues for newspapers all over France. The idea is therefore to understand if and how the introduction of ads on TV affected the ad revenues for newspapers, both local and national ones, in France. They use a difference-in-difference approach to estimate the effect of the introduction of advertising on the advertising revenues of newspapers. Thus, the treatment is…? Exactly, the introduction of ads on TV. Angelucci and Cagé argue that national newspapers were more affected by this change than local newspapers. They have many more hypotheses which they test in this paper, but for now we will focus on this one. As mentioned above, their data is available and we can use it to replicate their results. You need to sign up, to get it.\n\nnewspapers &lt;- read_dta(\"data/Angelucci_Cage_AEJMicro_dataset.dta\")\n\nNext we will have to do some minor data management. Fortunately enough, this replication material is already pretty clean and we do not have to mutate() or filter() our way around too much. The only thing we actually have to do, is to convert some variables into factors and to create a new variable which is the ratio of advertising revenue over circulation. Here I specify the argument across() again of the mutate() function and within it, I give a vector c() containing all the variables that should be transformed to factors.\nHere is a brief explanation of the variables in the dataset:\n\n\nyear: The year of the observation\n\nid_news: A unique identifier for each newspaper\n\nlocal: A binary indicator (0 or 1) representing whether a newspaper is local or national (1 = local, 0 = national)\n\nnational: A binary indicator (0 or 1) representing whether a newspaper is national or local (1 = national, 0 = local)\n\nra_cst: The advertising revenue of the newspaper (in constant (2014) euros)\n\nps_cst: The price of subscriptions for the newspaper. This variable measures how much the newspaper charges for its subscriptions, reflecting its pricing strategy and revenue from readers\n\nqtotal: The circulation of the newspaper. This variable measures the number of copies sold, reflecting the newspaper’s readership and revenue from readers\n\nafter_national: A binary indicator (0 or 1) representing the interaction between a newspaper’s national status and the post-television advertising era. Specifically, it marks the period after a newspaper has started national circulation and also corresponds to times after the introduction or significant increase of television advertising. This variable is designed to capture the combined effects of a newspaper reaching a national audience and the competitive or complementary impacts of television advertising on newspaper revenues or strategies\n\nra_cst_div_qtotal: A derived variable representing the advertising revenue per unit of circulation (ra_cst / qtotal). This variable is calculated to assess the efficiency or effectiveness of advertising revenue in relation to the newspaper’s circulation size\n\n\nnewspapers &lt;-\n  newspapers |&gt;\n  select(\n    year, id_news, after_national, local, national, ra_cst, ps_cst, qtotal\n    ) |&gt; \n  mutate(ra_cst_div_qtotal = ra_cst / qtotal, \n         after_national =  if_else(year &gt;= 1967, 1, 0),\n         across(c(id_news, local, national, after_national), as.factor),\n         year = as.integer(year)) |&gt; \n  rename()\n\nnewspapers\n\n# A tibble: 1,196 × 9\n    year id_news after_national local national    ra_cst ps_cst  qtotal\n   &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;          &lt;fct&gt; &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1  1960 1       0              1     0         52890272   2.29  94478.\n 2  1961 1       0              1     0         56601060   2.20  96289.\n 3  1962 1       0              1     0         64840752   2.13  97313.\n 4  1963 1       0              1     0         70582944   2.43 101068.\n 5  1964 1       0              1     0         74977888   2.35 102103.\n 6  1965 1       0              1     0         74438248   2.29 105169.\n 7  1966 1       0              1     0         81383000   2.31 126235.\n 8  1967 1       1              1     0         80263152   2.88 128667.\n 9  1968 1       1              1     0         87165704   3.45 131824.\n10  1969 1       1              1     0        102596384   3.28 132417.\n# ℹ 1,186 more rows\n# ℹ 1 more variable: ra_cst_div_qtotal &lt;dbl&gt;\n\n\n\n4.2.1 Inspecting your data\nOne of the first things you can do, is to plot your data points and see if something stands out. This might serve as a first indicator of anything that concerns the parallel trends assumption for example. The code below plots the development of advertising revenue for French newspapers in a given year. The panels are divided into local newspaper or national newspaper. Remember that our control group are local newspapers and the treatment group are the national ones. Essentially, we would expect that before the intervention, both groups should have parallel trajectories in their outcomes. This is the parallel trends assumption. If the parallel trends assumption holds, the difference between the treatment and control group should be constant over time. If the parallel trends assumption is violated, the DiD estimator will be biased.\n\nnewspapers |&gt;\n  mutate(type = if_else(local == 1, \"Local\", \"National\")) |&gt;\n  ggplot(aes(x = year, y = ra_cst)) +\n  geom_point(alpha = 0.5) +\n  scale_y_continuous(\n    labels = dollar_format(\n      prefix = \"$\",\n      suffix = \"M\",\n      scale = 0.000001)) +\n  labs(x = \"Year\", y = \"Advertising revenue\") +\n  facet_wrap(vars(type), nrow = 2) +\n  theme_minimal() +\n  geom_vline(xintercept = 1966.5, linetype = \"dashed\")\n\nWarning: Removed 144 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nIn the top panel/the local newspapers, the revenue data points are quite dense and cluster at regular intervals, suggesting that local newspapers had relatively stable advertising revenues year-to-year. There’s no obvious trend or shift in revenue around the dashed vertical line, which likely represents the year 1967, the year when television advertising was introduced in France.\nThe distribution of data points in the lower panel showing the observations of national newspapers is less dense compared to local newspapers, which could suggest greater variability in the advertising revenues of national newspapers. There appears to be a change around the dashed vertical line at 1967. 1 After this year, there seems to be a wider spread of data points, including some years with significantly lower advertising revenues compared to previous years. This could indicate that national newspapers were more impacted by the introduction of television advertising. Overall, the graph suggests that the introduction of television advertising in 1967 may have had a differential impact on local and national newspapers, with national newspapers possibly experiencing greater negative effects on their advertising revenue.\nHowever, just because some face validity seems to indicate that there is a difference in the treatment and control group, does not mean that it actually is in the data. We need to test this with a model that we will construct in the next section.\n\n4.2.2 Building the DiD model\nThis here is the regression formula for the DiD analysis, including the interaction term in which we specify the treatment. The treatment is the introduction of ads on TV and the interaction term is the product of the treatment and the national status of the newspaper\n\\[\n\\ln(\\mathrm{ra\\_cst}) = \\beta_0 + \\beta_1 \\mathrm{national} + \\beta_2 \\mathrm{after\\_national} + \\beta_3(\\mathrm{national} \\times \\mathrm{after\\_national}) + \\beta_4 \\mathrm{year} + \\alpha_i + \\epsilon\n\\] - ln(ra_cst): The natural logarithm of advertising revenue for a newspaper. Log transformation is often used in economic data to help normalize the distribution of skewed variables and to interpret the coefficients in terms of percentage changes\n\nnational: This is a dummy variable indicating whether a newspaper is national (1) or local (0). This variable distinguishes the treatment group from the control group.\nafternational: A dummy variable indicating the time period after the introduction of television advertising in 1967 (1 for years 1967 and later, 0 for earlier years). It captures the before-and-after comparison.\nnational*after_national: The interaction term between national and after_national. This term is crucial for DiD analysis as it estimates the differential effect of the introduction of television advertising on national newspapers compared to local newspapers over time\nyear: A continuous variable representing the year of observation. Including this allows controlling for linear time trends that affect all newspapers\nalpha: Newspaper fixed effects that control for all unobserved, time-invariant differences between newspapers\nepsilon: The error term\n\nThis is the code to specify exactly this. Note that in R national * after_national includes three variables due to the * operator. It specifies the main effect of national, the main effect of after_national and the interaction effect national:after_national, representing the differential impact of the post-television advertising period specifically on national newspapers relative to local ones. If we had only put in national:after_national, we would have only included the interaction term and would have had to add the main effects manually. This means that the formula log(ra_cst) ~ national*after_national + ... is shorthand for log(ra_cst) ~ national + after_national + national:after_national + ....\n\nnewspaper_did_model &lt;-\n  lm(log(ra_cst) ~ national * after_national + year + id_news,\n     data = newspapers)\n\nYou can see that this is as straightforward as what we have already done in Session 1. It is a simple lm() and an interaction effect; nothing more…\n\n4.2.3 Interpreting the DiD model results\nHere, I am showing you a different way of displaying models in R/Quarto. I am making use of the fact that my quartobook is rendered to html. I am using the modelsummary package to display the results of the model in a nice table. Similar to the stargazer package, you can change almost every aspect of the table. I am using the coef_omit argument to exclude the fixed effects of the individual newspapers from the table. This is because we are not interested in them and it would have given us a lot of coefficients. I am also using the coef_map argument to rename the coefficients in the table. This package is maybe a bit more advanced but also more versatile.\n\nmodel_coefs &lt;- c(\n  `national1` = \"National Newspapers\",\n  `after_national1` = \"Period After TV Ads\",\n  `national1:after_national1` = \"Interaction Effect (National * After TV Ads)\",\n  `year` = \"Year\"\n)\n\nmodelsummary(newspaper_did_model,\n             title = \"Difference-in-Differences Model Summary\",\n             # omit the fixed effects using a regular expression\n             coef_omit = \"id_news\\\\d+\",\n             stars = TRUE,\n             coef_map = model_coefs\n             )\n\n\nDifference-in-Differences Model Summary\n\n\n (1)\n\n\n\nNational Newspapers\n−1.039***\n\n\n\n(0.078)\n\n\nPeriod After TV Ads\n−0.001\n\n\n\n(0.022)\n\n\nInteraction Effect (National * After TV Ads)\n−0.228***\n\n\n\n(0.032)\n\n\nYear\n0.046***\n\n\n\n(0.003)\n\n\nNum.Obs.\n1052\n\n\nR2\n0.985\n\n\nR2 Adj.\n0.984\n\n\nAIC\n36169.6\n\n\nBIC\n36581.2\n\n\nLog.Lik.\n345.343\n\n\nRMSE\n0.17\n\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\nThis table reports the results from a difference-in-differences (DiD) regression model, which examines the impact of television advertising on the advertising revenues of national newspapers compared to local newspapers over time. The intercept we can disregard in this case. As a reminder, it is when all IVs are set to 0. It does not really make sense in our case, especially due to the year variable. I did drop the coefficients of all the fixed effects for the individual newspapers (which were in the factor variable of id_news) because we are not very interested in them and it would have given us an individual coefficient for each newspaper which would have been a lot of displayed coefficients. I used a regular expression (regex) for that. For now, you do not have to know what this is, but it is a very powerful tool to manipulate character strings. At some point, I will add this to the text-as-data tutorial or to the already existing webscraping script.\n\nNational Newspapers: The coefficient for national newspapers is -1.039 and is highly statistically significant (p &lt; 0.001). This suggests that, holding other factors constant, the log of advertising revenue for national newspapers is, on average, 1.039 units lower than for local newspapers. This could imply that national newspapers, on their own, tend to have lower advertising revenue in this model or that they were already at a disadvantage before the treatment period.\nPeriod After TV Ads: The coefficient for the period after the introduction of television advertisements is very small and not statistically significant (-0.001, p &gt; 0.1). This indicates that the introduction of television advertising, when not considering whether a newspaper is national or local, does not have a significant overall effect on advertising revenues.\nYear: The coefficient for the year variable is positive and significant (0.046, p &lt; 0.001), indicating that there is a general positive trend in advertising revenue over time across all newspapers in the sample.\nInteraction Effect (National * After TV Ads): This is the center piece of our model since we try to estimate the causal effect through this interaction term. Fortunately for us, it is significant and negative (-0.228, p &lt; 0.001). In DiD analyses, this interaction term captures the differential impact of the treatment effect (here, the introduction of television ads) on the treated group (national newspapers). The negative sign suggests that after television advertising started, national newspapers experienced a significant decrease in advertising revenue compared to local newspapers. This term essentially captures the essence of the DiD strategy by showing the relative effect post-treatment for the group of interest!\n\nSo what do we conclude now? Concerning our Difference-in-Difference causal design, the model supports the hypothesis that the introduction of television advertising had a different negative impact on national newspapers’ advertising revenues compared to local newspapers! The DiD model effectively isolates this effect by comparing the changes in revenues over time between the two groups (of course assuming parallel trends before the treatment).",
    "crumbs": [
      "Diff-in-Diff",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diff-in-Diff</span>"
    ]
  },
  {
    "objectID": "session4/session4.html#references",
    "href": "session4/session4.html#references",
    "title": "\n4  Diff-in-Diff\n",
    "section": "\n4.3 References",
    "text": "4.3 References\n\n\nAbou-Chadi, Tarik. 2016. “Niche Party Success and Mainstream Party\nPolicy Shifts – How Green and Radical Right Parties Differ in Their\nImpact.” British Journal of Political Science 46 (2):\n417–36. https://doi.org/10.1017/S0007123414000155.\n\n\nAdams, James, and Zeynep Somer-Topcu. 2009. “Policy Adjustment by\nParties in Response to Rival Parties’ Policy Shifts: Spatial Theory and\nthe Dynamics of Party Competition in Twenty-Five Post-War\nDemocracies.” British Journal of Political Science 39\n(4): 825–46. https://doi.org/10.1017/S0007123409000635.\n\n\nAngelucci, Charles, and Julia Cagé. 2019. “Newspapers in Times of\nLow Advertising Revenues.” American Economic Journal:\nMicroeconomics 11 (3): 319–64. https://doi.org/10.1257/mic.20170306.\n\n\nBeck, Nathaniel, and Jonathan N. Katz. 1995. “What to Do (and Not\nto Do) with Time-Series Cross-Section Data.” The American\nPolitical Science Review 89 (3): 634–47. https://doi.org/10.2307/2082979.\n\n\nBudge, Ian, and Dennis Farlie. 1983. Explaining and Predicting\nElections: Issue Effects and Party Strategies in Twenty-Three\nDemocracies. London ; Boston: Allen & Unwin.\n\n\nCarpenter, Christopher, and Carlos Dobkin. 2015. “The Minimum\nLegal Drinking Age and Crime.” The Review of Economics and\nStatistics 97 (2): 521–24. https://doi.org/10.1162/REST_a_00489.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New\nHaven ; London: Yale University Press.\n\n\nDinas, Elias, and Kostas Gemenis. 2010. “Measuring Parties’\nIdeological Positions with Manifesto Data: A Critical Evaluation of the\nCompeting Methods.” Party Politics 16 (4): 427–50. https://doi.org/10.1177/1354068809343107.\n\n\nGemenis, Kostas. 2013. “What to Do (and Not to Do) with the\nComparative Manifestos Project Data.” Political Studies\n61 (April): 23–43. https://doi.org/10.1111/1467-9248.12015.\n\n\nGreen, Jane, and Sara Hobolt. 2008. “Owning the Issue Agenda:\nParty Strategies and Vote Choices in British Elections.”\nElectoral Studies 27 (3): 460–76. https://doi.org/10.1016/j.electstud.2008.02.003.\n\n\nHuntington-Klein, Nick. n.d. The Effect: An Introduction to Research\nDesign and Causality  the Effect. Accessed April 10,\n2024. https://theeffectbook.net/.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New\nScience of Cause and Effect. 1st ed. New York: Basic Books.\n\n\nRuedin, Didier, and Laura Morales. 2019. “Estimating Party\nPositions on Immigration: Assessing the Reliability and Validity of\nDifferent Methods.” Party Politics 25 (3): 303–14. https://doi.org/10.1177/1354068817713122.",
    "crumbs": [
      "Diff-in-Diff",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diff-in-Diff</span>"
    ]
  },
  {
    "objectID": "session4/session4.html#footnotes",
    "href": "session4/session4.html#footnotes",
    "title": "\n4  Diff-in-Diff\n",
    "section": "",
    "text": "I am not going to lie, I was hoping for a but of a clearer shift here… As you can see the causal effect is there mathematically but visually this could have been a bit more evident. This however emphasizes the importance of not only relying on visual inspection but rather your estimation strategy and being clear about the assumptions you are making.↩︎",
    "crumbs": [
      "Diff-in-Diff",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Diff-in-Diff</span>"
    ]
  },
  {
    "objectID": "session5/session5.html",
    "href": "session5/session5.html",
    "title": "\n5  Regression Discontinuity Design & Instrumental Variables\n",
    "section": "",
    "text": "5.1 Causal Inference References\nI have mentioned some references on Causal Inference in class that I find extremely useful and that have been an inspiration to my scripts.\nThe big ones out there that have helped me a lot are:",
    "crumbs": [
      "Regression Discontinuity Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Discontinuity Design & Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "session5/session5.html#causal-inference-references",
    "href": "session5/session5.html#causal-inference-references",
    "title": "\n5  Regression Discontinuity Design & Instrumental Variables\n",
    "section": "",
    "text": "The Effect by Huntington-Klein (n.d.)\n\nThe Book of Why by Pearl and Mackenzie (2018)\n\nThe Causal Mixtape Cunningham (2021)",
    "crumbs": [
      "Regression Discontinuity Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Discontinuity Design & Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "session5/session5.html#regression-discontinuity-design",
    "href": "session5/session5.html#regression-discontinuity-design",
    "title": "\n5  Regression Discontinuity Design & Instrumental Variables\n",
    "section": "\n5.2 Regression Discontinuity Design",
    "text": "5.2 Regression Discontinuity Design\nThis session is all about Regression Discontinuity Design (RDD). RDDs are useful when we have a treatment that is assigned discontinuously creating a cutoff or a jump. It creates a divide within a sample at that said cutoff and introduces a treatment. On one side, we have people (observations) which have been treated and than after/before the cutoff, people that have not been treated. Most treatments are binary, i.e. can either be 0 or 1.\nThe variable that creates this discontinuity is called the running variable . It is sometimes also refered to as the forcing variable or forcing function. All these terms refer to the same thing however. The cutoff is the moment where the running variable introduces the treatment; for example:\n\nIf age is your running variable, turning 21 introduces the treatment of being allowed to drink\nBarely managing to win slightly more than 50% makes a candidate get into office\n\nA third term that we will have to keep in mind is the bandwidth. In plain English, it simply means how many observations to the left and to the right of the cutoff we are going to compare. We assume that most individuals/observations that are very close to the cutoff are almost the same or at least very similar and thus comparable. The further away we get from the cutoff point the more noise is introduced and the more other and different variables might explain our outcome variable. As we will see later, the choice of bandwidth is highly important for RDDs and must be theoretically and empirically thoroughly discussed. But when is that not the case for any statistical decision that we make?\nSo to recap: In RDDs, a running variable introduces a treatment on a continuous variable and thus creates a cutoff. The difference between the people left and right of the cutoff is therefore that one side has been treated wheras the other has not. It is important to note that this running variable is somewhat inevitable. It is hard to not turn 21 in your life assuming you make it until then. We all have turned 21 at some point.\n\n5.2.1 Once again, visualizing your data\nI have been preaching over and over again that it is very important that you first inspect your data and always visualize your data. RDD is actually a great use case in which visualizing the data at hand helps you identify potential discontinuities in your data.\nBelow I simulate both the running_varaible as well as our outcome to graphically highlight the idea of RDD. Further, the tibble also includes a binary variable that highlights whether the cutoff has taken place or not.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nsimulated_rdd_data_before_treatment &lt;- \n  tibble(\n    outcome = rnorm(200, mean = 1, sd = 1),\n    running_variable = c(1:200),\n    treatment = \"before treatment\"\n  ) \n\nsimulated_rdd_data_after_treatment &lt;- \n  tibble(outcome = rnorm(200, mean = 3, sd = 1),\n        running_variable = c(201:400),\n        treatment = \"after treatment\"\n  )\n\nrbind(simulated_rdd_data_before_treatment, \n      simulated_rdd_data_after_treatment) |&gt; \n  ggplot(aes(x = running_variable, y = outcome, color = treatment)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(formula = y ~ x, method = \"lm\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAbove you can easily see that there is a cutoff where running variable takes on value 201. This is only simulated data and we do not measure any specific variables yet. But visually speaking, this is something you should observe in your data. I do not want to recommend data mining in any way; it is bad practice and we should work on theoretically driven models. But if you observe such a pattern in your data, maybe there is something happening at the cutoff point that deserves our causal inference and thus RDD attention…?\n\n5.2.2 Sharp vs. fuzzy RDD\nThis below is a fuzzy RDD design! It is fuzzy because the running variable increases the probability of the treatment but does not guarantee it. This is in contrast to a sharp RDD where the running variable directly introduces the treatment. A sharp RD design occurs for variables like age for example. You can hardly escape turning 21 if you are 20 years and 364 days old. However, spending in a constituency is not a sharp RDD; it is fuzzy. It is fuzzy because the spending is not directly dependent on the vote share but rather on the vote share being above 50% but never directly at 50%. It only raises the probability of the treatment.\nPut in formal terms, a sharp RDD is happens when the treatment is deterministic and the rate jumps from 0% to 100% at the cutoff. We are working with a fuzzy RDD when the treatment at the cutoff is not from 0 to 1 (or 100%) but rather from 0 to some value between 0 and 1.\n\n5.2.2.1 Fuzzy RDD example\nBelow I simulate a fuzzy regression discontinuity design. I am simulating election results in 1000 constituencies. The vote share is randomly distributed around the 50% threshold. The government spending is increased if the vote share is above 50%. The spending is increased by a random noise to add variability. The idea behind this is that the government wants to reward constituencies where they have a majority but especially those where they have a very narrow majority.\n\nnum_constituencies &lt;- 1000\n\n# Simulating election results\nelection_data &lt;- tibble(\n  constituency = c(1:1000),\n  vote_share = runif(num_constituencies, min = 49, max = 51), # Vote share around the threshold\n  spending = rnorm(num_constituencies, mean = 100, sd = 20) # Base government spending\n)\n\n# Increase spending if vote share &gt;= 50 (majority threshold)\nelection_data &lt;- \n  election_data |&gt; \n  mutate(\n    noise = rnorm(n = num_constituencies, mean = 15, sd = 5), # Noise to add variability\n    spending = if_else(vote_share &gt;= 50, spending + noise, spending)\n  )\n\n# Visualizing the discontinuity\nelection_data |&gt; \n  ggplot(aes(x = vote_share, y = spending)) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(\n    data = filter(election_data, vote_share &lt; 50),\n    method = \"lm\",\n    color = \"blue\",\n    formula = y ~ poly(x, 2) # Polynomial regression for smoother line\n  ) +\n  geom_smooth(\n    data = filter(election_data, vote_share &gt;= 50),\n    method = \"lm\",\n    color = \"red\",\n    formula = y ~ poly(x, 2) # Polynomial regression for smoother line\n  ) +\n  theme_minimal() +\n  labs(x = \"Vote Share (%)\", y = \"Government Spending\")\n\n\n\n\n\n\n# Estimating the effect of crossing the threshold\nelection_data &lt;- \n  election_data |&gt; \n  mutate(majority = if_else(vote_share &lt; 50, 0, 1))\n\n\nmodel &lt;- lm(spending ~ vote_share + vote_share^2, data = election_data)\nsummary(model)\n\n\nCall:\nlm(formula = spending ~ vote_share + vote_share^2, data = election_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-59.800 -12.558   0.311  13.246  76.156 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -452.478     56.851  -7.959 4.69e-15 ***\nvote_share    11.223      1.137   9.874  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.28 on 998 degrees of freedom\nMultiple R-squared:  0.08899,   Adjusted R-squared:  0.08808 \nF-statistic: 97.49 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\nlibrary(rdrobust)\n\nrd_result &lt;-\n  rdrobust(y = election_data$spending,\n           x = election_data$vote_share,\n           c = 50, \n           h = 0.2)\n\nrd_result |&gt; summary()\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                      Manual\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  467          533\nEff. Number of Obs.              88          118\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.200        0.200\nBW bias (b)                   0.200        0.200\nrho (h/b)                     1.000        1.000\nUnique Obs.                     467          533\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     7.297     5.247     1.391     0.164    [-2.988 , 17.581]    \n        Robust         -         -     0.044     0.965   [-12.648 , 13.232]    \n=============================================================================\n\n\n\n5.2.2.1.1 Sharp RDD\nBelow, I am using an example by Rohan Alexander from his book Telling Stories with Data again. He reproduces a study by Carpenter and Dobkin (2015). The authors are interested in estimating the causal effect of access to alcohol on crime. They use the minimum legal drinking age (MLDA) as an instrument for access to alcohol. The idea is that the MLDA is a sharp running variable. You cannot escape turning 21 if you are 20 years and 364 days old. Their data is available here.\n\nlibrary(haven)\n\ncarpenter_dobkin &lt;-\n  read_dta(\n    \"data/carpenter_dobkin_replication.dta\"\n  )\n\ncarpenter_dobkin\n\n# A tibble: 2,922 × 143\n   days_to_21   all felony misdemeanor fbi_offense_miss violent murder\n        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1      -1461  5289   1556        3064              206     686      7\n 2      -1460  4800   1460        2756              196     581      8\n 3      -1459  4464   1356        2589              214     561      8\n 4      -1458  4425   1382        2511              180     587     15\n 5      -1457  4451   1423        2535              169     596      5\n 6      -1456  4501   1375        2593              176     587      9\n 7      -1455  4331   1341        2459              159     575     14\n 8      -1454  4432   1372        2568              179     599      6\n 9      -1453  4576   1401        2650              201     604      3\n10      -1452  4608   1486        2577              172     609     14\n# ℹ 2,912 more rows\n# ℹ 136 more variables: manslaughter &lt;dbl&gt;, rape &lt;dbl&gt;, robbery &lt;dbl&gt;,\n#   assault &lt;dbl&gt;, aggravated_assault &lt;dbl&gt;, ot_assault &lt;dbl&gt;, property &lt;dbl&gt;,\n#   burglary &lt;dbl&gt;, larceny &lt;dbl&gt;, mv_theft &lt;dbl&gt;,\n#   stolen_prop_buy_rec_poss &lt;dbl&gt;, vandalism &lt;dbl&gt;, ill_drugs &lt;dbl&gt;,\n#   cocaine_opio_sale_manuf &lt;dbl&gt;, mj_sale_manuf &lt;dbl&gt;,\n#   dang_non_narc_sale_manuf &lt;dbl&gt;, cocaine_opio_posses &lt;dbl&gt;, …\n\n\n\ncarpenter_dobkin_prepared &lt;-\n  carpenter_dobkin |&gt;\n  mutate(age = 21 + days_to_21 / 365) |&gt;\n  select(age, assault, aggravated_assault, dui, traffic_violations) |&gt;\n  pivot_longer(\n    cols = c(assault, aggravated_assault, dui, traffic_violations),\n    names_to = \"arrested_for\",\n    values_to = \"number\"\n  )\n\ncarpenter_dobkin_prepared |&gt;\n  mutate(\n    arrested_for =\n      case_when(\n        arrested_for == \"assault\" ~ \"Assault\",\n        arrested_for == \"aggravated_assault\" ~ \"Aggravated assault\",\n        arrested_for == \"dui\" ~ \"DUI\",\n        arrested_for == \"traffic_violations\" ~ \"Traffic violations\"\n      )\n  ) |&gt;\n  ggplot(aes(x = age, y = number)) +\n  geom_point(alpha = 0.05) +\n  facet_wrap(facets = vars(arrested_for), scales = \"free_y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\ncarpenter_dobkin_dui_only &lt;-\n  carpenter_dobkin_prepared |&gt;\n  filter(\n    arrested_for == \"dui\",\n    abs(age - 21) &lt; 2\n  ) |&gt;\n  mutate(is_21_or_more = if_else(age &lt; 21, 0, 1))\n\nBelow I am using the rdrobust() function from the rdrobust package to estimate the causal effect of the minimum legal drinking age on DUI arrests. The rdrobust() function requires the dependent variable y, the running variable x, the cutoff c, and the bandwidth h. The all argument is set to TRUE to include all robustness checks that you might want to do. Note that the choice of bandwidth is extremely important for the result and has to be empirically and theoretically grounded.\n\nlibrary(rdrobust)\nrdd_dui &lt;- rdrobust(\n  # specifies the DV\n  y = carpenter_dobkin_dui_only$number,\n  # specifies the running variable of the RDD\n  x = carpenter_dobkin_dui_only$age,\n  # specifies the cutoff\n  c = 21,\n  # specifies the bandwidth, here the unit of analysis is years\n  h = 2,\n  all = TRUE\n) \n\nrdd_dui |&gt; summary()\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1459\nBW type                      Manual\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  729          730\nEff. Number of Obs.             729          730\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   2.000        2.000\nBW bias (b)                   2.000        2.000\nrho (h/b)                     1.000        1.000\nUnique Obs.                     729          730\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional   192.790     6.221    30.989     0.000   [180.597 , 204.984]   \nBias-Corrected   207.423     6.221    33.341     0.000   [195.230 , 219.617]   \n        Robust   207.423    11.524    17.999     0.000   [184.837 , 230.010]   \n=============================================================================\n\n\nThe conventional estimate of the coefficient suggests that immediately upon crossing the age threshold of 21, there is an estimated increase of approximately 192.79 DUI arrests in the specified age bandwidth. This is a substantial effect, indicating a significant increase in DUI arrests that coincide with reaching the legal drinking age. The bias-corrected method provides a slightly higher estimate of the increase in DUI arrests, at about 207.42. This method adjusts for potential biases in the conventional estimate, potentially offering a more accurate depiction of the effect. The higher figure further emphasizes the impact of reaching the age threshold on DUI arrests. The robust estimate matches the bias-corrected estimate but accounts for potential heteroskedasticity or autocorrelation in the data, reflected in its larger standard error. This method aims to provide a more reliable estimate of the effect size, affirming the significant increase in DUI arrests but acknowledging greater uncertainty in the estimate. The robustness checks thus suggest that the effect is not sensitive to the choice of bandwidth, which is a good sign.\n\n5.2.2.1.2 Other robustness checks to consider\nTo assess if the estimated effect of crossing a specific threshold (e.g., the legal drinking age on DUI arrests) is robust and not sensitive to the choice of bandwidth in a Regression Discontinuity (RD) design, we can employ several robustness checks. These checks help ensure that the findings are not artifacts of particular specifications but reflect a genuine causal relationship. Here are some key strategies:\n\nVarying Polynomial Orders: The choice of polynomial order for fitting the regression on either side of the cutoff can influence the results. Testing the model with different polynomial orders and checking for consistency in the estimated effect size can help ensure that the results are not sensitive to this choice.\nVarying Bandwidths: Conduct the analysis using a range of bandwidths around the cutoff point. This involves re-estimating the model with different bandwidth sizes to observe if the estimated effect significantly changes. rdrobust() chooses the otpimal bandwidth automatically. However, this does not mean that we should blindly trust it! If the estimated effect remains stable across a variety of bandwidths, this suggests that the findings are robust to the choice of bandwidth.\nPlacebo Tests or Fake Cutoffs: Applying the same RD design to points away from the actual cutoff can serve as a placebo test. If significant effects are detected at these placebo cutoffs, it might suggest that the observed effect at the real cutoff could be due to other factors rather than the policy or intervention being studied.\n\n5.2.3 Named Entity Recognition (OPTIONAL!!!)\nThis is still work in Progress!\nThis is the first time we are really entering the realm of text-as-data methods. The web scraping script in the appendix is actually the first preliminary stage, but this is the first time we will really work with textual statistics. I must have mentioned a few times in class that this part of quantitative methods fascinates me the most and I use them on a daily basis. Unfortunately, I won’t have the time to give you a great introduction to it. With a bit of luck, I will write a script in the appendix as an introduction to text-as-data. If that doesn’t work out, you are more than welcome to come to the course Malo and I are organizing on Computational Social Sciences between the 3rd and 4th semester in January 2025!\nIn this tutorial I will introduce you to Named Entity Recognition (NER). NER is a task from the field of Natural Language Processing (NLP). It is about detecting entities in texts and classifying them according to certain categories. These entities can be places, organizations, persons/names, data or miscellaneous. Strictly speaking, there are two NLP tasks that NER can perform for us: firstly, detecting entities in texts and then, in a second step, classifying them according to predefined categories.\nAs is so often the case with NLP tasks, this method also works with pre-trained models. There are many NER models, but this script will deal with spaCy; simply because there is an R package for it. NER models are pretrained in the sense that they have seen a human annotated corpus of text before, where the entities have all been detected and classified by hand. In short, the model learns what to look for and what to classify and how once it has been found. These models can be refined later, but for this tutorial we will work with off-the-shelf models as provided by the developers.\n\n5.2.3.1 Installation of spaCy\nFirst, we need to install the spacyr package. This package is an R wrapper for the spaCy NLP library which is usually running in Python. And as a general disclaimer, this might be tricky step – after this, everything will be easy. To be able to do NER in R, we will have to do three things:\n\nInstall Python on your computer\nInstall the reticulate package in R\nInstall the spacyr package in R\n\nAs we need to have a Python environment running, we will have to install Python on your computer first. You can download Python here. Download the latest version of Python and install it. Next, you will have to install the reticulate library in R; here is an in-depth tutorial on how to do this. Here is the next thing, you might want to take a look at which is the spacyr library. You can find the documentation here. For a tutorial on how to use it (that does not necessarily go beyond the documentation), you can check out this blog post.\nIf you managed to install all three things, these lines of code should run smoothly:\n\nlibrary(reticulate)\nlibrary(spacyr)\n\n\n5.2.3.2 Installation of spaCy models\nAs I mentioned above, Named Entity Recognition relies on pre-trained models that have been trained on a human annotated corpus. These models are not part of the spacyr package, but you can download them from the spaCy website. The spacyr package provides a function to download these models. You can download the English model with the following code:\n\n5.2.3.3 What is a corpus?\nBefore we start, a few words about the corpus we will be working with. We will use the State of the Union corpus, which comes with the quanteda package. quanteda is one of the leading and most important packages in R when it comes to textual statistics. I won’t spend much time explaining the idea of corpora, but here are a few key points:\nTo use any TaDa methods, we must construct a corpus. You could compare it to the sampling we do from a population of interest. Generally speaking your corpus can be anything as long as it makes sense (very broad statement, I know) but it should at least be useful depending on your dependent variable or guiding research question. A corpus is never value free. This is a key-point. The written text of a society is a certain reflection of society itself. This means that marginalized groups of any given society are also very likely to be marginalized in text. Textual data captures societal structure (structuralism and so on…). We must care about ethics! A corpus must be validated (and validated, and validated again). Ideally we should have the entirety of a corpus. In our example, we are looking at the State of the Union addresses. This is a very specific corpus. We could also look at all speeches by a certain president, all speeches by a certain party, all speeches of a certain type (inaugural addresses, farewell addresses, etc.). But you should definitely justify why we are looking at this specific corpus, this specific set of speeches. Exhaustiveness might be a good rule of thumb. Why stop at only looking at the SOTU from the 1970s on?\n\n5.2.3.4 How to construct a corpus?\nWe can always do things by hand…\n… or we automate it ;) For this I recommend that you go check out the appendix chapter on webscraping but let me tell you that the beautiful Internet is full of corpora which are waiting to be retrieved (scraped), cleaned, and used for text as data! Some might already be available. Party manifestos can be retroengineered from the Comparative Manifesto Project. The State of the Union corpus is available within the quanteda package. Plenty of parliamentary speeches can be found here.\nBut you can get as creative as you want! You can scrape entire newspaper archives or other digital trace data from the web. Or let’s say you are interested in lyrics of songs. Why not scrape https://genius.com/? Any text that is available on the web can be scraped and used as a corpus!\n\n5.2.3.5 United Nations corpus\nLoad both the reticulate and the spacyr library to your environment.\n\nlibrary(spacyr)\nlibrary(reticulate)\n\nNext, we are going to initialize the spaCy session. R will go look for a python environment installed on your computer that it can use to do our NER. You need to specify which spaCy model you would like to use in your session.\n\nspacy_initialize(model = \"en_core_web_lg\")\n\nIt reads as follows: en_ indicates that we are working with a model trained on English text, and core_ means that it could technically detect more than just entities, web_ means that it was trained on web text, and lg means that it is a large model. Other available models in English are en_core_web_sm (small model) and en_core_web_md (medium model). The bigger the model, the better the accuracy of the NER. 1 If you want to use a different model, you can check out this list here of the official spaCy website and see whether your language of interest is available. 2\nYou always have to finalize your spacy session at the end!\n\nspacy_finalize()",
    "crumbs": [
      "Regression Discontinuity Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Discontinuity Design & Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "session5/session5.html#references",
    "href": "session5/session5.html#references",
    "title": "\n5  Regression Discontinuity Design & Instrumental Variables\n",
    "section": "\n5.3 References",
    "text": "5.3 References\n\n\nCarpenter, Christopher, and Carlos Dobkin. 2015. “The Minimum Legal Drinking Age and Crime.” The Review of Economics and Statistics 97 (2): 521–24. https://doi.org/10.1162/REST_a_00489.\n\n\nCunningham, Scott. 2021. Causal Inference: The Mixtape. New Haven ; London: Yale University Press.\n\n\nHuntington-Klein, Nick. n.d. The Effect: An Introduction to Research Design and Causality  the Effect. Accessed April 10, 2024. https://theeffectbook.net/.\n\n\nPearl, Judea, and Dana Mackenzie. 2018. The Book of Why: The New Science of Cause and Effect. 1st ed. New York: Basic Books.",
    "crumbs": [
      "Regression Discontinuity Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Discontinuity Design & Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "session5/session5.html#footnotes",
    "href": "session5/session5.html#footnotes",
    "title": "\n5  Regression Discontinuity Design & Instrumental Variables\n",
    "section": "",
    "text": "There is technically also a en_core_web_trf model which is a so-called transformer model. This model is not available in the spacyr package and we would have to use it in Python. In my experience so far, the large model suffices for most purposes!↩︎\nOtherwise, you can always try to find a model on huggingface.co. In that case, you will have to code in Python though.↩︎",
    "crumbs": [
      "Regression Discontinuity Design",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Regression Discontinuity Design & Instrumental Variables</span>"
    ]
  },
  {
    "objectID": "session6/session6.html",
    "href": "session6/session6.html",
    "title": "\n6  Time Series\n",
    "section": "",
    "text": "6.1 Introduction\nThis week’s R session will introduce you to the logic of time series cross section in RStudio. Be aware that just because it sounds very impressive does not mean that we need to be in awe. It is rather straight forward and once you can wrap your head around it, you will see that – as always – the actual coding part is not that complicated (except but… we still have to manage our data).\nThis week’s session will also introduce you to a dataset which you have not seen: the Comparative Manifesto Project (CMP) or also referred to as Manifesto Research on Political Representation. If you are only interested in party positions based on their manifestos, and thus only in intervals of 4-5 years, it is a great source for everything that has to do with party positions and political competition over time, within party families or within several countries – or everything all at once.\nThe idea of the CMP is that individual human-coders split the party manifestos for each party at each election into quasi-sentences and attribute it to a certain topic. The topics are predefined and go back to the saliency theory of Budge and Farlie (1983). In their pre-definition, they are also attributed to either left-wing politics or right-wing politics. The idea of the saliency theory is that parties will only emphasize those issues which are favorable to them. Therefore, we can place parties in a (one- or twodimensional) space. Examining these estimates of party positions over time can give us insights on party competition.\nI could spend hours talking about how to model party competition. But according to Jan, I spend way too much of my time thinking about models and operationalizations,1 which is why I will not bother you with my thoughts on the CMP and its advantages or inconveniences. Let me say this at least: the CMP is a powerful and widely used database; it relies, however, on very very strong assumptions, which can be problematic (imho). If you are interested in party competition and party positions, feel free to contact me or come see me after class. Also, here are some references, which you might want to consult to check out scientific publications using the CMP or articles evaluating the validity of the CMP’s measures; for different uses of the CMP: Green and Hobolt (2008) or Adams and Somer-Topcu (2009); for critical evaluations of the CMP: Dinas and Gemenis (2010), Gemenis (2013), Ruedin and Morales (2019).",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#introduction",
    "href": "session6/session6.html#introduction",
    "title": "\n6  Time Series\n",
    "section": "",
    "text": "I would also like to emphasize that this week’s code is inspired by a publication by Abou-Chadi (2016). I have not had the time to come up with a witty coding idea myself and the paper by Abou-Chadi came with a replication dataset. The TSCS model with which we are going to play around with does not follow the exact same method but is loosely based on his paper and generates somewhat close results. The purpose of this script is to expose you to the logic of ARIMA models, TSCS, especially to fixed and random effects models.\n\n6.1.1 Some Thoughts on Time Series in general…\nNow, you might be wondering if you are ever going to use these seemingly incomprehensible formulas, equations and terms like Error Correction Model, lagged dependent variable or structural equation approach. The boring answer is that this is up to you. It was up to me too at one point. Remember that I took this class too, had my mind made up about a multinomal model I wanted to do and was pretty sure that I would never dig deeper into time series. Little did I know that your research interests sometimes force you to learn methods you thought you would never need. This happened to me a couple months later, when at the beginning of my third semester I wanted to work on parties and the evolution of party positions in response to other party’s shifts. Jan suggested that I would have to look at time series at some point. I did and I have come to hate and to like them. I did not say love them, I like them. They are still quite awful in terms of math and push me to the boundaries of what I can grasp…\nTime series are one of the best statistical methods to understand social and many other change(s) over time. They allow you to understand causal relationships between many kinds of variables over time and most importantly the drivers of that relationship or change across time. There is a huge abundance of questions that relate to changes over time that we are interested in explaining. It does not have to be limited to my boring party competition stuff: what about crime and approval ratings, and then trying to see whether there is a statistical relationship between increases in crime and decreases in approval ratings.\n\n6.1.2 … and more specifically on Time Series in R\nThis is now the 5th session and you might still believe me that R can be fun, and that it can be easy in some cases. The logic and math behind time series analysis can be tricky, I am aware of that. But you will see that many things are taken care of by packages like the plm or tseries packages for panel data analysis (Whatever that is for now…) or general time series analyses. Some things we still need to do ourselves, especially the specification of our ARIMA models which you have seen in Jan’s class. I will try to further give you some help on that as well as some other tricks!",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#what-is-time-series-cross-section-tscs",
    "href": "session6/session6.html#what-is-time-series-cross-section-tscs",
    "title": "\n6  Time Series\n",
    "section": "\n6.2 What is Time Series Cross Section (TSCS)?",
    "text": "6.2 What is Time Series Cross Section (TSCS)?\nI suppose you have seen this with Jan, so I am only going to briefly recapitulate the idea of TSCS. Generally speaking, we have two things going on with which we can have a lot of data fun. First, we have observations for more than one unit (e.g., countries, parties) and for more than one point in time. If you wish to work with TSCS models, you first need to find a dataset that has enough observations over time and in more than one unit. As a rough guesstimation, I would say 20 observations at least as the bare minimum, ideally many more. Unfortunately, this is where data availability can be an issue 2 In the next paragraphs, I will try to show you what we need to understand to conduct TSCS analyses in R. For all the tricky math stuff, please refer to Jan’s lecture which gives a much better and thorough overview on everything that is going on underneath the hood of TSCS and its assumptions. If you have any questions on this, he is the better reference than me but I can try my best in answering your questions.\n\n6.2.1 Data Structure\nAs we will work with the CMP, I know that we have enough observations to have time series. However, we must make sure that the structure is according to time series across units. The data structure (and you might have seen this table in Jan’s document) should look somewhat like this:\n\n\nUnit\nTime\n\n\n\n1\n1\n\n\n1\n2\n\n\n1\n3\n\n\n1\n…\n\n\n1\nT\n\n\n2\n1\n\n\n2\n2\n\n\n2\n3\n\n\n2\n…\n\n\n2\nT\n\n\n…\n…\n\n\nN\n1\n\n\nN\n…\n\n\nN\nT\n\n\n\nHere we have a structure that allows us to do TSCS models. The data is structured in a way that it follows the order of unit per year.",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#the-comparative-manifesto-project",
    "href": "session6/session6.html#the-comparative-manifesto-project",
    "title": "\n6  Time Series\n",
    "section": "\n6.3 The Comparative Manifesto Project",
    "text": "6.3 The Comparative Manifesto Project\nLet’s import the CMP dataset. As you can see country 41 corresponds to Germany. I have imported some variables of the CMP as well but for now, we will simply look at the variable rile. It’s the CMP’s special measure of an aggregated left-right position of parties based on the ideas of the saliency theory by Budge and Farlie (1983).\n\nlibrary(tidyverse)\n\n\n# for the download of the dataset, please click on the link for\n# of this script the CMP in this script\ncmp &lt;- read_csv(\"data/cmp.csv\") |&gt;\n  # I only import the variables I am interested in; check the codebook first\n  # ALWAYS\n  select(country,\n         partyname,\n         date,\n         rile,\n         per602_2,\n         per601_2,\n         per607,\n         per608) |&gt;\n  # this is an old code I have used; 41 = Germany (check out the codebook)\n  filter(country == 41) |&gt;\n  # here I align the names as there are some inconsistencies over the years\n  mutate(\n    party = case_when(\n      partyname == \"Alliance‘90/Greens\" ~ \"Grüne\",\n      partyname == \"Greens/Alliance‘90\" ~ \"Grüne\",\n      partyname == \"Alternative for Germany\" ~ \"AFD\",\n      partyname == \"Christian Democratic Union/Christian Social Union\" ~ \"CDU/CSU\",\n      partyname == \"Free Democratic Party\" ~ \"FDP\",\n      partyname == \"Party of Democratic Socialism\" ~ \"Linke\",\n      partyname == \"Social Democratic Party of Germany\" ~ \"SPD\",\n      partyname == \"The Left\" ~ \"Linke\",\n      TRUE ~ NA_character_\n    ),\n    year = str_sub(date, 1, 4)\n  )\n\nRows: 4778 Columns: 174\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (7): countryname, edate, partyname, partyabbrev, corpusversion, datase...\ndbl (167): country, oecdmember, eumember, date, party, parfam, coderid, manu...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n6.3.1 Quick excursion: the stringr package\nIn the last line of the code, I create a new variable called year. Since the CMP is based on party manifestos, the initial date variable is estimated for the respective election day. But we are only interested in the year that the election took place. The function I use, str_sub() is from the stringr package which is one of the most useful ones you can find in R. We will talk about it again in the next session when we will have to transform textual data in R. What this function does is that it will take the first the fourth character of the variable and substract/delete all the others. I do this because the intital date variable follows the pattern of “yyyy/mm/dd”. And as I only want to have the “yyyy” part, I take the first four characters (or in our case numbers). If I wanted create a variable yearmonth for example, I would have specified str_sub(date, 1, 6). By the way, if you ever want to indicate in R that you want something until the value of something, you can do so by specifying -1. In our case, let’s say I only wanted to get the month and the day of the date variable (which would be the 5th to 8th character), I could specify it as follows: str_sub(date, 5, -1).\nEnough theoretical talk about functions. Let’s look at the left-right estimations of the CMP per party:\n\nggplot(cmp, aes(x = year, y = rile, color = party)) +\n  geom_point(alpha = 0.3) +\n  stat_smooth(aes(group = party), method = \"loess\", se = FALSE) +\n  scale_color_manual(values = c(\n    \"#20aefa\",\n    \"#000000\",\n    \"#FFFF00\",\n    \"#00e81b\",\n    \"#f26dd5\",\n    \"#fa2024\"\n  )) +\n  # without this line, the lables of the x-axis would overlap\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +\n  labs(\n    title = \"Left-Right Position of German Parties\",\n    x = \"Year\",\n    y = \"Left-Right Position\"\n  )\n\n\n\n\n\n\n\nThis gives us the smoothed means for every party over the span of 1949 until 2021. We should start with a . This means that we will look at the evolution of one series over time; in our case, this specifically means that we will look at the evolution of the left-right position of over time. Let’s start with the Christian-Democrats, the CDU, and the Social-Democrats, the SPD. I will quickly create an object, only containing values of the CDU and another one for only the SPD.\n\ncdu &lt;- cmp |&gt;\n  select(party, rile, year) |&gt;\n  filter(party == \"CDU/CSU\")\n\nspd &lt;- cmp |&gt;\n  select(party, rile, year) |&gt;\n  filter(party == \"SPD\")\n\nWe are, however, not interested in the mean position over time but the inidividual values over time. The first thing you should always do, when working with time series data, is to plot your variable of interest. It’s like looking at a data set when you first import it. Sometimes you can already detect things like seasonality that we will test for either way:\n\ncdu |&gt;\n  ggplot(aes(year, rile)) +\n  geom_line(aes(group = party)) +\n  geom_point(alpha = 0.5,\n             shape = 20,\n             size = 2) +\n  labs(title = \"RILE CDU/CSU over time\",\n       x = \"Year\",\n       y = \"Right to Left\") +\n  theme_minimal()",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series</span>"
    ]
  },
  {
    "objectID": "session6/session6.html#footnotes",
    "href": "session6/session6.html#footnotes",
    "title": "\n6  Time Series\n",
    "section": "",
    "text": "He is absolutely right in reminding me to work on my theory :(↩︎\nUntil we see many more ways to get to data in the next session, when we speak about quantitative text analysis.↩︎\nIf you are wondering why we are using multiculturalism and not immigration, you’re raising a good question. It is not the authors fault but problems of the CMP measures. But as I said, I will not go into too much detail on my criticism of the CMP’s measures.↩︎",
    "crumbs": [
      "Time Series",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Time Series</span>"
    ]
  }
]