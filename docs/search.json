[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced RStudio Labsessions",
    "section": "",
    "text": "Course Overview\nThis repository contains all the course material for the RStudio Labsessions for the Spring semester 2024 at the School of Research at SciencesPo Paris. The class follows Brenda van Coppenolle’s and Jan Rovny’s lecture on Quantitative Methods II. Furthermore, the RStudio part of the course is a direct continuation of Malo Jan’s RStudio introduction course. If you feel the need to go back to some basics of general R use, data management or visualization, feel free to check out his course’s website. Rest assured, however, that 1) we will recap plenty of things, 2) make slow but steady progress, 3) and come back to the essentials of data wrangling again during the semester while building statistical models.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Structure",
    "text": "Course Structure\nIn total we will see each other 6 times. The lessons will be structured in such a way that I will first present something to you and explain my script. Ideally, you will then start coding in groups of 2 and work on exercises related to the topic. You can find more information about the exercises in the subsection “course validation”. I will of course be there to help you. The rest you solve at home and send me your final script. At the beginning of each next meeting we will go through the solutions together. Also, I upload my own script before each session, so you can use it as a template when solving the tasks and also later, when the course is over, as a template for further coding (if you like of course…).\n\n\n\n\n\n\n\n\nSession\nDescription\nDates\n\n\n\n\nSession 1\nRStudio Recap & OLS\n01/02 & 08/02\n\n\nSession 2\nLogistic Regressions\n15/02 & 29/02\n\n\nSession 3\nMultinomial Regression\n07/03 & 14/03\n\n\nSession 4\nCausal Inference I\n21/03 & 28/03\n\n\nSession 5\nCausal Inference II\n04/04 & 11/04\n\n\nSession 6\nTime Series\n18/04 & 25/04",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-validation",
    "href": "index.html#course-validation",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Validation",
    "text": "Course Validation\nIn the two weeks between each lecture, you will be given exercises to upload to the designated link for each session. The document where you write the solutions must be written in Markdown format.\nI will grade your solutions to my exercises on a 0 to 5 scale. I would like to see that you have done something and hopefully finished the exercise. If you are unable to finish the exercise, it is no problem and I do understand that not everybody feels as comfortable with R as some other people might do. Handing something in is key to getting points! This class can be finished by everyone and I do not want you to worry about your grade too much. But I would like that you all at least try to solve the exercises! Work in groups of two and try to hand in something after each session. The precise deadline will be communicated in class, the course’s GitHub page and on the Moodle page.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#optional-course-parts",
    "href": "index.html#optional-course-parts",
    "title": "Advanced RStudio Labsessions",
    "section": "Optional Course Parts",
    "text": "Optional Course Parts\nWhen I taught the course last year, some students approached me and asked for several levels of difficulty. I will try to implement this in the homework and in class. I have also decided to add an optional part to each session. In the optional parts, I will introduce new packages, advanced methods, and I will also upload a few scripts in the appendix on things like text-as-data, webscraping or similar, if I have the time. Also – again, if the times allows it – I will go through these optional parts in class. But please be assured that if you decide not to follow the optional parts, that is okay. But if you do, I can promise you will make better and faster progress. Lastly, if you are interested in certain things, want to learn about specific methods or how to implement things or workflows in RStudio, please do not hesitate to contact me and I will see if I can squeeze it in somewhere.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Advanced RStudio Labsessions",
    "section": "Requirements",
    "text": "Requirements\nYou must have downloaded R and RStudio by the beginning of the course (you need to install both!) before our sessions. Please let me know if you encounter any problems during the installation. Here is a quick guide on how to do that: https://rstudio-education.github.io/hopr/starting.html\nR and RStudio are both free and open source. You need both of them installed in order to operate with the R coding language.\nFor R, go on the CRAN website and download the file for your respective operating system: https://cran.r-project.org/ For RStudio, you need to do the same thing by clicking on this link: https://posit.co/products/open-source/rstudio/ RStudio has received a new name recently (“posit”) but you will still find all the necessary steps behind this link under the name of RStudio.\nOtherwise, there are few prerequisites except that you must bring your computer to the sessions with the required programs installed. I will provide you with datasets in each case and I will explain everything else in the course.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#help-and-office-hours",
    "href": "index.html#help-and-office-hours",
    "title": "Advanced RStudio Labsessions",
    "section": "Help and Office Hours",
    "text": "Help and Office Hours\nThere are unfortunately no regular office hours. But please do not hesitate to reach out, if you have any concerns, questions or feedback for me! My inbox is always open. I tend to reply quickly but in the case that I have not replied in under 48h, simply send the email again. I will not be offended!\nLearning how to code and working with RStudio can be a struggle and a tough task. I have started out once like you and I will try to keep that in mind. Feel free to always ask questions in class or if you see me on campus. The most important thing, however, is that you try!",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Advanced RStudio Labsessions",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis is only a quick section to give credit, where credit is due! Malo Jan is one of my daily inspirations for anything that has to do with research and RStudio. I have taught this class already last year and had most of my scripts written in PDFs but Rohan Alexander’s book Telling Stories with Data served as a new inspiration to write this course in its Quarto book format. Also shout outs to Felix Lennert and some of his ideas for the homework.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "1.1 Introduction\nThis is a short recap of things you have seen last year and will need this year as well. It will refresh your understanding of the linear regression method called ordinary least squares (OLS). This script is supposed to serve as a cheat sheet for you to which you can always come back to.\nThese are the main points of today’s session and script:\nThese are the packages we will be using in this session:\nneeds(\n  tidyverse,\n  rio, \n  stargazer,\n  broom,\n)",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#introduction",
    "href": "session1/session1.html#introduction",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "A refresher of Ordinary Least Squares (OLS)\nWhat is Base R and what are packages?\nA recap of basic coding in R\nBuilding & Interpreting a simple linear model\nVisualizing Residuals\nThe broom package (OPTIONAL!)",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#ordinary-least-squares-ols",
    "href": "session1/session1.html#ordinary-least-squares-ols",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.2 Ordinary Least Squares (OLS)",
    "text": "1.2 Ordinary Least Squares (OLS)\nOLS regressions are the powerhouse of statistics. The world must have been a dark place without them. They are the most basic form of linear regression and are used to predict the value of a dependent variable (DV) based on the value of independent variables (IVs). It is important to note that the relationship between the DV and the IVs is assumed to be linear.\nAs a quick reminder, this is the formula for a basic linear model: \\(\\widehat{Y} = \\widehat{\\alpha} + \\widehat{\\beta} X\\).\nOLS is a certain kind of method of linear model in which we choose the line which has the least prediction errors. This means that it is the best way to fit a line through all the residuals with the least errors. It minimizes the sum of the squared prediction errors \\(\\text{SSE} = \\sum_{i=1}^{n} \\widehat{\\epsilon}_i^2\\)\nFive main assumptions have to be met to allow us to construct an OLS model:\n\nLinearity: Linear relationship between IVs and DVs\nNo endogeneity between \\(y\\) and \\(x\\)\n\nErrors are normally distributed\nHomoscedasticity (variance of errors is constant)\nNo multicolinearity (no linear relationship between the independent variables)\n\nFor this example, I will be working with some test scores of a midterm and a final exam which I once had to work through. We are trying to see if there is a relationship between the score in the midterm and the grade of the final exam. Theoretically speaking, we would expect most of the students who did well on the first exam to also get a decent grade on the second exam. If our model indicates a statistical significance between the independent and the dependent variable and a positive coefficient of the former on the latter, this theoretical idea then holds true.",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#coding-recap",
    "href": "session1/session1.html#coding-recap",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.3 Coding Recap",
    "text": "1.3 Coding Recap\nBefore we start, let’s refresh our coding basics again. RStudio works with packages and libraries. There is something called Base R, which is the basic infrastructure that R always comes with when you install it. The R coding language has a vibrant community of contributors who have written their own packages and libraries which you can install and use. As Malo, I am of the tidyverse school and mostly code with this package or in its style when I am wrangling with data, changing its format or values and so on. Here and there, I will, however, try to provide you with code that uses Base R or other packages. In coding, there are many ways to achieve the same goal – and I will probably be repeating this throughout the semester – and we always strive for the fastest or most automated way. I will not force the tidyverse environment on you but I do think that it is one of the most elegant and fastest way of doing statistics in R. It is sort of my RStudio dialect but you can obviously stick to yours if you have found it. Also, as long as you find a way that works for you, that is fine with me!\nTo load the packages, we are going to need:\n\nlibrary(tidyverse)\n\nNext we will import the dataset of grades.\n\ndata &lt;- read_csv(\"course_grades.csv\")\n\nRows: 200 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): midterm|final_exam|final_grade|var1|var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe path which I specify in the read_csv file is short as this quarto document has the same working directory to which the data set is also saved. If you, for example, have your dataset on your computer’s desktop, you can access it via some code like this one:\n\ndata &lt;- read_csv(\"~/Desktop/course_grades.csv\")\n\nOr if it is within a folder on your desktop:\n\ndata &lt;- read_csv(\"~/Desktop/folder/course_grades.csv\")\n\n\n\n\n\n\n\nImportant\n\n\n\nI will be only working within .Rproj files and so should you. 1 This is the only way to ensure that your working directory is always the same and that you do not have to change the path to your data set every time you open a new RStudio session. Further, this is the only way to make sure that other collaborators can easily open your project and work with it as well. Simply zip the file folder in which you have your code and\n\n\nYou can also import a dataset directly from the internet. Several ways are possible that all lead to the same end result:\n\ndataset_from_internet_1 &lt;- read_csv(\"https://www.chesdata.eu/s/1999-2019_CHES_dataset_meansv3.csv\")\n  \n# this method uses the rio package\nlibrary(rio)\ndataset_from_internet_2 &lt;- import(\"https://jan-rovny.squarespace.com/s/ESS_FR.dta\")\n\nLet’s take a first look at the data which we just imported:\n\n# tidyverse\nglimpse(data)\n\nRows: 200\nColumns: 1\n$ `midterm|final_exam|final_grade|var1|var2` &lt;chr&gt; \"17.4990613754243|15.641013…\n\n# Base R\nstr(data)\n\nspc_tbl_ [200 × 1] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ midterm|final_exam|final_grade|var1|var2: chr [1:200] \"17.4990613754243|15.64101334897|17.63|NA|NA\" \"17.7446326301825|18.7744366510731|14.14|NA|NA\" \"13.9316618079058|14.9978584022336|18.2|NA|NA\" \"10.7068243984724|11.9479428399047|19.85|NA|NA\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `midterm|final_exam|final_grade|var1|var2` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nSomething does not look right, this happens quite frequently when saving a csv file. It stands for comma separated value. R is having trouble reading this file since I have saved all grades with commas instead of points. Thus, we need to use the read_delim() function. Sometimes the read_csv2() function also does the trick. You’d be surprised by how often you encounter this problem. This is simply to raise your awareness to it!\nThe read_delim() function is the overall function of the readr package to read any sort of data file, whereas read_csv() and read_csv2() are specific functions to read csv files. The read_delim() function has a delim argument which you can use to specify the delimiter of your data file. For the sake of the example, I had purposefully saved the csv file using the | delimiter.\n\ndata &lt;- read_delim(\"course_grades.csv\", delim = \"|\")\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"|\"\ndbl (3): midterm, final_exam, final_grade\nlgl (2): var1, var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data)\n\nRows: 200\nColumns: 5\n$ midterm     &lt;dbl&gt; 17.499061, 17.744633, 13.931662, 10.706824, 17.118799, 17.…\n$ final_exam  &lt;dbl&gt; 15.641013, 18.774437, 14.997858, 11.947943, 15.694728, 17.…\n$ final_grade &lt;dbl&gt; 17.63, 14.14, 18.20, 19.85, 14.67, 20.26, 16.90, 13.40, 12…\n$ var1        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ var2        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nThis time, it has been properly imported. But by looking closer at it, we can see that there are two columns in the data frame that are empty and do not even have a name. We need to get rid of these first. Here are several ways of doing this. In coding, many ways lead to the same goal. In R, some come with a specific package, some use Base R. It is up to you to develop your way of doing things.\n\n# This is how you could do it in Base R\ndata &lt;- data[, -c(4, 5)]\n\n# Using the select() function of the dplyr package you can drop the fourth\n# and fifth columns by their position using the - operator and the -c() to\n# remove multiple columns\ndata &lt;- data  |&gt;  select(-c(4, 5))\n\n# I have stored the mutated data set in the old object; \n# you can also just transform the object itself...\ndata |&gt; select(-c(4, 5))\n\n# ... or create a new one\ndata_2 &lt;- data |&gt; select(-c(4, 5))\n\nNow that we have set up our data frame, we can build our OLS model. For that, we can simply use the lm() function that comes with Base R, it is built into R so to speak. In this function, we specify the data and then construct the model by using the tilde (~) between the dependent variable and the independent variable(s). Store your model in an object which can later be subject to further treatment and analysis.\n\nmodel &lt;- lm(final_exam ~ midterm, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = final_exam ~ midterm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6092 -0.8411 -0.0585  0.8712  3.3086 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.62482    0.73212   6.317 1.72e-09 ***\nmidterm      0.69027    0.04819  14.325  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.34 on 198 degrees of freedom\nMultiple R-squared:  0.5089,    Adjusted R-squared:  0.5064 \nF-statistic: 205.2 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nSince the summary() function only shows us something in our console and the output is not very pretty, I encourage you to use the broom package for a nicer regression table.\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.62     0.732       6.32 1.72e- 9\n2 midterm        0.690    0.0482     14.3  2.10e-32\n\n\nYou can also use the stargazer package in order to export your tables to text or LaTeX format which you can then copy to your documents.\n\nlibrary(stargazer)\nstargazer(model, type = \"text\", out = \"latex\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                            final_exam         \n-----------------------------------------------\nmidterm                      0.690***          \n                              (0.048)          \n                                               \nConstant                     4.625***          \n                              (0.732)          \n                                               \n-----------------------------------------------\nObservations                    200            \nR2                             0.509           \nAdjusted R2                    0.506           \nResidual Std. Error      1.340 (df = 198)      \nF Statistic          205.196*** (df = 1; 198)  \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#interpretation-of-ols-results",
    "href": "session1/session1.html#interpretation-of-ols-results",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.4 Interpretation of OLS Results",
    "text": "1.4 Interpretation of OLS Results\nHow do we interpret this?\n\n\nR2: Imagine you’re trying to draw a line that best fits a bunch of dots (data points) on a graph. The R-squared value is a way to measure how well that line fits the dots. It’s a number between 0 and 1, where 0 means the line doesn’t fit the dots at all and 1 means the line fits the dots perfectly. R-squared tells us how much of the variation in the dependent variable is explained by the variation in the predictor variables.\n\nAdjusted R2: Adjusted R-squared is the same thing as R-squared, but it adjusts for how many predictor variables you have. It’s like a better indicator of how well the line fits the dots compared to how many dots you’re trying to fit the line to. It always adjusts the R-squared value to be a bit lower so you always want your adjusted R-squared value to be as high as possible.\n\nResidual Std. Error: The residual standard error is a way to measure the average distance between the line you’ve drawn (your model’s predictions) and the actual data points. It’s like measuring how far off the line is from the actual dots on the graph. Another way to think about this is like a test where you want to get as many answers correct as possible and if you are off by a lot in your answers, the residual standard error would be high, but if you are only off by a little, the residual standard error would be low. So in summary, lower residual standard error is better, as it means that the model is making predictions that are closer to the true values in the data.\n\nF Statistics: The F-statistic is like a test score that tells you how well your model is doing compared to a really simple model. It’s a way to check if the model you’ve built is any better than just guessing. A large F-statistic means that your model is doing much better than just guessing.",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#visualizing-the-regression-line",
    "href": "session1/session1.html#visualizing-the-regression-line",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.5 Visualizing the Regression Line",
    "text": "1.5 Visualizing the Regression Line",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#the-broom-package-optional",
    "href": "session1/session1.html#the-broom-package-optional",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.6 The broom package (OPTIONAL!)",
    "text": "1.6 The broom package (OPTIONAL!)\nThe broom package in R is designed to bridge the gap between R’s statistical output and tidy data. 2 It takes the output of various R statistical functions and turns them into tidy data frames. This is particularly useful because many of R’s modeling functions return outputs that are not immediately suitable for further data analysis or visualization within the tidyverse framework.\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\n\n1.6.1 Nesting with nest()\n\nNesting in the context of the broom package usually refers to the idea of creating a list-column in a data frame (or even better a tibble) where each element of this list-column is itself a data frame (again, even better a tibble) or a model object. In a complex analysis, you might fit separate models to different subsets of data. Nesting allows you to store each of these models (or their broom-tidied summaries that we will see in the next three sub-sections) within a single, larger data frame (for the third time, the best is to do this with a tibble) for easier manipulation and analysis. For questions on what tibbles are see the below section on tibbles.\nLet’s go back to the midterm data which I have used before and randomly assign students into two classes (Class A or Class B) pretending that two different classes of students had taken the exams. I will then later on nest the data by class so that you can understand the logic of nesting.\n\ndata &lt;- data |&gt; \n  mutate(class = sample(c(\"A\", \"B\"), size = nrow(data), replace = TRUE))\n\nNow I will group my observations by class using group_by() and then nest them within these groups. The output will only contain two rows, class A and class B, and each row will contain a tibble with the observations of the respective class.\n\ndata |&gt; \n  group_by(class) |&gt;\n  nest()\n\n# A tibble: 2 × 2\n# Groups:   class [2]\n  class data              \n  &lt;chr&gt; &lt;list&gt;            \n1 A     &lt;tibble [92 × 3]&gt; \n2 B     &lt;tibble [108 × 3]&gt;\n\n\nThat is a very simple application of a nesting process. You can group_by() and nest() by many different variables. You might want to nest your observations per year or within countries. You can also nest by multiple variables at the same time. We will see this idea again in the next session when we will talk about the purrr package and how to automatically run regressions for several countries at the same time\n\n1.6.2 Model estimates with tidy()\n\nThe tidy() function takes statistical output ofa model and turns it into a tidy tibble. This means each row is an observation (e.g., a coefficient in a regression output) and each column is a variable (e.g., estimate, standard error, statistic). For instance, after fitting a linear model, you can use tidy() to create a data frame where each row represents a coefficient, with columns for estimates, standard errors, t-values, and p-values.\n\n1.6.3 Key metrics with glance()\n\nglance() provides a one-row summary of a model’s information. It captures key metrics that describe the overall quality or performance of a model, like \\(R^2\\), AIC, BIC in the context of linear models. This is useful for getting a quick overview of a model’s performance metrics.\n\n1.6.4 Residuals with augment()\n\nThe augment() function adds information about individual observations to the original data, such as fitted values or residuals in a regression model. You want to do this when you are evaluating a model fit at the observation level, checking for outliers, or understanding the influence of individual data points. We will talk more about model diagnostics in the next session!\nLet’s take a look at how this would work out in practice. For simplicity sake, we will set the nest() logic aside for a second and only look at the tidy(), glance(), and augment() functions.\nHere I only build the same model that we have already seen above:\n\nmodel &lt;- lm(final_exam ~ midterm, data = data)\n\n\nlibrary(broom)\ntidy(logit)\n\nNow let’s glance the hell out of the model:\n\nglance(logit)\n\nDon’t worry about what these things might mean for now, AIC and BIC will for example come up again next session.\n\n1.6.5 What is a tibble?",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#footnotes",
    "href": "session1/session1.html#footnotes",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "Malo’s explanation and way of introducing you to RStudio projects can be found here.↩︎\nAs a quick reminder these three principles are guiding when we speak of tidy data:↩︎",
    "crumbs": [
      "Session 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session2/session2.html",
    "href": "session2/session2.html",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "2.1 Introduction\nYou have seen the logic of Logistic Regressions with Professor Rovny in the lecture. In this lab session, we will understand how to apply this logic to R and how to build a model, interpret and visualize its results and how to run some diagnostics on your models. If the time allows it, I will also show you how automatize the construction of your model and run several logistic regressions for many countries at once.\nThese are the main points of today’s session and script:",
    "crumbs": [
      "Session 2",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#introduction",
    "href": "session2/session2.html#introduction",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "Getting used to the European Social Survey\nCleaning data: dropping rows, columns, creating and mutating variables\nBuilding a generalized linear model (glm()); special focus on logit/probit\nExtracting and interpreting the coefficients\nVisualization of results\nIntroduction to the purrr package and automation in RStudio (OPTIONAL!)",
    "crumbs": [
      "Session 2",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#data-management-data-cleaning",
    "href": "session2/session2.html#data-management-data-cleaning",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.2 Data Management & Data Cleaning",
    "text": "2.2 Data Management & Data Cleaning\nAs I have mentioned last session, I will try to gradually increase the data cleaning part. It is integral to R and operationalizing our quantitative questions in models. A properly cleaned data set is worth a lot. This time we will work on how to drop values of variables (and thus rows of our dataset) which we are either not interested in or, most importantly, because they skew our estimations.\n\n# these are the packages, I will need for this session\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Session 2",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#importing-the-data",
    "href": "session2/session2.html#importing-the-data",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.3 Importing the data",
    "text": "2.3 Importing the data\nWe have seen how to import a dataset. Create an .Rproj of your choice and create a folder in which the dataset of this lecture resides. You can download this dataset from our Moodle page. I have pre-cleaned it a bit. If you were to download this wave of the European Social Survey from the Internet, it would be a much bigger data set. I encourage you to do this and try to figure out ways to manipulate your data but for now, we’ll stick to the slightly cleaner version.\n\n# importing the data; if you are unfamiliar with this operator |&gt; , ask me or\n# go to my document \"Recap of RStudio\" which you can find on Moodle\ness &lt;- read_csv(\"ESS_10_fr.csv\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs you can see from the dataset’s name, we are going to work with the European Social Survey. It is the biggest, most comprehensive and perhaps also most important survey on social and political life in the European Union. It comes in waves of two years and all the European states which want to pay for it produce their own data. In fact, the French surveys (of which we are going to use the most recent, 10th wave) are produced at SciencesPo, at the Centre de Données Socio-Politiques (CDSP)!\nThe ESS is extremely versatile if you need a broad and comprehensive data set for both national politics in Europe or to compare European countries. Learning how to use it, how to manage and clean the ESS waves will give you all the instruments to work with almost any data set that is “out there”. Also, some of you might want to use the ESS waves for your theses or research papers. There is a lot that can be done with it, not only cross-sectionally but also over time. So give it a try :)\nEnough advertisement for the ESS, let’s get back to wrangling with our data! As always, the first step is to inspect (“glimpse”) at our data and the data frame’s structure. We do this to see if obvious issues arise at a first glance.\n\nglimpse(ess)\n\nRows: 33,351\nColumns: 25\n$ name     &lt;chr&gt; \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS1…\n$ essround &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ edition  &lt;dbl&gt; 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2…\n$ proddate &lt;chr&gt; \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.1…\n$ idno     &lt;dbl&gt; 10002, 10006, 10009, 10024, 10027, 10048, 10053, 10055, 10059…\n$ cntry    &lt;chr&gt; \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"…\n$ dweight  &lt;dbl&gt; 1.9393836, 1.6515952, 0.3150246, 0.6730366, 0.3949991, 0.8889…\n$ pspwght  &lt;dbl&gt; 1.2907065, 1.4308782, 0.1131722, 1.4363747, 0.5848892, 0.6274…\n$ pweight  &lt;dbl&gt; 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177…\n$ anweight &lt;dbl&gt; 0.28100810, 0.31152576, 0.02463945, 0.31272244, 0.12734002, 0…\n$ prob     &lt;dbl&gt; 0.0003137546, 0.0003684259, 0.0019315645, 0.0009040971, 0.001…\n$ stratum  &lt;dbl&gt; 185, 186, 175, 148, 138, 182, 157, 168, 156, 135, 162, 168, 1…\n$ psu      &lt;dbl&gt; 2429, 2387, 2256, 2105, 2065, 2377, 2169, 2219, 2155, 2053, 2…\n$ polintr  &lt;dbl&gt; 4, 1, 3, 4, 1, 1, 3, 3, 3, 3, 1, 4, 2, 2, 3, 3, 2, 2, 4, 2, 3…\n$ trstplt  &lt;dbl&gt; 3, 6, 3, 0, 0, 0, 5, 1, 2, 0, 5, 4, 7, 5, 2, 2, 2, 2, 0, 3, 0…\n$ trstprt  &lt;dbl&gt; 3, 7, 2, 0, 0, 0, 3, 1, 2, 0, 7, 4, 2, 6, 2, 1, 3, 1, 0, 3, 3…\n$ vote     &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2…\n$ prtvtefr &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ clsprty  &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2…\n$ gndr     &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1…\n$ yrbrn    &lt;dbl&gt; 1945, 1978, 1971, 1970, 1951, 1990, 1981, 1973, 1950, 1950, 1…\n$ eduyrs   &lt;dbl&gt; 12, 16, 16, 11, 17, 12, 12, 12, 11, 3, 12, 12, 15, 15, 19, 11…\n$ emplrel  &lt;dbl&gt; 1, 3, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1…\n$ uemp12m  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n$ uemp5yr  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n\n\nAs we can see, there are many many variables (25 columns) with many many observations (33351). Some are quite straight-forward and the name is clear (“essround”, “age”) and some much less. Sometimes we can guess the meaning of a variable’s name. But most of the time – either because guessing is too annoying or because the abbreviation is not making any sense – we need to turn to the documentation of the data set. You can find the documentation of this specific version of the data set in an html-file on Moodle (session 2).\nEvery (good and serious) data set has some sort of documentation somewhere. If not, it is not a good data set and I am even tempted to say that we should be careful in using it! The documentation for data sets is called a code book. Code books are sometimes well crafted documents and sometimes just terrible to read. In this class, you will be exposed to both kinds of code books in order to familiarize you with both.\nIn fact, this dataframe still contains many variables which we either won’t need later on or that are simply without any information. Let’s get rid of these first. This is a step which you can also do later on but I believe that it is smart to this right at the beginning in order to have a neat and tidy data set from the very beginning.\nYou can select variables (select()) right at the beginning when importing the csv file.\n\ness &lt;- read_csv(\"ESS_10_fr.csv\")  |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  )\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHowever, I am realizing that when looking at the number of rows that my file is a bit too large for only one wave and only one country. By inspecting the ess$cntry variable, I can see that I made a mistake while downloading the dataset because it contains all countries of wave 10 instead of just one. We can fix this really easily when importing the dataset:\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  ) |&gt;\n  filter(cntry == \"FR\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis only leaves us with the values for France!\n\n2.3.0.1 Cleaning our DV\nAt this point, you should all check out the codebook of this data set and take a look at what the values mean. If we take the variable of ess$vote for example, we can see that there are many numeric values of which we can make hardly any sense (without guessing and we don’t do this over here) of what they might stand for.\n\nsummary(ess$vote) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.799   2.000   8.000 \n\n# remember that you can summarize() both dataframes and individual variables\n\nOr in a table containing the amount of times that a value was given:\n\ntable(ess$vote)\n\n\n   1    2    3    7    8 \n1025  590  307    9   46 \n\n\nHere we can see that the variable vote contains the numeric values of 1 to 3 and then 7, 8, and 9. If we take a look at the code book, we can see what they stand for:\n\n1 = yes (meaning that the respondent voted)\n2 = no (meaning that the respondent did not vote)\n3 = not eligible to vote\n7 = Refusal\n8 = Don’t know\n9 = No answer\n\nThe meaning behind the values of 7, 8, and 9 are quite common and you will find them in almost all data sets which were made out of surveys. Respondents might not want to give an answer, the answer was not able to be read, or they indicated that they did not remember.\nSpoiler alert: We will work on voter turnout this session: Therefore, we will try to see what makes people vote and what decreases the likelihood that they vote at election day. The question of our dependent variable will thus be: Has the respondent voted or not?. Mathematically, this question cannot be answered with a linear regression which uses the OLS method as the dependent variable is binary meaning that 0 = has not voted/1 = has voted. There is only two possible outcomes and the variable is not continuous (one of the assumptions of an OLS).\nBut if we were to use the variable on voting turnout as it is right now, we would neither have a binary variable nor have a reliable variable as it contains values in which we are both not interested in and that will skew our estimations strongly. In fact, we cannot do a logistic regression (logit) on variables other than binary.\nThus, we first need to transform our dependent variable. We need to get rid of unwanted values and transform the 1s and 2s in 0s and 1s.\nFirst we will get rid of the unwanted values in which we are not interested.\n\n# dplyr\ness &lt;- ess |&gt;\n  filter(!vote %in% c(3, 7, 8))\n\nHere are two other ways to do it:\n\n# this one would be in base R\ness[!vote %in% c(3, 7, 8, 9)]\n\n# using the subset() function, this returns a logical vector which elements of\n# vote are not in the set of values 3, 7, 8, or 9\ness &lt;- subset(ess, vote %in% c(3,7,8,9) == F) \n\n# Alternatively you can use the %in% function with ! operator as well like this:\ness &lt;- subset(ess, !vote %in% c(3,7,8,9))\n\nQuick check to see if we got rid of all the values:\n\ntable(ess$vote)\n\n\n   1    2 \n1025  590 \n\n\nPerfect, now we just need to transform the ones and twos into zeros and ones. This is both out of convention and also to torture you with some more data management. Since we are interested in people who do not vote, we will code those people as 0 and those who did vote as 1.\n\ness &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0))\n\nThe mutate() function is not perfectly intuitive at first sight. Here, I use the ifelse() function within mutate() to check if vote is equal to 1, if it is then it will be replaced by 0 and if not it will be replaced by 1.\nIn Base R, you could do it like this but I believe that the mutate() function is probably the most elegant way of doing things… It’s up to you though:\n\n# only use the ifelse() function\ness$vote &lt;- ifelse(ess$vote == 1, 1, 0)\n\n# This will leave the value of 1 at 1 and change 2 to 0 for the column vote.\ness$vote[ess$vote == 1] &lt;- 1\ness$vote[ess$vote == 2] &lt;- 0\n\nWe are this close to having finished our data management part and to being able to finally work on our model. But we still have many many variables which are as “untidy” as our initial dependent variable was. If you know what the values are that you do not want – and if you are absolutely certain that they are not important in other variables – there is a quick way of getting rid of these. How am I absolutely certain that I can confidently transform the rows containing certain values without losing information? The answer always lies in the code book :)\n\nlibrary(naniar)\nunwanted_numbers &lt;- c(66, 77, 88, 99, 7777, 8888, 9999)\ness_clean &lt;- ess |&gt; \n  replace_with_na_all(condition = ~.x %in% unwanted_numbers)\n\nLastly, and I promise that this is the last data wrangling part for today and that we will get to our model in a moment, we need to check in the code book if specific values that we cannot simply replace over the whole data frame are still void of interest.\nOur independent variables of interest:\n\npolitical interest (polintr): c(7:9) needs to be dropped\ntrust in politicians (trstplt): no recoding necessary (unwanted values already NAs)\ntrust in political parties (trstprt): already done\nfeeling close to a party (clsprty): transform 1/2 into 0/1, drop c(7:8)\n\ngender (gndr): transform into 0/1 and drop the no answers\nyear born (yrbrn): already done\nyears of full-time education completed (eduyrs): already done\n\nWe will do every single step at once now using the pipes %&gt;% (tidyverse) or |&gt; (Base R) to have a tidy and elegant way of doing everything at once:\n\n# specify a string of numbers we are absolutely certain we won't need\nunwanted_numbers &lt;- c(66, 77, 88, 99, 7777, 8888, 9999)\n\n# make sure to create a new object/data frame; if you don't and re-run your code\n# a second time, it will transform some mutated values again!\ness_final &lt;- ess |&gt; \n  # filtering the dependent variable to get rid of any unnecessary rows\n  filter(!vote %in% c(3, 7, 8, 9)) |&gt; \n  # using the naniar package, we can transform unwanted values to NAs\n  naniar::replace_with_na_all(condition = ~.x %in% unwanted_numbers) |&gt; \n  # mutate allows us to transform values within variables into other \n  # values or NAs\n  # vote as binary 1 (voted) & 0 (abstention)\n  mutate(vote = ifelse(vote == 1, 1, 0), \n         # replace values 7 to 9 with NAs\n         polintr = replace(polintr, polintr %in% c(7:9), NA),\n         # replace values 7 to 9 with NAs\n         clsprty = replace(clsprty, clsprty %in% c(7:9), NA),\n         # recode the variable to 0 and 1\n         clsprty = recode(clsprty, `1` = 1, `2` = 0),\n         # same for gender\n         gndr = recode(gndr, `1` = 0, `2` = 1))\n\n\n2.3.1 Constructing the logit-model\nIf you have made it this far and still bear with me, you have made it to the fun part! Specifying the model and running it, literally only takes one line of code (or two depending on the amount of independent variables). And as you can see, it is really straightforward. The glm() function stands for generalized linear model and comes with Base R.\nIn Professor Rovny’s lecture, we have seen that for a Maximum Likelihood Estimation (MLE) you need to know or have an assumption about the distribution of your dependent variable. And according to this distribution, you need to find the right linear model. If you have a binary outcome, your distribution is binomial. Within the function, we thus specify the family of the distribution as such. Note that you could also specify other families such as Gaussian, poisson, gamma or many more. We are not going to touch further on that but the glm() function is quite powerful. We can specify, within the family = argument, that we are doing a logistic regression. This can be done by adding link = logit to the argument. If ever you wanted to be precise and call a probit or cauchy link, it is here that you can specify this. The standard, however, is set to logit, so we would technically not be forced to specify it in this case.\nIn terms of the model we are building right now, it follows the idea that voting behavior (voted/not-voted) is a function of political interest, trust in politicians, trust in parties, feeling close to a specific party, as well as usual control variables such as gender, age, and education:\nBy no means is this regression just extensive enough to be published. It is just one example in which I suspect that political interest, trust in politics and politicians, and party affiliation are explanatory factors.\n\nlogit &lt;- glm(\n  vote ~ polintr + trstplt + trstprt + clsprty + gndr +\n    yrbrn + eduyrs,\n  data = ess_final,\n  family = binomial(link = logit)\n)\n\nThe object called logit contains our model with its coefficients, confidence intervals and many more things that we will play with! But as you can see, the actual construction of the model is more than simple…\n\nlibrary(broom)\ntidy(logit)\n\n# A tibble: 8 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 100.       8.69       11.5   1.17e-30\n2 polintr      -0.436    0.0726     -6.01  1.88e- 9\n3 trstplt       0.0829   0.0481      1.72  8.46e- 2\n4 trstprt       0.0344   0.0515      0.668 5.04e- 1\n5 clsprty       0.710    0.130       5.48  4.32e- 8\n6 gndr         -0.0280   0.122      -0.230 8.18e- 1\n7 yrbrn        -0.0510   0.00448   -11.4   4.54e-30\n8 eduyrs        0.113    0.0195      5.82  5.76e- 9\n\n\nYou have seen both the broom package as well as stargazer in the last session.\n\nstargazer::stargazer(\n  logit,\n  type = \"text\",\n  dep.var.labels = \"Voting Behavior\",\n  dep.var.caption = c(\"Voting turnout; 0 = abstention | 1 = voted\"),\n  covariate.labels = c(\n    \"Political Interest\",\n    \"Trust in Politicians\",\n    \"Trust in Parties\",\n    \"Feeling Close to a Party\",\n    \"Gender\",\n    \"Year of Birth\",\n    \"Education\"\n  )\n)\n\n\n===================================================================\n                         Voting turnout; 0 = abstention | 1 = voted\n                         ------------------------------------------\n                                      Voting Behavior              \n-------------------------------------------------------------------\nPolitical Interest                       -0.436***                 \n                                          (0.073)                  \n                                                                   \nTrust in Politicians                       0.083*                  \n                                          (0.048)                  \n                                                                   \nTrust in Parties                           0.034                   \n                                          (0.051)                  \n                                                                   \nFeeling Close to a Party                  0.710***                 \n                                          (0.130)                  \n                                                                   \nGender                                     -0.028                  \n                                          (0.122)                  \n                                                                   \nYear of Birth                            -0.051***                 \n                                          (0.004)                  \n                                                                   \nEducation                                 0.113***                 \n                                          (0.019)                  \n                                                                   \nConstant                                 100.041***                \n                                          (8.692)                  \n                                                                   \n-------------------------------------------------------------------\nObservations                               1,546                   \nLog Likelihood                            -828.497                 \nAkaike Inf. Crit.                        1,672.994                 \n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n2.3.2 Interpretation of a logistic regression\nInterpreting the results of a logistic regression can be a bit tricky because the predictions are in the form of probabilities, rather than actual outcomes. This sounds quite abstract and you are right, it is abstract. However, with a proper understanding of the coefficients and odds ratios, you can gain insights into the relationship between your independent variables and the binary outcome variable even without transforming your coefficients into more easily intelligible values.\nFirst the really boring and technical definition: The coefficients of a logistic regression model represent the change in the log-odds of the outcome for a one-unit change in the predictor variable (holding all other predictors constant). The sign of the coefficient indicates the direction of the association: positive coefficients indicate that as the predictor variable increases, the odds of the outcome also increase, while negative coefficients indicate that as the predictor variable increases, the odds of the outcome decrease.\nThe odds ratio, which can be calculated from the coefficients and we will see how that works in a second (exponentation is the key word), represents the ratio of the odds of the outcome for a particular value of the predictor variable compared to the odds of the outcome for a reference value of the predictor variable. An odds ratio greater than 1 indicates that the predictor variable is positively associated with the outcome, while an odds ratio less than 1 indicates that the predictor variable is negatively associated with the outcome.\nIt’s also important to keep in mind that a logistic regression model makes assumptions about the linearity, independence and homoscedasticity of the data, if these assumptions are not met it can affect the model’s performance and interpretation. We will see the diagnostics of logistic regression models again next session.\nIs this really dense and did I lose you? It is dense but I hope you bear with me because we will see that it becomes much clearer once we apply this theory to our model but also once we exponentiate the coefficients (reversing the logarithm so to speak) and interpret them as odds-ratios.\nBut from a first glimpse at our model summary we can see that political interest, trust in politicians, closeness to a party, age and education are all statistically significant, meaning that their p-value is &lt;.05! I will not regard any other variable that is not statistically significant as you do not usually interpret non-significant variables.\nNext, we can already say that the association of interest, closeness to party and age with voting behavior is negative. This is quite logical and makes sense in our case. If we look at the scales on which these variables are coded (code book!), we can see that the higher the value of the variable, the less interested, close or aged the respondents were. Thus, it decreases their likelihood to vote on voting day. Trust in politicians is coded the other way around. If I had been a little more thorough, it would have been good to put each independent variable on the same scale… But it means that trust in politicians (in fact meaning that they trust them less) raises the likelihood of not voting somehow (positive association).\n\n2.3.2.1 Odds-ratio\nIf you exponentiate the coefficients of your model, you can interpret them as odds-ratios. Odds ratios (ORs) are often used in logistic regression to describe the relationship between a predictor variable and the outcome. ORs are easier to interpret than the coefficients of a logistic regression because they provide a measure of the change in the odds of the outcome for a unit change in the predictor variable.\nAn OR greater than 1 indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome, and an OR less than 1 indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome.\nThe OR can also be used to compare the odds of the outcome for different levels of the predictor variable. For example, an OR of 2 for a predictor variable means that the odds of the outcome are twice as high for one level of the predictor variable compared to another level. Therefore, odds ratios are often preferred to coefficients for interpreting the results of a logistic regression, especially in applied settings.\nI will try to rephrase this and make it more accessible so that odds-ratios maybe become more intelligible (they are really nasty statistical stuff):\nImagine you’re playing a game where you have to guess whether a coin will land on heads or tails. If the odds of the coin landing on heads is the same as the odds of it landing on tails, then the odds-ratio would be 1. This means that the chances of getting heads or tails are the same. But if the odds of getting heads is higher than the odds of getting tails, then the odds-ratio would be greater than 1. This means that the chances of getting heads is higher than the chances of getting tails. On the other hand, if the odds of getting tails is higher than the odds of getting heads, then the odds-ratio would be less than 1. This means that the chances of getting tails is higher than the chances of getting heads. In logistic regression, odds-ratio is used to understand the relationship between a predictor variable (let’s say “X”) and an outcome variable (let’s say “Y”). Odds ratio tells you how much the odds of Y happening change when X changes.\nSo, for example, if the odds ratio of X is 2, that means that if X happens, the odds of Y happening are twice as high as when X doesn’t happen. And if the odds ratio of X is 0.5, that means that if X happens, the odds of Y happening are half as high as when X doesn’t happen.\n\n# simply use exp() on the coefficients of the logit\nexp(coef(logit))\n\n (Intercept)      polintr      trstplt      trstprt      clsprty         gndr \n2.800183e+43 6.464189e-01 1.086470e+00 1.034965e+00 2.033083e+00 9.723649e-01 \n       yrbrn       eduyrs \n9.502474e-01 1.120084e+00 \n\n# here would be a second way of doing it\nexp(logit$coefficients)\n\n (Intercept)      polintr      trstplt      trstprt      clsprty         gndr \n2.800183e+43 6.464189e-01 1.086470e+00 1.034965e+00 2.033083e+00 9.723649e-01 \n       yrbrn       eduyrs \n9.502474e-01 1.120084e+00 \n\n\nWe can also, and should, add the 95% confidence intervals (CI). As a quick reminder, the CI is a range of values that is likely to contain the true value of a parameter (the coefficients of our predictor variables in our case). This comes at a certain level of confidence. The most commonly used levels (attention, this is only a statistical convention!) of confidence are 95% and sometimes 99%.\nA 95% CI for a parameter, for example, means that if the logistic regression model were fitted to many different samples of data, the true value of the parameter would fall within the calculated CI for 95% of those samples.\n\n# most of the times the extra step in the next lines is not necessary and this \n# line of code is enough\nexp(cbind(OR = coef(logit), confint(logit)))\n\nWaiting for profiling to be done...\n\n\n                      OR        2.5 %       97.5 %\n(Intercept) 2.800183e+43 1.426529e+36 9.118492e+50\npolintr     6.464189e-01 5.599783e-01 7.445568e-01\ntrstplt     1.086470e+00 9.892863e-01 1.194783e+00\ntrstprt     1.034965e+00 9.351755e-01 1.144451e+00\nclsprty     2.033083e+00 1.578414e+00 2.623582e+00\ngndr        9.723649e-01 7.653280e-01 1.235391e+00\nyrbrn       9.502474e-01 9.418138e-01 9.585074e-01\neduyrs      1.120084e+00 1.078523e+00 1.164144e+00\n\n# here, however, we must combine both the exponentiate coefficients with the 95% confidence intervals\n# the format() function, helps me to show the numbers without the exponentiated \n# \"e\" and without scientific notation; the round() function within this function gives me values which are rounded on the 5th decimal place.\nformat(round(exp(cbind(\n  OR = coef(logit), confint(logit)\n)), 5),\nscientific = FALSE, digits = 4)\n\nWaiting for profiling to be done...\n\n\n            OR                                                        \n(Intercept) \"       28001829913514797465761023437076878514454528.0000\"\npolintr     \"                                                  0.6464\"\ntrstplt     \"                                                  1.0865\"\ntrstprt     \"                                                  1.0350\"\nclsprty     \"                                                  2.0331\"\ngndr        \"                                                  0.9724\"\nyrbrn       \"                                                  0.9503\"\neduyrs      \"                                                  1.1201\"\n            2.5 %                                                     \n(Intercept) \"              1426529172752230200649154036500529152.0000\"\npolintr     \"                                                  0.5600\"\ntrstplt     \"                                                  0.9893\"\ntrstprt     \"                                                  0.9352\"\nclsprty     \"                                                  1.5784\"\ngndr        \"                                                  0.7653\"\nyrbrn       \"                                                  0.9418\"\neduyrs      \"                                                  1.0785\"\n            97.5 %                                                    \n(Intercept) \"911849199216411839268345883146918313683541200732160.0000\"\npolintr     \"                                                  0.7446\"\ntrstplt     \"                                                  1.1948\"\ntrstprt     \"                                                  1.1444\"\nclsprty     \"                                                  2.6236\"\ngndr        \"                                                  1.2354\"\nyrbrn       \"                                                  0.9585\"\neduyrs      \"                                                  1.1641\"\n\n\nThis exponentiated value, the odds ratio (OR), now allows us to say that for a one unit increase in political interest, for example, the odds of voting (versus not voting) decrease. The same goes for the other variables.\nThe last thing about odds-ratio and I hope that this is the easiest to interpret, is when you try to make percentages out of it:\n\n# the [-1] drops the value of the intercept as it is statistically meaningless\n# we put another minus one to get rid of 1 as a threshold for interpreting the\n# odds-ratio\n# we multiply by 100 to have percentages\n100*(exp(logit$coefficients[-1])-1)\n\n   polintr    trstplt    trstprt    clsprty       gndr      yrbrn     eduyrs \n-35.358112   8.646994   3.496496 103.308308  -2.763507  -4.975257  12.008377 \n\n\nThis allows us to say that being politically uninterested decreases the odds of voting by 35%. Much more straightforward right?\n\n2.3.2.2 Predicted Probabilities\nPredicted probabilities also allow us to understand our logistic regression. In logistic regressions, the predicted probabilities and ORs are two different ways of describing the relationship between the predictor variables and the outcome. Predicted probabilities refer to the probability that a specific outcome will occur, given a set of predictor variables. They are calculated using the logistic function, which maps the linear combination of predictor variables (also known as the log-odds) to a value between 0 and 1.\nThe importance here is that we chose the predictor variables and at which values of those we are trying to predict the outcome. This is what we call “holding independent variables constant” while we calculate the predicted probability for a specific independent variable of interest.\nI will repeat this to make sure that everybody can follow along. With the predicted probabilities, we are trying to make out the effect of one specific variable of interest on our dependent variable, while we hold every other variable at their mean, median in some cases or, in the case of a dummy variable, at one of the two possible values. By holding them constant, we can be sure to see the singular effect of our independent variable of interest.\nIn our case, let “feeling close to a party” (1 = yes; 0 = no) be our independent variable of interest. We take our old ess_final dataframe and create a new one. In the newdata dataframe, we hold all values at their respective means or put our binary/dummy variables to 1. It is an arbitrary choice to put it to one here. We could also put it to 0. The only variable that we allow to alternate freely to find the predicted probabilities is our variable of interest clsprty.\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = mean(polintr),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = c(0, 1),\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n    ))\n\nIf that all worked out, we can predict the values for this specific independent variable by using the Base R predict() function:\n\nnewdata$preds &lt;- predict(logit, newdata = newdata, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata, aes(x = clsprty, y = preds)) +\n  geom_line() +\n  ylab(\"Likelihood of Voting\") + xlab(\"Feeling Close to a Party\")\n\n\n\n\n\n\n\nWe can also do the same thing to see the predicted probability of political interest on voting behavior. This is a bit more interesting as the variable is not binary like ess$clsprty:\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata_1 &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = c(1:4),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = 1,\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n  )\n)\n\n\nnewdata_1$preds &lt;- predict(logit, newdata = newdata_1, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata_1, aes(x = polintr, y = preds)) +\n  geom_line() +\n  ylab(\"predicted probability\") + xlab(\"political interest\")\n\n\n\n\n\n\n\n\n#combines value data frame created above with predicted probabilities evaluated \n# at the data values\nnewdata_1 &lt;-\n  cbind(newdata_1,\n        predict(\n          logit,\n          newdata = newdata_1,\n          type = \"link\",\n          se = TRUE\n        )) \n\n\nnewdata_1 &lt;- within(newdata_1, {\n  pp &lt;- plogis(fit)                   # predicted probability\n  lb &lt;- plogis(fit - (1.96 * se.fit)) # builds lower bound of CI\n  ub &lt;- plogis(fit + (1.96 * se.fit)) # builds upper bound of CI\n})\n\n\nggplot(newdata_1, aes(x = polintr, y = pp)) +\n  geom_line(aes(x = polintr, y = pp, color = as.factor(gndr))) +\n  geom_ribbon(aes(ymin = lb, ymax = ub), alpha = 0.3) +\n  theme(legend.position = \"none\") +\n  ylab(\"predicted probability to abstain from voting\") +\n  xlab(\"political interest\")\n\n\n\n\n\n\n\n\n2.3.3 Making life easiest\nYou are going to hate me if I tell you that all these steps which we just computed by hand… can be done by using a package. This is only 0.01% of me trying to be mean but mostly because it is extremely helpful and necessary to understand what is going on under the hood of predicted probabilities. The interpretation of logistic regressions is tricky and if you do not know what you are computing, it is even more complicated.\nWorking with packages is great, and I am aware that I always encourage you to use packages that make your life easier. But and this is an important “but” we do not always understand what is going on under the hood of a package. It is like putting your logistic regression into a black box, shaking it really well, and then taking a look at the output and putting it on shaky interpretational terms.\nBut enough of personal defense, as to why I made you suffer through all this. Here is my code to do most of the steps at once:\n\n# this package contains everything we need craft predicted probabilities and\n# visualize them as well\nlibrary(ggeffects)\n\n# like the predcit() function of Base R, we use ggpredict() and specify\n# our variable of interest\ndf &lt;- ggpredict(logit, terms = \"polintr\")\n\n# this is the simplest way of plotting this\nggplot(df, aes(x = x, y = predicted)) +\n  # our graph is more or less a line, so geom_line() applies\n  geom_line() +\n  # geom_ribbon() with the values that ggpredict() provided for the confidence\n  # intervals then gives\n  # us a shade around the geom_()line as CIs\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1)\n\n\n\n\n\n\n\nAnd voilà, your output it less than 10 lines of code.",
    "crumbs": [
      "Session 2",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#automating-things-in-r-optional",
    "href": "session2/session2.html#automating-things-in-r-optional",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.4 Automating things in R (OPTIONAL!)",
    "text": "2.4 Automating things in R (OPTIONAL!)\nWhen programming, it usually takes time to understand and apply things. The next step should often be to think about how to automate something in order to make processes faster and more elegant. Once we have understood a process relatively well, we can apply it to other areas in an automated way relatively easily.\nFor example, let’s think about our logistical regressions today. We worked with the ESS and looked at the 10th round of the data set for France. Our model aimed to investigate which variables can predict abstention. But I can also ask myself this question over a certain period of time, i.e. over several waves of the ESS, or for several countries. If I now tell you that you should do the logit for all countries of the ESS, the first step would be to run my code for n = countries of the ESS. However, this would result in a very long script, would not be very elegant and the longer the code, the higher the probability of errors.\nIf you are programming and realize that you have to do the same steps several times and it is actually the same step, only that a few parameters (such as the names of variables) change, then you can be sure that this could also be automated. At a certain point in programming, the goal should always be to create a pipeline for the work steps, which has the goal of making our code run as automatically as possible.\nIn RStudio we have several ways to automate this. For example, you can write so-called for-loops, which perform certain operations one after the other for certain list entries. Or you can write your own functions. At some point, someone decided to write the function glimpse() or read_csv2(), for example. As private users of R, we can do the same. In this way, we can, for example, accommodate several operations within a function that we write ourselves, which can then be applied simultaneously to several objects or a data set with different countries.\nI am aware that this is only the second session and that may sound like a lot. Everything from here on is optional, but I think it’s important that you see this as early as possible. Some of you may already feel comfortable enough to try something like this out for yourselves. If you don’t, that’s okay too. You will get there, trust me! I just want to show you what is possible and what you can do with R.\n\n2.4.1 Writing your function\nFunctions in R are incredibly powerful and essential for efficient and automated programming. A function in R is defined using the function keyword. The basic structure includes the name of the function, a set of parameters, and the body where the actual computations are performed.\nThe basic syntax is as follows:\nThere are some minor conventions in RStudio when writing functions. Some of them also apply to other parts than just functions.\n\nYou should give them some “breathing” space. When you write the accolades, put a space bar in between.\n\n\n# this is bad\nfunction(x){\n  x + 1\n}\n\n\n# this is good\nfunction(x) {\n  x + 1\n}\n\n\nYou should always put a space between the equal sign and the value you assign to a variable. Place spaces around all infix operators (=, +, -, &lt;-, etc.)\n\n\n# this is bad\ndata&lt;-read.csv(\"data.csv\")\n\n\ndata &lt;- read.csv(\"data.csv\")\n\n\n“stretch” your code when possible ctrl + shift + a can be of help for that\n\n\n# this is bad but in a longer example\ncountry_model &lt;- function(df) {glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, family = binomial(link = \"logit\"), data = df)\n}\n\n\n# this is better\ncountry_model &lt;- function(df) {\n  glm(\n    vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs,\n    family = binomial(link = \"logit\"),\n    data = df\n  )\n}\n\n\nWe usually assign verbs to functions. This means that the name of the function should be a verb that describes what the function does. If we want to create a function that reads the ESS files and does several operations at once, we should call it something like read_ess().\n\n2.4.2 The purrr package\nThe purrr package in R is a powerful tool that helps in handling repetitive tasks more efficiently. It’s part of the Tidyverse, a collection of R packages designed to make data science faster, easier, and more fun!\nIn simple terms, purrr improves the way you work with lists and vectors in R. It provides functions that allow you to perform operations, i.e. pre-existing functions or functions you will write yourselves, on each element of a list or vector without writing explicit loops. This concept is known as functional programming.\n\n\n\n\n\n\nWhy use purrr instead of for-loops?\n\n\n\n\n\n\nSimplifies Code: purrr makes your code cleaner and more readable. Instead of writing several lines of loop code, you can achieve the same with a single line using purrr.\nConsistency and Safety: purrr functions are consistent in their behavior, which reduces the chances of errors that are common in for loops, like mistakenly altering variables outside the loop.\nHandles Complexity Well: When working with complex or nested lists, purrr offers tools that make these tasks much simpler compared to traditional loops.\nIntegration with tidyverse: Since purrr is part of the tidyverse, it integrates smoothly with other Tidyverse packages, making your entire data analysis workflow more efficient.\n\n\n\n\nPut simply, we can use the functions of the purrr package to apply a function to each element of a list or vector. Depending on the specific purrr-function, our output can be different but we can also specify the output in our manually written function which we feed into purrr.\n\n2.4.3 The map() function\nThe map() function, part of the purrr package in R, is built around a simple yet powerful concept: applying a function to each element of a list or vector and returning a new list with the results. This concept is known as “mapping,” hence the name map().\nHere’s a breakdown of the logic behind map():\n\nInput: The primary input to map() is a list or a vector. This could be a list of numbers, characters, other vectors, or even more complex objects like data frames.\nFunction Application: You specify a function that you want to apply to each element of the list. This function could be a predefined function in R, or a custom function you’ve written. The key is that this function will be applied individually to each element of your list/vector.\nIteration: map() internally iterates over each element of your input list/vector. You don’t need to write a loop for this; map() handles it for you. For each element, map() calls the function you specified.\nOutput: For each element of the list/vector, the function’s result is stored. map() then returns a new list where each element is the result of applying the function to the corresponding element of the input.\nFlexibility in Output Type: The purrr package provides variations of map() to handle different types of output. For example, map_dbl() if your function returns doubles, map_chr() for character output, and so on. This helps in ensuring that the output is in the format you expect.\n\n2.4.4 Automatic regressions for several countries\nThis is absolutely only optional. I do not ask you to reproduce anything of this at any point in this class. I simply wanted to show you what you can do in R and what I mean when I say that automating stuff makes life easier.\nI present you here with a code that does the logistic regression we have been doing but on all countries of the ESS at the same time and then plots us the odds-ratio of our variable of interest “political interest”, as well as comparing McFadden pseudo R2 (we’ll see this term next session again).\nI will combine things from the optional section of Session 1 and the purrr package to do this. The idea is to build one model per country of the ESS, nest() it in a new tibble (What are tibbles again?) 1 where each row contains the information necessary for one country-model and to then use map() to apply the function tidy() from the broom package to each of these models.\nBelow, you can find a simple function that takes a dataframe as input and returns a logistic regression model. Within, I only specify the glm() function which I have shown you above. Within the parentheses of function() I specify the name of the input object. This name is arbitrary and can be anything you want. I chose df for dataframe. It has to appear somewhere within your function later on; usually there where your operation on the input object is supposed to be performed. In my case this is data = df.\n\ncountry_model &lt;- function(df) {\n  glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, \n      family = binomial(link = \"logit\"), data = df)\n}\n\n\nI will first talk you through the different steps and then provide you one long pipeline that does all this in one step.\n\nFirst, I import the ESS data and select the variables I want to use. I also clean the data a bit and get rid of unwanted observations or values.\n\n# Recoding the 'vote' variable to binary (1 or 0)\n# and filtering the dataset based on specified criteria\nprepared_ess &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt; \n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  )\n\nThen we will nest() the data as described here where I explain the logic of nesting.\n\n# library for McFadden pseudo R2\nlibrary(pscl)\nlibrary(broom)\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  select(cntry,\n         vote,\n         polintr,\n         trstplt,\n         trstprt,\n         clsprty,\n         gndr,\n         yrbrn,\n         eduyrs)\n\ness_model &lt;- ess |&gt; \n  as_tibble() |&gt;  # Convert the data frame to a tibble\n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt;\n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  ) |&gt;\n  group_by(cntry) |&gt;\n  nest() |&gt;\n  mutate(\n    model = map(data, country_model),\n    tidied = map(model, ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE)),\n    glanced = map(model, glance),\n    augmented = map(model, augment),\n    mcfadden = map(model, ~ pR2(.x)[4])\n  )\n\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\n\n\n\n# Comparing AICs\npR2(logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n -828.4972459 -1011.5800658   366.1656399     0.1809870     0.2108881 \n         r2CU \n    0.2889617 \n\ness_model |&gt;\n  unnest(mcfadden) |&gt;\n  ggplot(aes(fct_reorder(cntry, mcfadden), mcfadden)) +\n  geom_col() + coord_flip() +\n  scale_x_discrete(\"Country\")\n\n\n\n\n\n\n\n\n# Comparing coefficients\ness_model |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"polintr\") |&gt;\n  ggplot(aes(\n    reorder(cntry, estimate),\n    y = exp(estimate),\n    color = cntry,\n    ymin = exp(conf.low),\n    ymax = exp(conf.high)\n  )) +\n  geom_errorbar() +\n  geom_point() +\n  scale_x_discrete(\"Country\") +\n  ylab(\"Odds-Ratio of political interest\") +\n  xlab(\"Country\")",
    "crumbs": [
      "Session 2",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#footnotes",
    "href": "session2/session2.html#footnotes",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "The quick answer is that tibbles are a modern take on data frames in R, offering improved printing (showing only the first 10 rows and fitting columns to the screen), consistent subsetting behavior (always returning tibbles), tolerance for various column types, support for non-standard column names, and no reliance on row names. They represent a more adaptable, user-friendly approach for handling data in R, especially suited for large, complex datasets.↩︎",
    "crumbs": [
      "Session 2",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html",
    "href": "appendix/rmarkdown.html",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "",
    "text": "A.1 What are Quarto & RMarkdown?\nQuarto is an open source tool that allows you to combine code, its outputs, with text in order to publish reproducible and high quality scripts and documents. You can include several (different) programming languages such as R, Python, Julia or others in either the same or single scripts.\nBefore quarto, Markdown was the standard. RMarkdown is a version that is specifically tailored to R. If you know how to use tools like Obsidian, you might already be familiar with the Markdown syntax. Similarly to Quarto, it is a framework in which you can code in R, document your code, annotate it with text and present your research, graphs and models to others. It generates documents which are not only nice to look at but also the best way to present your quantitative work to others. Quarto acts as a more powerful wrapper around Markdown. Once you have understood the syntax, and believe me when I say that it is not tricky, you might even choose to take your notes in the Markdown format. Every Markdown document needs to be knitted. This will transform your text, code and the commands (which you have told R to do) into either HTML, PDF or Word documents. This is what we call the output of your Markdown document.\nIn the Markdown language, you have to explain to the computer what you want it to show in the output. Unlike how we would see it in Microsoft Word or Pages, you do not have any buttons which allow you to write in italic or bold letters. You need to let R know, via certain commands that I will present to you below, what it is supposed to do with the text and code you have written.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#getting-started",
    "href": "appendix/rmarkdown.html#getting-started",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.2 Getting started",
    "text": "A.2 Getting started\nIn order to get started with Quarto, you first need to install it from here. Then, you will have to open an .qmd file in RStudio. Go to the upper left corner, click on the plus sign on the white page and select Quarto document. A new document will open. It might contain some text which we will go through and you’ll understand in a second. It is possible that you first have to install the R Markdown package from CRAN. To do this, run this line of code in your console:\n\nif(!require(rmarkdown)) \n  install.packages(\"rmarkdown\", repos = \"http://cran.us.r-project.org\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#annotating-text",
    "href": "appendix/rmarkdown.html#annotating-text",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.3 Annotating text",
    "text": "A.3 Annotating text\nHere are the most common things you might want to do with your text. These are commands that must be put before and after the words or sentences that you want to change. If you are used to coding html, this might seem familiar:\n\n\nitalics can be done with two ** in between which you put the words that should be italic\n\nbold words follow the same principle but putting two ** in front and ** two behind your chunk of text\nif you want to itemize or enumerate something simply use hyphens like this - or “1.”, “2.” etc.\nany headers must be preceded by a #, the more # you add the smaller the header will become; this way you can add up to six different sizes of headers\nany mathematical equations or variables can be written in LaTeX style by putting dollar signs around your text: $y = \\alpha + \\beta_1*x + \\epsilon$ becomes \\(y = \\alpha + \\beta_1*x + \\epsilon\\)\n\n\nThese commands that regard the textual output (anything that is not code in your final Markdown document) might not be that interesting for you (yet). You do not need to know how to write LaTeX equations or perfect Markdown documents. It is simply a quick walk through of what is possible. And after all, we are interested in the final code and a little less about what is italic and bold in your text…",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#including-code",
    "href": "appendix/rmarkdown.html#including-code",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.4 Including code",
    "text": "A.4 Including code\nSince this is a class on coding in R, you will have to include code in your files. This can be done in two ways. You might want to include code in your text like this data &lt;- read_csv(data). This can be done by fitting two accent grave as you would say in French around the piece of code. When I say accent grave it is the thing on top of à and è. Usually when we speak about a package, a function, an argument or a line of code that we want to specifically present, we put it as a piece of code. Thus, when I speak of the tidyverse package or the install.packages() function, we put two accents grave around the words so that they appear the way they appear in this sentence.\nOr you might want to include a whole chunk of code which then gives you the result in the final output:\n\nx &lt;- 2\ny &lt;- 2\nx + y\n\n[1] 4\n\n\nA whole chunk can be added by either typing the chunk delimiters ```{r} and ``` in two seperate lines. It is much easier if you use your keyboard hotkeys to do that. For Mac use Cmd + Option + I and for Windows use Ctrl + Alt + I . This will automatically generate a chunk in which you can write your code. Within this field, you can write your code, run it, assign variables as you would usually do. Sometimes you might not want to include its results, or the warnings or the chunk itself. Here are the ways to do that:\n\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\n\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\n\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\n\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\n\neval = FALSE tells knitr to skip the code in the chunk, but still include the results in the finished file. You can use this if a chunk is very computationally intensive, or if you need to knit the document but the code is not working!\n\nI encourage you to use these different functions at different points. It is no problem if you do not. But for example, when we load our packages that we will need for our R script at the beginning of our code (like this library(tidyverse)), R generates an output in the console which will also appear in the final Markdown document. To prevent the document from showing that, we would have to add the line include = FALSE. It is absolutely fine if you decide to show the code and its result as it is. It might just be a longer document and a little less elegant.\nEven before knitting (producing the final document), you can run the chunks of code. This can be either done on the right side of the chunk by clicking on the “play button” or by simply using your keyboard hotkey that you would usually use to run code. R will immediately show you if there is an issue with your code, just like it would do if this was not a RMarkdown file. The warnings and errors are the same and the troubleshooting process would then also be the same.\nYou are required to put the solutions to the exercises of this class in chunks of codes and discuss them outside of the chunks with some text. You will find instructions within the exercises regarding the interpretation of results or as to why you might have done something in a certain way. These comments must be put outside of the chunks of code. However, if you wish to annotate your code within the chunks, simply use the # as you would do in a normal R file.\n\ndata &lt;- read_csv(\"data.csv\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  },
  {
    "objectID": "appendix/rmarkdown.html#knitting-your-document",
    "href": "appendix/rmarkdown.html#knitting-your-document",
    "title": "Appendix A — Quarto & The Markdown Language",
    "section": "\nA.5 Knitting your document",
    "text": "A.5 Knitting your document\nI mentioned earlier that you have to “knit” your Quarto document at the end. RStudio compiles the document for you with all the commands and codes you have written. To knit, you need to click on the icon in your top bar above your code where you can see the knitting needle and yarn. The default setting is to produce a document in html format. However, you can also have a PDF document produced that is knitted in LaTeX style. These documents are the ones you have to hand in to me on the Moodle page for this class.\nHowever, please note that any error in your code will result in the document not being able to be knitted. This means that your code must be correct and work in order to create an html or pdf document. This is because in Markdown, most of the time, your results are displayed and based on each other, just like in any other programming. If one place doesn’t work, most of the later ones won’t work either. But R tells you, at the latest during the knitting process, in which line of your code the problem is.\n\n\n\n\n\n\nImportant\n\n\n\nIf you want to send me your script because you have to hand in your homework or because you cannot find a solution, it might happen that your code does not work. This then also means that R cannot knit the document to an html or pdf output. In that case, the chunks of code which are creating problems must be set to eval = FALSE to be included but not run! This way, I can see what you have done and where the problem is while having a knitted/rendered document at the same time!\n\n\nIf you have made it this far, I really hope that this quick setup tutorial for Quarto has helped you. If not, do not hesitate to get in touch and I’ll try to help! I highly encourage you though to play around with it, see what works and what doesn’t. Most of the time a quick Google search solves most of the problems. However, it is my job to help you, so do not hesitate to reach out!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Quarto & The Markdown Language</span>"
    ]
  }
]